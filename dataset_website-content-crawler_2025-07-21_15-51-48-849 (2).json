[{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "loadedTime": "2025-07-21T15:49:42.413Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 0,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "title": "",
    "description": "Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Web scraping basics for JavaScript devs | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Web+scraping+basics+for+JavaScript+devs"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "9939",
      "date": "Mon, 21 Jul 2025 15:42:16 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-9290\"",
      "expires": "Mon, 21 Jul 2025 15:52:16 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "55E1:2DF259:E02F2:F3B47:687E5FD6",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 abdd58a00d77becb00b33b2cb6e6e8e0.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000070-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753112537.567581, VS0, VE11",
      "x-fastly-request-id": "59467ffc58cc815189967f9430cfb8741ce15af5",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Hit from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "ImoE_tWqE-HxgeRTPqQOgRKYYBeIukxO1_w_lWf-PZBIURgyVtTFjA==",
      "age": "440",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Web scraping basics for JavaScript devs | Academy | Apify Documentation\nLearn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.\nWelcome to Web scraping basics for JavaScript devs, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you're looking for a quick start, we recommend trying this tutorial instead.\nThis course is made by Apify, the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you'll be able to run your scrapers on any computer. No Apify account needed.\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the Apify platform course, where we'll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\nWhy learn scraper development?​\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technologies like React or Next.js will be a piece of cake.\nCourse Summary​\nWhen we set out to create the Academy, we wanted to build a complete guide to web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\nThis is what you'll learn in the Web scraping basics for JavaScript devs course:\nWeb scraping basics for JavaScript devs \nBasics of data extraction\nBasics of crawling\nBest practices\nRequirements​\nYou don't need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don't be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you're new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using [] instead of () can make a lot of difference.\nIf you don't already have basic programming knowledge and would like to be well-prepared for this course, we recommend learning about JavaScript basics and CSS Selectors.\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\nIdeally, you should have at least a moderate understanding of the following concepts:\nJavaScript + Node.js​\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and async...await), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\nasync...await (YouTube)\nJavaScript loops (MDN)\nModularity in Node.js\nGeneral web development​\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because their knowledge will be assumed (unless we're showing something out of the ordinary).\nHTML\nHTTP protocol\nDevTools\njQuery or Cheerio​\nWe'll be using the Cheerio package a lot to parse data from HTML. This package provides an API using jQuery syntax to help traverse downloaded HTML within Node.js.\nNext up​\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. Let's get to it!\nIf you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the Basics of crawling section.",
  "markdown": "# Web scraping basics for JavaScript devs | Academy | Apify Documentation\n\n**Learn how to develop web scrapers with this comprehensive and practical course. Go from beginner to expert, all in one place.**\n\n* * *\n\nWelcome to **Web scraping basics for JavaScript devs**, a comprehensive, practical and long form web scraping course that will take you from an absolute beginner to a successful web scraper developer. If you're looking for a quick start, we recommend trying [this tutorial](https://blog.apify.com/web-scraping-javascript-nodejs/) instead.\n\nThis course is made by [Apify](https://apify.com/), the web scraping and automation platform, but we will use only open-source technologies throughout all academy lessons. This means that the skills you learn will be applicable to any scraping project, and you'll be able to run your scrapers on any computer. No Apify account needed.\n\nIf you would like to learn about the Apify platform and how it can help you build, run and scale your web scraping and automation projects, see the [Apify platform course](https://docs.apify.com/academy/apify-platform), where we'll teach you all about Apify serverless infrastructure, proxies, API, scheduling, webhooks and much more.\n\n## Why learn scraper development?[​](#why-learn \"Direct link to Why learn scraper development?\")\n\nWith so many point-and-click tools and no-code software that can help you extract data from websites, what is the point of learning web scraper development? Contrary to what their marketing departments say, a point-and-click or no-code tool will never be as flexible, as powerful, or as optimized as a custom-built scraper.\n\nAny software can do only what it was programmed to do. If you build your own scraper, it can do anything you want. And you can always quickly change it to do more, less, or the same, but faster or cheaper. The possibilities are endless once you know how scraping really works.\n\nScraper development is a fun and challenging way to learn web development, web technologies, and understand the internet. You will reverse-engineer websites and understand how they work internally, what technologies they use and how they communicate with their servers. You will also master your chosen programming language and core programming concepts. When you truly understand web scraping, learning other technologies like React or Next.js will be a piece of cake.\n\n## Course Summary[​](#summary \"Direct link to Course Summary\")\n\nWhen we set out to create the Academy, we wanted to build a complete guide to web scraping - a course that a beginner could use to create their first scraper, as well as a resource that professionals will continuously use to learn about advanced and niche web scraping techniques and technologies. All lessons include code examples and code-along exercises that you can use to immediately put your scraping skills into action.\n\nThis is what you'll learn in the **Web scraping basics for JavaScript devs** course:\n\n*   [Web scraping basics for JavaScript devs](https://docs.apify.com/academy/web-scraping-for-beginners)\n    *   [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction)\n    *   [Basics of crawling](https://docs.apify.com/academy/web-scraping-for-beginners/crawling)\n    *   [Best practices](https://docs.apify.com/academy/web-scraping-for-beginners/best-practices)\n\n## Requirements[​](#requirements \"Direct link to Requirements\")\n\nYou don't need to be a developer or a software engineer to complete this course, but basic programming knowledge is recommended. Don't be afraid, though. We explain everything in great detail in the course and provide external references that can help you level up your web scraping and web development skills. If you're new to programming, pay very close attention to the instructions and examples. A seemingly insignificant thing like using `[]` instead of `()` can make a lot of difference.\n\n> If you don't already have basic programming knowledge and would like to be well-prepared for this course, we recommend learning about [JavaScript basics](https://developer.mozilla.org/en-US/curriculum/core/javascript-fundamentals/) and [CSS Selectors](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors).\n\nAs you progress to the more advanced courses, the coding will get more challenging, but will still be manageable to a person with an intermediate level of programming skills.\n\nIdeally, you should have at least a moderate understanding of the following concepts:\n\n### JavaScript + Node.js[​](#javascript-and-node \"Direct link to JavaScript + Node.js\")\n\nIt is recommended to understand at least the fundamentals of JavaScript and be proficient with Node.js prior to starting this course. If you are not yet comfortable with asynchronous programming (with promises and `async...await`), loops (and the different types of loops in JavaScript), modularity, or working with external packages, we would recommend studying the following resources before coming back and continuing this section:\n\n*   [`async...await` (YouTube)](https://www.youtube.com/watch?v=vn3tm0quoqE&ab_channel=Fireship)\n*   [JavaScript loops (MDN)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Loops_and_iteration)\n*   [Modularity in Node.js](https://javascript.plainenglish.io/how-to-use-modular-patterns-in-nodejs-982f0e5c8f6e)\n\n### General web development[​](#general-web-development \"Direct link to General web development\")\n\nThroughout the next lessons, we will sometimes use certain technologies and terms related to the web without explaining them. This is because their knowledge will be **assumed** (unless we're showing something out of the ordinary).\n\n*   [HTML](https://developer.mozilla.org/en-US/docs/Web/HTML)\n*   [HTTP protocol](https://developer.mozilla.org/en-US/docs/Web/HTTP)\n*   [DevTools](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools)\n\n### jQuery or Cheerio[​](#jquery-or-cheerio \"Direct link to jQuery or Cheerio\")\n\nWe'll be using the [**Cheerio**](https://www.npmjs.com/package/cheerio) package a lot to parse data from HTML. This package provides an API using jQuery syntax to help traverse downloaded HTML within Node.js.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nThe course begins with a small bit of theory and moves into some realistic and practical examples of extracting data from the most popular websites on the internet using your browser console. [Let's get to it!](https://docs.apify.com/academy/web-scraping-for-beginners/introduction)\n\n> If you already have experience with HTML, CSS, and browser DevTools, feel free to skip to the [Basics of crawling](https://docs.apify.com/academy/web-scraping-for-beginners/crawling) section.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction",
    "loadedTime": "2025-07-21T15:49:57.229Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction",
    "title": "Basics of data extraction | Academy | Apify Documentation",
    "description": "Learn about HTML, CSS, and JavaScript, the basic building blocks of a website, and how to use them in web scraping and data extraction.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Basics of data extraction | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn about HTML, CSS, and JavaScript, the basic building blocks of a website, and how to use them in web scraping and data extraction."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Basics+of+data+extraction"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "8361",
      "date": "Mon, 21 Jul 2025 15:49:49 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-858c\"",
      "expires": "Mon, 21 Jul 2025 15:59:49 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "CC96:26B6DB:11C77C:13343F:687E619C",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 abdd58a00d77becb00b33b2cb6e6e8e0.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000164-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753112989.157850, VS0, VE15",
      "x-fastly-request-id": "abc2564f11df86f4e0326f2fdae7026d2a4d51e0",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "CY_nebycZb65zlaExIc5zfve-bveCPQP5Q0aIGpRvoCZxYYLDVq0sQ==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Basics of data extraction | Academy\nLearn about HTML, CSS, and JavaScript, the basic building blocks of a website, and how to use them in web scraping and data extraction.\nEvery web scraping project starts with some detective work. To a human, it's completely obvious where the data is on the web page, but a computer needs very precise instructions to find the data we want. We can leverage three elementary components of each website to give those instructions: HTML, CSS, and JavaScript\nHTML​\nFor the browser to be able to show you the web page with all its text and images, the data needs to be present somewhere. This data source is called HTML (HyperText Markup Language) and it gets downloaded to your computer whenever you open a website. If you want to extract data from a website, you need to show your computer where to find it in the HTML.\nTo learn more about markup, we recommend the resources about HTML provided by MDN, the official documentation of the web.\nCSS​\nCSS (Cascading Style Sheets) is a language that is used to give websites their style. It controls shapes, colors, positioning and even animations. The style is then added to the page's HTML and together, they define the page's content and structure. In web scraping, we can leverage CSS to find the data we want using CSS selectors.\nTo learn more about styles and selectors, we recommend the resources about CSS provided by MDN, the official documentation of the web.\nJavaScript​\nHTML and CSS give websites their structure and style, but they are static. To be able to meaningfully interact with a website, you need to throw JavaScript into the mix. It is the language of the web, and you don't need to be a programmer to learn the basics. You don't even need any special software, because you can try it right now, in your browser.\nTo learn more about programming in browser, we recommend the resources about JavaScript provided by MDN, the official documentation of the web.\nNext up​\nWe will show you how to use the browser DevTools to inspect and interact with a web page.",
  "markdown": "# Basics of data extraction | Academy\n\n**Learn about HTML, CSS, and JavaScript, the basic building blocks of a website, and how to use them in web scraping and data extraction.**\n\n* * *\n\nEvery web scraping project starts with some detective work. To a human, it's completely obvious where the data is on the web page, but a computer needs very precise instructions to find the data we want. We can leverage three elementary components of each website to give those instructions: HTML, CSS, and JavaScript\n\n## HTML[​](#html \"Direct link to HTML\")\n\nFor the browser to be able to show you the web page with all its text and images, the data needs to be present somewhere. This data source is called HTML (HyperText Markup Language) and it gets downloaded to your computer whenever you open a website. If you want to extract data from a website, you need to show your computer where to find it in the HTML.\n\n> To learn more about markup, we recommend the [resources about HTML](https://developer.mozilla.org/en-US/docs/Learn/HTML) provided by MDN, the official documentation of the web.\n\n## CSS[​](#css \"Direct link to CSS\")\n\nCSS (Cascading Style Sheets) is a language that is used to give websites their style. It controls shapes, colors, positioning and even animations. The style is then added to the page's HTML and together, they define the page's content and structure. In web scraping, we can leverage CSS to find the data we want using CSS selectors.\n\n> To learn more about styles and selectors, we recommend the [resources about CSS](https://developer.mozilla.org/en-US/docs/Learn/CSS) provided by MDN, the official documentation of the web.\n\n## JavaScript[​](#javascript \"Direct link to JavaScript\")\n\nHTML and CSS give websites their structure and style, but they are static. To be able to meaningfully interact with a website, you need to throw JavaScript into the mix. It is the language of the web, and you don't need to be a programmer to learn the basics. You don't even need any special software, because you can try it right now, in your browser.\n\n> To learn more about programming in browser, we recommend the [resources about JavaScript](https://developer.mozilla.org/en-US/docs/Learn/JavaScript) provided by MDN, the official documentation of the web.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nWe will show you [how to use the browser DevTools](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools) to inspect and interact with a web page.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools",
    "loadedTime": "2025-07-21T15:49:57.699Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools",
    "title": "Starting with browser DevTools | Academy | Apify Documentation",
    "description": "Learn about browser DevTools, a valuable tool in the world of web scraping, and how you can use them to extract data from a website.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/browser-devtools"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Starting with browser DevTools | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn about browser DevTools, a valuable tool in the world of web scraping, and how you can use them to extract data from a website."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Starting+with+browser+DevTools"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "10145",
      "date": "Mon, 21 Jul 2025 15:49:48 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-9cfd\"",
      "expires": "Mon, 21 Jul 2025 15:59:48 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "4962:28FE1C:133432:143E38:687E619C",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 abdd58a00d77becb00b33b2cb6e6e8e0.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100033-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753112989.867162, VS0, VE16",
      "x-fastly-request-id": "396094b5489281da661688068fa2c7382fbec521",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "rxQqHGn9kGYHxSg6fac5I44XVPytqdgtxqjG5MkC2D8Qg6F4uHJ5pg==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Starting with browser DevTools | Academy\nLearn about browser DevTools, a valuable tool in the world of web scraping, and how you can use them to extract data from a website.\nEven though DevTools stands for developer tools, everyone can use them to inspect a website. Each major browser has its own DevTools. We will use Chrome DevTools as an example, but the advice is applicable to any browser, as the tools are extremely similar. To open Chrome DevTools, you can press F12 or right-click anywhere in the page and choose Inspect. Now go to Wikipedia and open your DevTools there.\nElements tab​\nWhen you first open Chrome DevTools on Wikipedia, you will start on the Elements tab (In Firefox it's called the Inspector). You can use this tab to inspect the page's HTML on the left hand side, and its CSS on the right. The items in the HTML view are called elements.\nOn a screen that is narrow or has a small resolution, the CSS information can appear under the HTML tab, not on the right.\nEach element is enclosed in an HTML tag. For example <div>, <p>, and <span> are all tags. When you add something inside of those tags, like <p>Hello!</p> you create an element. You can also see elements inside other elements in the Elements tab. This is called nesting, and it gives the page its structure.\nAt the bottom, there's the JavaScript console, which is a powerful tool which can be used to manipulate the website. If the console is not there, you can press ESC to toggle it. All of this might look super complicated at first, but don't worry, there's no need to understand everything yet - we'll walk you through all the important things you need to know.\nSelecting an element​\nIn the top left corner of DevTools, there's a little arrow icon with a square.\nClick it and then hover your mouse over The Free Encyclopedia, Wikipedia's subtitle. DevTools will show you information about the HTML element being hovered over. Now click the element. It will be selected in the Elements tab, which allows for further inspection of the element and its content.\nInteracting with an element​\nAfter you select the subtitle element, right-click the highlighted element in the Elements tab to show a menu with available actions. For now, select Store as global variable (Use in Console in Firefox). You'll see that a new variable called temp1 (temp0 in Firefox) appeared in your DevTools Console. You can now use the Console to access the element's properties using JavaScript.\nFor example, if you wanted to scrape the text inside the element, you could use the textContent property to get it. Copy and paste (or type) the following command into your Console and press Enter. The text of your temp1 element - The Free Encyclopedia - will display in the Console.\nNow run this command to get the HTML of the element:\nAnd finally, run the next command to change the text of the element.\ntemp1.textContent = 'Hello World!';\nBy changing HTML elements from the Console, you can change what's displayed on the page. This change only happens on your own computer so don't worry, you haven't hacked Wikipedia.\nIn JavaScript, the web page is called document. From the Console you can interact with it in many ways. Go through document basics to learn more.\nNext up​\nIn this lesson, we learned the absolute basics of interaction with a page using the DevTools. In the next lesson, you will learn how to extract data from it. We will extract data about the on-sale products on the Warehouse store.\nIt isn't a real store, but a full-featured demo of a Shopify online store. And that is perfect for our purposes. Shopify is one of the largest e-commerce platforms in the world, and it uses all the latest technologies that a real e-commerce web application would use. Learning to scrape a Shopify store is useful, because you can immediately apply the learnings to millions of websites.",
  "markdown": "# Starting with browser DevTools | Academy\n\n**Learn about browser DevTools, a valuable tool in the world of web scraping, and how you can use them to extract data from a website.**\n\n* * *\n\nEven though DevTools stands for developer tools, everyone can use them to inspect a website. Each major browser has its own DevTools. We will use Chrome DevTools as an example, but the advice is applicable to any browser, as the tools are extremely similar. To open Chrome DevTools, you can press **F12** or right-click anywhere in the page and choose **Inspect**. Now go to [Wikipedia](https://www.wikipedia.org/) and open your DevTools there.\n\n![Wikipedia with Chrome DevTools open](https://docs.apify.com/assets/images/browser-devtools-wikipedia-d20b19ea46ed30572858ddc63d9e0f23.png)\n\n## Elements tab[​](#elements-tab \"Direct link to Elements tab\")\n\nWhen you first open Chrome DevTools on Wikipedia, you will start on the Elements tab (In Firefox it's called the **Inspector**). You can use this tab to inspect the page's HTML on the left hand side, and its CSS on the right. The items in the HTML view are called [**elements**](https://docs.apify.com/academy/concepts/html-elements).\n\n![Elements tab in Chrome DevTools](https://docs.apify.com/assets/images/browser-devtools-elements-tab-fb7aa7fc2b9442bb7fd94dbc6955e4c8.png)\n\n> On a screen that is narrow or has a small resolution, the CSS information can appear under the HTML tab, not on the right.\n\nEach element is enclosed in an HTML tag. For example `<div>`, `<p>`, and `<span>` are all tags. When you add something inside of those tags, like `<p>Hello!</p>` you create an element. You can also see elements inside other elements in the **Elements** tab. This is called nesting, and it gives the page its structure.\n\nAt the bottom, there's the **JavaScript console**, which is a powerful tool which can be used to manipulate the website. If the console is not there, you can press **ESC** to toggle it. All of this might look super complicated at first, but don't worry, there's no need to understand everything yet - we'll walk you through all the important things you need to know.\n\n![Console in Chrome DevTools](https://docs.apify.com/assets/images/browser-devtools-console-0752bf16933c5b7b8858dac3bbd80694.png)\n\n## Selecting an element[​](#selecting-an-element \"Direct link to Selecting an element\")\n\nIn the top left corner of DevTools, there's a little arrow icon with a square.\n\n![Chrome DevTools element selection tool](https://docs.apify.com/assets/images/browser-devtools-element-selection-c1cf8032d6d23ad5941c7ebf2b0f1ae5.png)\n\nClick it and then hover your mouse over **The Free Encyclopedia**, Wikipedia's subtitle. DevTools will show you information about the HTML element being hovered over. Now click the element. It will be selected in the **Elements** tab, which allows for further inspection of the element and its content.\n\n![Chrome DevTools element hover effect](https://docs.apify.com/assets/images/browser-devtools-hover-b85b0699eef969e79c92fda46154bbe2.png)\n\n## Interacting with an element[​](#interacting-with-elements \"Direct link to Interacting with an element\")\n\nAfter you select the subtitle element, right-click the highlighted element in the Elements tab to show a menu with available actions. For now, select **Store as global variable** (**Use in Console** in Firefox). You'll see that a new variable called `temp1` (`temp0` in Firefox) appeared in your DevTools Console. You can now use the Console to access the element's properties using JavaScript.\n\nFor example, if you wanted to scrape the text inside the element, you could use the `textContent` property to get it. Copy and paste (or type) the following command into your Console and press Enter. The text of your `temp1` element - The Free Encyclopedia - will display in the Console.\n\nNow run this command to get the HTML of the element:\n\nAnd finally, run the next command to change the text of the element.\n\n```\ntemp1.textContent = 'Hello World!';\n```\n\nBy changing HTML elements from the Console, you can change what's displayed on the page. This change only happens on your own computer so don't worry, you haven't hacked Wikipedia.\n\n![Chrome DevTools JavaScript command execution](https://docs.apify.com/assets/images/browser-devtools-console-commands-9f82a9905f884595024c32ee4519760a.png)\n\n> In JavaScript, the web page is called `document`. From the Console you can interact with it in many ways. Go through [document basics](https://developer.mozilla.org/en-US/docs/Web/API/Document_object_model/Using_the_Document_Object_Model) to learn more.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nIn this lesson, we learned the absolute basics of interaction with a page using the DevTools. In the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools), you will learn how to extract data from it. We will extract data about the on-sale products on the [Warehouse store](https://warehouse-theme-metal.myshopify.com/).\n\nIt isn't a real store, but a full-featured demo of a Shopify online store. And that is perfect for our purposes. Shopify is one of the largest e-commerce platforms in the world, and it uses all the latest technologies that a real e-commerce web application would use. Learning to scrape a Shopify store is useful, because you can immediately apply the learnings to millions of websites.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools",
    "loadedTime": "2025-07-21T15:50:03.401Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools",
    "title": "Finding elements with DevTools | Academy | Apify Documentation",
    "description": "Learn how to use browser DevTools, CSS selectors, and JavaScript via the DevTools console to extract data from a website.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Finding elements with DevTools | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to use browser DevTools, CSS selectors, and JavaScript via the DevTools console to extract data from a website."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Finding+elements+with+DevTools"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "14838",
      "date": "Mon, 21 Jul 2025 15:50:01 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-f89e\"",
      "expires": "Mon, 21 Jul 2025 16:00:01 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "51B9:116E77:10B718:11C1E9:687E61A9",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 abdd58a00d77becb00b33b2cb6e6e8e0.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100122-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113002.776597, VS0, VE13",
      "x-fastly-request-id": "9c52ce69fd1255a656712cfddc9fcdb8340a0ebc",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "goVEuM8azEhiZ-G8TvEugu6WNNPItB0J_Cc6LsBLFlOsMfV3bIzi2A==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Finding elements with DevTools | Academy\nLearn how to use browser DevTools, CSS selectors, and JavaScript via the DevTools console to extract data from a website.\nWith the knowledge of the basics of DevTools we can finally try doing something more practical - extracting data from a website. Let's try collecting the on-sale products from the Warehouse store. We will use CSS selectors, JavaScript, and DevTools to achieve this task.\nWhy use a Shopify demo and not a real e-commerce store like Amazon? Because real websites are usually bulkier, littered with promotions, and they change very often. Many have multiple versions of pages, and you never know in advance which one you will get. It will be important to learn how to deal with these challenges in the future, but for this beginner course, we want to have a light and stable environment.\nSome other courses use so-called scraping playgrounds or sandboxes. Those are websites made solely for the purpose of learning scraping. We find those too dumbed down and not representative of real websites. The Shopify demo is a full-featured, real-world website.\nGetting structured data from HTML​\nWhen you open up the Sales section of Warehouse, you'll see that there's a grid of products on the page with names and pictures of products. We will learn how to extract all this information.\nOpen DevTools and select the name of the Sony SACS9 Active Subwoofer. When you click on it, it will get highlighted in the Elements tab.\nGreat, you have selected the element which contains the name of the subwoofer. Now we want to find all the elements that contain all the information about this subwoofer. Price, number of reviews, image and everything else you might need. We will use the Elements tab to do that. You can hover over the elements in the Elements tab, and they will get highlighted on the page as you move the mouse.\nStart from the previously selected element with the subwoofer's name and move your mouse up, hovering over each element, until you find the one that highlights the entire product card. Alternatively, you can press the up arrow a few times to get the same result.\nThe element that contains all the information about the subwoofer is called a parent element, and all the nested elements, including the subwoofer's name, price and everything else, are child elements.\nNow that we know how the parent element looks, we can extract its data, including the data of its children. Notice that the element has a class attribute with multiple values like product-item or product-item--vertical. Let's use those classes in the Console to extract data.\nSelecting elements in Console​\nWe know how to find an element manually using the DevTools, but that's not very useful for automated scraping. We need to tell the computer how to find it as well. We can do that using JavaScript and CSS selectors. The function to do that is called document.querySelector() and it will find the first element in the page's HTML matching the provided CSS selector.\nFor example document.querySelector('div') will find the first <div> element. And document.querySelector('.my-class') (notice the period .) will find the first element with the class my-class, such as <div class=\"my-class\"> or <p class=\"my-class\">.\nYou can also combine selectors. document.querySelector('p.my-class') will find all <p class=\"my-class\"> elements, but no <div class=\"my-class\">.\nLet's try to use document.querySelector() to find the Sony subwoofer. Earlier we mentioned that the parent element of the subwoofer had, among others, the product-item class. We can use the class to look up the element. Copy or type (don't miss the period . in .product-item) the following function into the Console and press Enter.\ndocument.querySelector('.product-item');\nIt will produce a result like this, but it won't be the Sony subwoofer.\nAbout the missing semicolon\nIn the screenshot, there is a missing semicolon ; at the end of the line. In JavaScript, semicolons are optional, so it makes no difference.\nWhen we look more closely by hovering over the result in the Console, we find that instead of the Sony subwoofer, we found a JBL Flip speaker. Why? Because earlier we explained that document.querySelector('.product-item') finds the first element with the product-item class, and the JBL speaker is the first product in the list.\nWe need a different function: document.querySelectorAll() (notice the All at the end). This function does not find only the first element, but all the elements that match the provided selector.\nRun the following function in the Console:\ndocument.querySelectorAll('.product-item');\nIt will return a NodeList (a type of array) with many results. Expand the results by clicking the small arrow button and then hover over the third (number 2, indexing starts at 0) element in the list. You'll find that it's the Sony subwoofer we're looking for.\nNaturally, this is the method we use mostly in web scraping, because we're usually interested in scraping all the products from a page, not just a single product.\nElements or nodes?\nThe list is called a NodeList, because browsers understand a HTML document as a tree of nodes. Most of the nodes are HTML elements, but there can be also text nodes for plain text, and others.\nHow to choose good selectors​\nOften you can select the same element with different CSS selectors. Try to choose selectors that are simple, human-readable, unique and semantically connected to the data. Selectors that meet these criteria are sometimes called resilient selectors, because they're the most reliable and least likely to change with website updates. If you can, avoid randomly generated attributes like class=\"F4jsL8\". They change often and without warning.\nThe product-item class is simple, human-readable, and semantically connected with the data. The subwoofer is one of the products. A product item. Those are strong signals that this is a good selector. It's also sufficiently unique in the website's context. If the selector was only an item, for example, there would be a higher chance that the website's developers would add this class to something unrelated. Like an advertisement. And it could break your extraction code.\nNow that we found the element, we can start poking into it to extract data. First, let's save the element to a variable so that we can work with it repeatedly. Run these commands in the Console:\nconst products = document.querySelectorAll('.product-item');\nconst subwoofer = products[2];\nIf you're wondering what an array is or what products[2] means, read the JavaScript arrays basics.\nNow that we have the subwoofer saved in a variable, run another command in the Console to print its text:\nAs you can see, we were able to extract information about the subwoofer, but the format is still not very useful - there's a lot of content that we don't need. For further processing (ex. in a spreadsheet), we would like to have each piece of data as a separate field (column). To do that, we will look at the HTML structure in more detail.\nFinding child elements​\nIn the Getting structured data from HTML section, we were browsing the elements in the Elements tab to find the element that contains all the data. We can use the same approach to find the individual data points as well.\nStart from the element that contains all data: <div class=\"product-item...\"> Then inspect all the elements nested within this element. You'll discover that:\nthe product's name is an <a> element with the class product-item__title, and\nthe price is held inside a <span> with the class price. Note that there are two prices. The sale price and the regular price. We want the sale price.\nWe will use this knowledge soon to extract the data.\nSelecting child elements​\nThe document.querySelector() function looks for a specific element in the whole HTML document, so if we called it with h3, it would find the first <h3> node in the document. But we can replace the document with any other parent element and the function will limit its search to child elements of the chosen parent.\nEarlier we selected the parent element of the Sony subwoofer and saved it to a variable called subwoofer. Let's use this variable to search inside the subwoofer element and find the product's name and price.\nRun two commands in the Console. The first will find the element with the subwoofer's name and save it to a variable called title. The second will extract the name and print it.\nconst title = subwoofer.querySelector('a.product-item__title');\ntitle.textContent;\nGreat! We found a way how to programmatically extract the name of the product. We're getting somewhere.\nNext, run the following two commands in the Console.\nconst price = subwoofer.querySelector('span.price');\nprice.textContent;\nIt worked, but the price was not alone in the result. We extracted it together with some extra text. This is very common in web scraping. Sometimes it's impossible to separate the data we need by element selection alone, and we have to clean the data using other methods.\nWhen it comes to data cleaning, there are two main approaches you can take. It's beneficial to understand both, as one approach may be feasible in a given situation while the other is not.\nRemove the elements that add noise to your data from the selection. Then extract the pre-cleaned data.\nExtract the data with noise. Use regular expressions or other text manipulation techniques to parse the data and keep only the parts we're interested in.\nFirst, let's look at removing the noise before extraction. When you look closely at the element that contains the price, you'll see that it includes another <span> element with the text Sale price. This <span> is what adds noise to our data, and we have to get rid of it.\nWhen we call subwoofer.querySelector('span.price') it selects the whole <span class=\"price ...:> element. Unfortunately, it also includes the <span class=\"visually-hidden\"> element that we're not interested in.\nWe can, however, use JavaScript to get only the actual text of the selected element, without any child elements. Run this command in the Console:\nprice.childNodes[2].nodeValue;\nWhy the third child node? Because the first one represents the empty space before <span class=\"visually-hidden\", the second is the noise <span> itself and the third one is the price. In any case, we were able to extract the clean price.\nThe second option we have is to take the noisy price data and clean it with string manipulation. The data looks like this:\nThis can be approached in a variety of ways. To start let's look at a naive solution:\nprice.textContent.split('$')[1];\nAnd there you go. Notice that this time we extracted the price without the $ dollar sign. This could be desirable, because we wanted to convert the price from a string to a number, or not, depending on individual circumstances of the scraping project.\nWhich method to choose? Neither is the perfect solution. The first method could break if the website's developers change the structure of the <span> elements and the price will no longer be in the third position - a very small change that can happen at any moment.\nThe second method seems more reliable, but only until the website adds prices in other currency or decides to replace $ with USD. It's up to you, the scraping developer to decide which of the methods will be more resilient on the website you scrape.\nIn production, we would probably use a regular expression like the following, or a specialized library for parsing prices from strings, but for this tutorial, we'll keep it simple.\nprice.textContent.match(/((\\d+,?)+.?(\\d+)?)/)[0];\nNext up​\nThis concludes our lesson on extracting and cleaning data using DevTools. Using CSS selectors, we were able to find the HTML element that contains data about our favorite Sony subwoofer and then extract the data. In the next lesson, we will learn how to extract information not only about the subwoofer, but about all the products on the page.",
  "markdown": "# Finding elements with DevTools | Academy\n\n**Learn how to use browser DevTools, CSS selectors, and JavaScript via the DevTools console to extract data from a website.**\n\n* * *\n\nWith the knowledge of the basics of DevTools we can finally try doing something more practical - extracting data from a website. Let's try collecting the on-sale products from the [Warehouse store](https://warehouse-theme-metal.myshopify.com/). We will use [CSS selectors](https://docs.apify.com/academy/concepts/css-selectors), JavaScript, and DevTools to achieve this task.\n\n> **Why use a Shopify demo and not a real e-commerce store like Amazon?** Because real websites are usually bulkier, littered with promotions, and they change very often. Many have multiple versions of pages, and you never know in advance which one you will get. It will be important to learn how to deal with these challenges in the future, but for this beginner course, we want to have a light and stable environment.\n> \n> Some other courses use so-called scraping playgrounds or sandboxes. Those are websites made solely for the purpose of learning scraping. We find those too dumbed down and not representative of real websites. The Shopify demo is a full-featured, real-world website.\n\n## Getting structured data from HTML[​](#getting-structured-data \"Direct link to Getting structured data from HTML\")\n\nWhen you open up the [Sales section of Warehouse](https://warehouse-theme-metal.myshopify.com/collections/sales), you'll see that there's a grid of products on the page with names and pictures of products. We will learn how to extract all this information.\n\n![Warehouse store with DevTools open](https://docs.apify.com/assets/images/devtools-collection-warehouse-9ab5f52429a865e8965f889abd5e2180.png)\n\nOpen DevTools and select the name of the **Sony SACS9 Active Subwoofer**. When you click on it, it will get highlighted in the Elements tab.\n\n![Selecting an element with DevTools](https://docs.apify.com/assets/images/devtools-collection-product-name-49124d4c5af086ac2577f349e1ffd989.png)\n\nGreat, you have selected the element which contains the name of the subwoofer. Now we want to find all the elements that contain all the information about this subwoofer. Price, number of reviews, image and everything else you might need. We will use the **Elements** tab to do that. You can hover over the elements in the Elements tab, and they will get highlighted on the page as you move the mouse.\n\nStart from the previously selected element with the subwoofer's name and move your mouse up, hovering over each element, until you find the one that highlights the entire product card. Alternatively, you can press the up arrow a few times to get the same result.\n\nThe element that contains all the information about the subwoofer is called a **parent element**, and all the nested elements, including the subwoofer's name, price and everything else, are **child elements**.\n\n![Selecting an element with hover](https://docs.apify.com/assets/images/devtools-collection-product-hover-dd23c141bea5f9739410eaa06e83ee80.png)\n\nNow that we know how the parent element looks, we can extract its data, including the data of its children. Notice that the element has a `class` attribute with multiple values like `product-item` or `product-item--vertical`. Let's use those classes in the Console to extract data.\n\n![Class attribute in DevTools](https://docs.apify.com/assets/images/devtools-collection-class-0c9aab05cfd98e0e534e0c9adbac28da.png)\n\n## Selecting elements in Console[​](#selecting-elements \"Direct link to Selecting elements in Console\")\n\nWe know how to find an element manually using the DevTools, but that's not very useful for automated scraping. We need to tell the computer how to find it as well. We can do that using JavaScript and CSS selectors. The function to do that is called [`document.querySelector()`](https://docs.apify.com/academy/concepts/querying-css-selectors) and it will find the first element in the page's HTML matching the provided [CSS selector](https://docs.apify.com/academy/concepts/css-selectors).\n\nFor example `document.querySelector('div')` will find the first `<div>` element. And `document.querySelector('.my-class')` (notice the period `.`) will find the first element with the class `my-class`, such as `<div class=\"my-class\">` or `<p class=\"my-class\">`.\n\nYou can also combine selectors. `document.querySelector('p.my-class')` will find all `<p class=\"my-class\">` elements, but no `<div class=\"my-class\">`.\n\nLet's try to use `document.querySelector()` to find the **Sony subwoofer**. Earlier we mentioned that the parent element of the subwoofer had, among others, the `product-item` class. We can use the class to look up the element. Copy or type (don't miss the period `.` in `.product-item`) the following function into the Console and press Enter.\n\n```\ndocument.querySelector('.product-item');\n```\n\nIt will produce a result like this, but it **won't be** the Sony subwoofer.\n\n![Query a selector with JavaScript](https://docs.apify.com/assets/images/devtools-collection-query-9f650202d95e659b6124adfc5bc1754d.png)\n\nAbout the missing semicolon\n\nIn the screenshot, there is a missing semicolon `;` at the end of the line. In JavaScript, semicolons are optional, so it makes no difference.\n\nWhen we look more closely by hovering over the result in the Console, we find that instead of the Sony subwoofer, we found a JBL Flip speaker. Why? Because earlier we explained that `document.querySelector('.product-item')` finds the **first element** with the `product-item` class, and the JBL speaker is the first product in the list.\n\n![Hover over a query result](https://docs.apify.com/assets/images/devtools-collection-query-hover-06b6ee753dee883803dc079bf2abecf1.png)\n\nWe need a different function: [`document.querySelectorAll()`](https://docs.apify.com/academy/concepts/querying-css-selectors) (notice the `All` at the end). This function does not find only the first element, but all the elements that match the provided selector.\n\nRun the following function in the Console:\n\n```\ndocument.querySelectorAll('.product-item');\n```\n\nIt will return a `NodeList` (a type of array) with many results. Expand the results by clicking the small arrow button and then hover over the third (number 2, indexing starts at 0) element in the list. You'll find that it's the Sony subwoofer we're looking for.\n\n![Hover over a query result](https://docs.apify.com/assets/images/devtools-collection-query-all-8d4bc26a48292f70f45bd4fe99fbbb0a.png)\n\nNaturally, this is the method we use mostly in web scraping, because we're usually interested in scraping all the products from a page, not just a single product.\n\nElements or nodes?\n\nThe list is called a `NodeList`, because browsers understand a HTML document as a tree of nodes. Most of the nodes are HTML elements, but there can be also text nodes for plain text, and others.\n\n## How to choose good selectors[​](#choose-good-selectors \"Direct link to How to choose good selectors\")\n\nOften you can select the same element with different CSS selectors. Try to choose selectors that are **simple**, **human-readable**, **unique** and **semantically connected** to the data. Selectors that meet these criteria are sometimes called **resilient selectors**, because they're the most reliable and least likely to change with website updates. If you can, avoid randomly generated attributes like `class=\"F4jsL8\"`. They change often and without warning.\n\nThe `product-item` class is simple, human-readable, and semantically connected with the data. The subwoofer is one of the products. A product item. Those are strong signals that this is a good selector. It's also sufficiently unique in the website's context. If the selector was only an `item`, for example, there would be a higher chance that the website's developers would add this class to something unrelated. Like an advertisement. And it could break your extraction code.\n\nNow that we found the element, we can start poking into it to extract data. First, let's save the element to a variable so that we can work with it repeatedly. Run these commands in the Console:\n\n```\nconst products = document.querySelectorAll('.product-item');const subwoofer = products[2];\n```\n\n> If you're wondering what an array is or what `products[2]` means, read the [JavaScript arrays basics](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/First_steps/Arrays).\n\nNow that we have the subwoofer saved in a variable, run another command in the Console to print its text:\n\n![Print text content of parent element](https://docs.apify.com/assets/images/devtools-print-parent-text-1e295ece2af49af7bd9cf50aba9d2868.png)\n\nAs you can see, we were able to extract information about the subwoofer, but the format is still not very useful - there's a lot of content that we don't need. For further processing (ex. in a spreadsheet), we would like to have each piece of data as a separate field (column). To do that, we will look at the HTML structure in more detail.\n\n### Finding child elements[​](#finding-child-elements \"Direct link to Finding child elements\")\n\nIn the [Getting structured data from HTML](#getting-structured-data) section, we were browsing the elements in the **Elements** tab to find the element that contains all the data. We can use the same approach to find the individual data points as well.\n\nStart from the element that contains all data: `<div class=\"product-item...\">` Then inspect all the elements nested within this element. You'll discover that:\n\n*   the product's name is an `<a>` element with the class `product-item__title`, and\n*   the price is held inside a `<span>` with the class `price`. Note that there are two prices. The sale price and the regular price. We want the sale price.\n\nWe will use this knowledge soon to extract the data.\n\n![Finding child elements in Elements tab](https://docs.apify.com/assets/images/devtools-find-child-elements-c503170e9925d6e564f97317bf7503b6.png)\n\n### Selecting child elements[​](#selecting-child-elements \"Direct link to Selecting child elements\")\n\nThe `document.querySelector()` function looks for a specific element in the whole HTML `document`, so if we called it with `h3`, it would find the first `<h3>` node in the `document`. But we can replace the `document` with any other parent element and the function will limit its search to child elements of the chosen parent.\n\nEarlier we selected the parent element of the Sony subwoofer and saved it to a variable called `subwoofer`. Let's use this variable to search inside the subwoofer element and find the product's name and price.\n\nRun two commands in the Console. The first will find the element with the subwoofer's name and save it to a variable called `title`. The second will extract the name and print it.\n\n```\nconst title = subwoofer.querySelector('a.product-item__title');title.textContent;\n```\n\n![Extract product title](https://docs.apify.com/assets/images/devtools-extract-product-title-b1934c7e0566336835fad2001dd126b7.png)\n\nGreat! We found a way how to programmatically extract the name of the product. We're getting somewhere.\n\nNext, run the following two commands in the Console.\n\n```\nconst price = subwoofer.querySelector('span.price');price.textContent;\n```\n\n![Extract product price](https://docs.apify.com/assets/images/devtools-extract-product-price-b6f83ca557a4c83972c75b78631df80e.png)\n\nIt worked, but the price was not alone in the result. We extracted it together with some extra text. This is very common in web scraping. Sometimes it's impossible to separate the data we need by element selection alone, and we have to clean the data using other methods.\n\nWhen it comes to data cleaning, there are two main approaches you can take. It's beneficial to understand both, as one approach may be feasible in a given situation while the other is not.\n\n1.  Remove the elements that add noise to your data from the selection. Then extract the pre-cleaned data.\n2.  Extract the data with noise. Use [regular expressions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions) or other text manipulation techniques to parse the data and keep only the parts we're interested in.\n\nFirst, let's look at **removing the noise before extraction**. When you look closely at the element that contains the price, you'll see that it includes another `<span>` element with the text **Sale price**. This `<span>` is what adds noise to our data, and we have to get rid of it.\n\n![Noise in element selection](https://docs.apify.com/assets/images/devtools-cleaning-noise-d6f3a7365fa9d2306086e00d8cd7fa88.png)\n\nWhen we call `subwoofer.querySelector('span.price')` it selects the whole `<span class=\"price ...:>` element. Unfortunately, it also includes the `<span class=\"visually-hidden\">` element that we're not interested in.\n\nWe can, however, use JavaScript to get only the actual text of the selected element, without any child elements. Run this command in the Console:\n\n```\nprice.childNodes[2].nodeValue;\n```\n\nWhy the third child node? Because the first one represents the empty space before `<span class=\"visually-hidden\"`, the second is the noise `<span>` itself and the third one is the price. In any case, we were able to extract the clean price.\n\n![Clean price selection](https://docs.apify.com/assets/images/devtools-clean-price-8411206ba8f39485d232cb7c1e5d066a.png)\n\nThe second option we have is to **take the noisy price data and clean it with string manipulation**. The data looks like this:\n\nThis can be approached in a variety of ways. To start let's look at a naive solution:\n\n```\nprice.textContent.split('$')[1];\n```\n\n![Split price from noise](https://docs.apify.com/assets/images/devtools-split-price-f86125408dc6f5ad844bb3d154931bc6.png)\n\nAnd there you go. Notice that this time we extracted the price without the `$` dollar sign. This could be desirable, because we wanted to convert the price from a string to a number, or not, depending on individual circumstances of the scraping project.\n\nWhich method to choose? Neither is the perfect solution. The first method could break if the website's developers change the structure of the `<span>` elements and the price will no longer be in the third position - a very small change that can happen at any moment.\n\nThe second method seems more reliable, but only until the website adds prices in other currency or decides to replace `$` with `USD`. It's up to you, the scraping developer to decide which of the methods will be more resilient on the website you scrape.\n\nIn production, we would probably use a regular expression like the following, or a specialized library for parsing prices from strings, but for this tutorial, we'll keep it simple.\n\n```\nprice.textContent.match(/((\\d+,?)+.?(\\d+)?)/)[0];\n```\n\n## Next up[​](#next \"Direct link to Next up\")\n\nThis concludes our lesson on extracting and cleaning data using DevTools. Using CSS selectors, we were able to find the HTML element that contains data about our favorite Sony subwoofer and then extract the data. In the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/devtools-continued), we will learn how to extract information not only about the subwoofer, but about all the products on the page.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/introduction",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/introduction",
    "loadedTime": "2025-07-21T15:49:58.007Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/introduction",
    "title": "Introduction | Academy | Apify Documentation",
    "description": "Start learning about web scraping, web crawling, data extraction, and popular tools to start developing your own scraper.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/introduction"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Introduction | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Start learning about web scraping, web crawling, data extraction, and popular tools to start developing your own scraper."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Introduction"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "8217",
      "date": "Mon, 21 Jul 2025 15:49:49 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-7de6\"",
      "expires": "Mon, 21 Jul 2025 15:59:49 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "EB27:2BB9C9:13FBB8:1505A6:687E619D",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 abdd58a00d77becb00b33b2cb6e6e8e0.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000120-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753112990.562614, VS0, VE10",
      "x-fastly-request-id": "2eee48a84e0ad4d0aa8552f6bf6d744ab51cd669",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "c0RsSNtRKLoZ2WZqpD5t4eeV7KluuSR6WFb3I9_bVuHDXCm_YqP8wg==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Introduction | Academy | Apify Documentation\nStart learning about web scraping, web crawling, data extraction, and popular tools to start developing your own scraper.\nWeb scraping or crawling? Web data extraction, mining, or collection? You can find various definitions on the web. Let's agree on explanations that we will use throughout this beginner course on web scraping.\nWeb data extraction (or collection) is a process that takes a web page, like an Amazon product page, and collects useful information from the page, such as the product's name and price. Web pages are an unstructured data source and the goal of web data extraction is to make information from websites structured, so that it can be processed by data analysis tools or integrated with computer systems. The main sources of data on a web page are HTML documents and API calls, but also images, PDFs, etc.\nWhat is crawling?​\nWhere web data extraction focuses on a single page, web crawling (sometimes called spidering 🕷) is all about movement between pages or websites. The purpose of crawling is to travel across the website to find pages with the information we want. Crawling and collection can happen either simultaneously, while moving from page to page, or separately, where one scraper focuses solely on finding pages with data, and another scraper collects the data. The main purpose of crawling is to collect URLs or other links that can be used to move around.\nWhat is web scraping?​\nWe use web scraping as an umbrella term for crawling, web data extraction and all other activities that have the purpose of converting unstructured data from the web to a structured format ready for integration or data analysis. In the advanced courses, you'll learn that web scraping is about much more than just HTML and URLs.\nNext up​\nIn the next lesson, you will learn about the basic building blocks of each web page. HTML, CSS and JavaScript.",
  "markdown": "# Introduction | Academy | Apify Documentation\n\n**Start learning about web scraping, web crawling, data extraction, and popular tools to start developing your own scraper.**\n\n* * *\n\nWeb scraping or crawling? Web data extraction, mining, or collection? You can find various definitions on the web. Let's agree on explanations that we will use throughout this beginner course on web scraping.\n\nWeb data extraction (or collection) is a process that takes a web page, like an Amazon product page, and collects useful information from the page, such as the product's name and price. Web pages are an unstructured data source and the goal of web data extraction is to make information from websites structured, so that it can be processed by data analysis tools or integrated with computer systems. The main sources of data on a web page are HTML documents and API calls, but also images, PDFs, etc.\n\n![product data extraction from Amazon](https://docs.apify.com/assets/images/beginners-data-extraction-2bb4d2e434080f9d29cb78c66c3a8ac2.png)\n\n## What is crawling?[​](#what-is-crawling \"Direct link to What is crawling?\")\n\nWhere web data extraction focuses on a single page, web crawling (sometimes called spidering 🕷) is all about movement between pages or websites. The purpose of crawling is to travel across the website to find pages with the information we want. Crawling and collection can happen either simultaneously, while moving from page to page, or separately, where one scraper focuses solely on finding pages with data, and another scraper collects the data. The main purpose of crawling is to collect URLs or other links that can be used to move around.\n\n## What is web scraping?[​](#what-is-web-scraping \"Direct link to What is web scraping?\")\n\nWe use web scraping as an umbrella term for crawling, web data extraction and all other activities that have the purpose of converting unstructured data from the web to a structured format ready for integration or data analysis. In the advanced courses, you'll learn that web scraping is about much more than just HTML and URLs.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nIn the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction), you will learn about the basic building blocks of each web page. HTML, CSS and JavaScript.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup",
    "loadedTime": "2025-07-21T15:50:23.718Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup",
    "title": "Project setup | Academy | Apify Documentation",
    "description": "Create a new project with npm and Node.js. Install necessary libraries, and test that everything works before starting the next lesson.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Project setup | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Create a new project with npm and Node.js. Install necessary libraries, and test that everything works before starting the next lesson."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Project+setup"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "10147",
      "date": "Mon, 21 Jul 2025 15:50:10 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-a51d\"",
      "expires": "Mon, 21 Jul 2025 16:00:10 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "CBEF:2DF259:10682B:11D6C9:687E61B1",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100124-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113010.281231, VS0, VE24",
      "x-fastly-request-id": "601f54ade7b48632cd10e945eb33e77acb1f674d",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "KGJe5jzUWYLIs87ubJE997c4W8rzz9k-12vXugTXBVV6xPEeVQmIVw==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Project setup | Academy | Apify Documentation\nSetting up your project\nCreate a new project with npm and Node.js. Install necessary libraries, and test that everything works before starting the next lesson.\nWhen you open a website in a browser, the browser first downloads the page's HTML. To do the same thing with Node.js, we will install a program - an npm module - to help us with it. npm modules are installed using npm, which is another program, automatically installed with Node.js.\nThe npmjs.com registry offers a huge collection of open-source libraries for Node.js. You can (and you should) utilize it to save time and tap into the amazing open-source community around JavaScript and Node.js.\nCreating a new project with npm​\nBefore we can install npm modules, we need to create an npm project. To do that, you can create a new directory or use the one that you already have open in VSCode (you can delete the hello.js file now) and from that directory run this command in your terminal:\nIt will set up an empty npm project for you and create a file called package.json. This is a very important file in Node.js programming as it contains information about the project.\nUse modern JavaScript​\nNode.js and npm support two types of projects, let's call them legacy and modern. For backwards compatibility, the legacy version is used by default. To switch to the modern version, open your package.json and add this line to the end of the JSON object. Don't forget to add a comma to the end of the previous line 😉\nMore recent versions of npm might already have \"type\": \"commonjs\", pre-defined; if so, simply replace commonjs with module.\nIf you want to learn more about JSON and its syntax, we recommend this tutorial on MDN.\nInstalling necessary libraries​\nNow that we have a project set up, we can install npm modules into the project. Let's install libraries that will help us with downloading and processing websites' HTML. In the project directory, run the following command, which will install two libraries into your project. got-scraping and Cheerio.\nnpm install got-scraping cheerio\ngot-scraping is a library that's made especially for scraping and downloading page's HTML. It's based on the popular got library, which means any features of got are also available in got-scraping. Both got and got-scraping are HTTP clients. To learn more about HTTP, visit this MDN tutorial.\nCheerio is a popular Node.js library for parsing and processing HTML. If you know how to work with jQuery, you'll find Cheerio familiar.\nTest everything​\nWith the libraries installed, create a new file in the project's folder called main.js. This is where we will put all our code. Before we start scraping, though, let's do a check that everything was installed correctly. Add this piece of code inside main.js.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconsole.log('it works!');\nThose import statements tell Node.js that it should give you access to the got-scraping library under the gotScraping variable and the Cheerio library under the cheerio variable.\nNow run this command in your terminal:\nIf you see it works! printed in your terminal, great job! You set up everything correctly. If you see an error that says Cannot use import statement outside a module, go back to the Use modern JavaScript paragraph and add the type property to your package.json. If you see a different error, try copying and pasting it into Google, and you'll find a solution soon.\nNext up​\nWith the project set up, the next lesson will show you how to use got-scraping to download the website's HTML and extract data from it with Cheerio.",
  "markdown": "# Project setup | Academy | Apify Documentation\n\n## Setting up your project\n\n**Create a new project with npm and Node.js. Install necessary libraries, and test that everything works before starting the next lesson.**\n\n* * *\n\nWhen you open a website in a browser, the browser first downloads the page's HTML. To do the same thing with Node.js, we will install a program - an npm module - to help us with it. npm modules are installed using `npm`, which is another program, automatically installed with Node.js.\n\n> The [npmjs.com](https://www.npmjs.com/) registry offers a huge collection of open-source libraries for Node.js. You can (and you should) utilize it to save time and tap into the amazing open-source community around JavaScript and Node.js.\n\n## Creating a new project with npm[​](#creating-a-project \"Direct link to Creating a new project with npm\")\n\nBefore we can install npm modules, we need to create an npm project. To do that, you can create a new directory or use the one that you already have open in VSCode (you can delete the **hello.js** file now) and from that directory run this command in your terminal:\n\nIt will set up an empty npm project for you and create a file called **package.json**. This is a very important file in Node.js programming as it contains information about the project.\n\n![npm init with VSCode](https://docs.apify.com/assets/images/vscode-npm-init-9a14ce01ee1ea6ec3e6d8c9bb7b27209.png)\n\n### Use modern JavaScript[​](#modern-javascript \"Direct link to Use modern JavaScript\")\n\nNode.js and npm support two types of projects, let's call them legacy and modern. For backwards compatibility, the legacy version is used by default. To switch to the modern version, open your **package.json** and add this line to the end of the JSON object. Don't forget to add a comma to the end of the previous line 😉\n\n![Update package.json with VSCode](https://docs.apify.com/assets/images/vscode-type-module-4981eb4a264712ba7e0a951578b13c10.png)\n\n> More recent versions of npm might already have `\"type\": \"commonjs\",` pre-defined; if so, simply replace `commonjs` with `module`.\n\nIf you want to learn more about JSON and its syntax, we recommend [this tutorial on MDN](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON).\n\n## Installing necessary libraries[​](#install-libraries \"Direct link to Installing necessary libraries\")\n\nNow that we have a project set up, we can install npm modules into the project. Let's install libraries that will help us with downloading and processing websites' HTML. In the project directory, run the following command, which will install two libraries into your project. **got-scraping** and Cheerio.\n\n```\nnpm install got-scraping cheerio\n```\n\n[**got-scraping**](https://github.com/apify/got-scraping) is a library that's made especially for scraping and downloading page's HTML. It's based on the popular [**got** library](https://github.com/sindresorhus/got), which means any features of **got** are also available in **got-scraping**. Both **got** and **got-scraping** are HTTP clients. To learn more about HTTP, [visit this MDN tutorial](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP).\n\n[Cheerio](https://github.com/cheeriojs/cheerio) is a popular Node.js library for parsing and processing HTML. If you know how to work with [jQuery](https://jquery.com/), you'll find Cheerio familiar.\n\n## Test everything[​](#testing \"Direct link to Test everything\")\n\nWith the libraries installed, create a new file in the project's folder called **main.js**. This is where we will put all our code. Before we start scraping, though, let's do a check that everything was installed correctly. Add this piece of code inside **main.js**.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';console.log('it works!');\n```\n\nThose `import` statements tell Node.js that it should give you access to the **got-scraping** library under the `gotScraping` variable and the Cheerio library under the `cheerio` variable.\n\nNow run this command in your terminal:\n\nIf you see **it works!** printed in your terminal, great job! You set up everything correctly. If you see an error that says _Cannot use import statement outside a module_, go back to the [Use modern JavaScript](#modern-javascript) paragraph and add the `type` property to your **package.json**. If you see a different error, try copying and pasting it into Google, and you'll find a solution soon.\n\n![Test your setup with VSCode](https://docs.apify.com/assets/images/vscode-test-setup-ee57f6c24936b1661402600019b6373a.png)\n\n## Next up[​](#next \"Direct link to Next up\")\n\nWith the project set up, the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-js-scraper) will show you how to use **got-scraping** to download the website's HTML and extract data from it with Cheerio.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/devtools-continued",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/devtools-continued",
    "loadedTime": "2025-07-21T15:50:23.919Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/devtools-continued",
    "title": "Extracting data with DevTools | Academy | Apify Documentation",
    "description": "Continue learning how to extract data from a website using browser DevTools, CSS selectors, and JavaScript via the DevTools console.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/devtools-continued"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Extracting data with DevTools | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Continue learning how to extract data from a website using browser DevTools, CSS selectors, and JavaScript via the DevTools console."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Extracting+data+with+DevTools"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "10460",
      "date": "Mon, 21 Jul 2025 15:50:11 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-bf34\"",
      "expires": "Mon, 21 Jul 2025 16:00:11 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "2F00:966A0:10B7B9:11C332:687E61B2",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000133-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113011.074925, VS0, VE529",
      "x-fastly-request-id": "2118fed23cee7a2f784a8f4d9ba508f163d47b9b",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "U6wY0qLQ2UYmI7PoPW9wCdERleXoON8rUglwPglGTjeufUcec2OLFA==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Extracting data with DevTools | Academy\nContinue learning how to extract data from a website using browser DevTools, CSS selectors, and JavaScript via the DevTools console.\nIn the previous parts of the DevTools tutorial, we were able to extract information about a single product from the Sales collection of the Warehouse store. If you missed the previous lessons, please go through them to understand the basic concepts. You don't need any of the code from there, though. We will start from scratch.\nFind all product elements​\nFirst, we will use the querySelectorAll() function from the previous lessons to get a list of all the product elements.\nRun this command in your Console:\nconst products = document.querySelectorAll('.product-item');\nproducts.length;\nThe length property of products tells us how many products we have in the list. It says 24 and if you count the number of products on the page, you'll find that it's correct. Good, that means our CSS selector is working perfectly to get all the products.\nLooping over elements​\nVisit this tutorial if you need to refresh the concept of loops in programming.\nNow, we will loop over each product and print their titles. We will use a so-called for..of loop to do it. It is a loop that iterates through all items of an array.\nRun the following command in the Console. Some notes:\nThe a.product-item__title selector and the extraction code come from the previous lesson.\nThe console.log() function prints the results to the Console.\nThe trim() function makes sure there are no useless whitespace characters around our data.\nfor (const product of products) {\nconst titleElement = product.querySelector('a.product-item__title');\nconst title = titleElement.textContent.trim();\nconsole.log(title);\n}\nLearn more about the for..of loop.\nWe will add the price extraction from the previous lesson to the loop. We will also save all the data to an array so that we can work with it. Run this in the Console:\nThe results.push() function takes its argument and pushes (adds) it to the results array. Learn more about it here.\nconst results = [];\n\nfor (const product of products) {\nconst titleElement = product.querySelector('a.product-item__title');\nconst title = titleElement.textContent.trim();\n\nconst priceElement = product.querySelector('span.price');\nconst price = priceElement.childNodes[2].nodeValue.trim();\n\nresults.push({ title, price });\n}\nAfter running the code, you'll see 24 printed to the Console. That's because the results array includes 24 products.\nNow, run this command in the Console to print all the products:\nYou may notice that some prices include the word From, indicating that the price is not final. If you wanted to process this data further, you would want to remove this from the price and instead save this information to another field.\nSummary​\nLet's recap the web scraping process. First, we used DevTools to find the element that holds data about a single product. Then, inside this parent element we found child elements that contained the data (title, price) we were looking for.\nSecond, we used the document.querySelector() function and its All variant to find the data programmatically, using their CSS selectors.\nAnd third, we wrapped this data extraction logic in a loop to automatically find the data not only for a single product, but for all the products on the page. 🎉\nNext up​\nAnd that's it! With a bit of trial and error, you will be able to extract data from any webpage that's loaded in your browser. This is a useful skill on its own. It will save you time copy-pasting stuff when you need data for a project.\nMore importantly though, it taught you the basics to start programming your own scrapers. In the next lessons, we will teach you how to create your own web data extraction script using JavaScript and Node.js.",
  "markdown": "# Extracting data with DevTools | Academy\n\n**Continue learning how to extract data from a website using browser DevTools, CSS selectors, and JavaScript via the DevTools console.**\n\n* * *\n\nIn the previous parts of the DevTools tutorial, we were able to extract information about a single product from the Sales collection of the [Warehouse store](https://warehouse-theme-metal.myshopify.com/collections/sales). If you missed the previous lessons, please go through them to understand the basic concepts. You don't need any of the code from there, though. We will start from scratch.\n\n## Find all product elements[​](#find-all-products \"Direct link to Find all product elements\")\n\nFirst, we will use the `querySelectorAll()` function from the previous lessons to get a list of all the product elements.\n\nRun this command in your Console:\n\n```\nconst products = document.querySelectorAll('.product-item');products.length;\n```\n\nThe `length` property of `products` tells us how many products we have in the list. It says **24** and if you count the number of products on the page, you'll find that it's correct. Good, that means our CSS selector is working perfectly to get all the products.\n\n![Print all products](https://docs.apify.com/assets/images/devtools-count-products-d590f5142b901919a0c31b50df5b0538.png)\n\n## Looping over elements[​](#looping-over-elements \"Direct link to Looping over elements\")\n\n> [Visit this tutorial](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Loops_and_iteration) if you need to refresh the concept of loops in programming.\n\nNow, we will loop over each product and print their titles. We will use a so-called `for..of` loop to do it. It is a loop that iterates through all items of an array.\n\nRun the following command in the Console. Some notes:\n\n*   The `a.product-item__title` selector and the extraction code come from the previous lesson.\n*   The `console.log()` function prints the results to the Console.\n*   The `trim()` function makes sure there are no useless whitespace characters around our data.\n\n```\nfor (const product of products) {    const titleElement = product.querySelector('a.product-item__title');    const title = titleElement.textContent.trim();    console.log(title);}\n```\n\n> [Learn more](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...of) about the `for..of` loop.\n\n![Print all products&#39; text](https://docs.apify.com/assets/images/devtools-product-titles-707d5bbdd367173c973801350f61e859.png)\n\nWe will add the price extraction from the previous lesson to the loop. We will also save all the data to an array so that we can work with it. Run this in the Console:\n\n> The `results.push()` function takes its argument and pushes (adds) it to the `results` array. [Learn more about it here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/push).\n\n```\nconst results = [];for (const product of products) {    const titleElement = product.querySelector('a.product-item__title');    const title = titleElement.textContent.trim();    const priceElement = product.querySelector('span.price');    const price = priceElement.childNodes[2].nodeValue.trim();    results.push({ title, price });}\n```\n\nAfter running the code, you'll see **24** printed to the Console. That's because the `results` array includes 24 products.\n\nNow, run this command in the Console to print all the products:\n\n![Print all products&#39; data](https://docs.apify.com/assets/images/devtools-print-all-products-79895abb91e05c4de5321f849b084bbf.png)\n\n> You may notice that some prices include the word **From**, indicating that the price is not final. If you wanted to process this data further, you would want to remove this from the price and instead save this information to another field.\n\n## Summary[​](#summary \"Direct link to Summary\")\n\nLet's recap the web scraping process. First, we used DevTools to **find the element** that holds data about a single product. Then, inside this **parent** element we **found child elements** that contained the data (title, price) we were looking for.\n\nSecond, we used the `document.querySelector()` function and its `All` variant to **find the data programmatically**, using their **CSS selectors**.\n\nAnd third, we wrapped this data extraction logic in a **loop** to automatically find the data not only for a single product, but for **all the products** on the page. 🎉\n\n## Next up[​](#next \"Direct link to Next up\")\n\nAnd that's it! With a bit of trial and error, you will be able to extract data from any webpage that's loaded in your browser. This is a useful skill on its own. It will save you time copy-pasting stuff when you need data for a project.\n\nMore importantly though, it taught you the basics to start programming your own scrapers. In the [next lessons](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/computer-preparation), we will teach you how to create your own web data extraction script using JavaScript and Node.js.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/computer-preparation",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/computer-preparation",
    "loadedTime": "2025-07-21T15:50:24.028Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/computer-preparation",
    "title": "Computer preparation | Academy | Apify Documentation",
    "description": "Set up your computer to be able to code scrapers with Node.js and JavaScript. Download Node.js and npm and run a Hello World script.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/computer-preparation"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Computer preparation | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Set up your computer to be able to code scrapers with Node.js and JavaScript. Download Node.js and npm and run a Hello World script."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Computer+preparation"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "9463",
      "date": "Mon, 21 Jul 2025 15:50:12 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-95cf\"",
      "expires": "Mon, 21 Jul 2025 16:00:12 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "4EFF:42E5F:1382A2:14F206:687E61B3",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100159-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113012.069960, VS0, VE14",
      "x-fastly-request-id": "fff2a00671f6ab8c090456edbdeadff6950ef264",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "Tgjjz_H8nHzSSGQh8hvK7ZWPr9RRlO4zqeH-GO08t41x9JoBHD4eEQ==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Computer preparation | Academy | Apify Documentation\nPrepare your computer for programming\nSet up your computer to be able to code scrapers with Node.js and JavaScript. Download Node.js and npm and run a Hello World script.\nBefore you can start writing scraper code, you need to have your computer set up for it. In this lesson, we will show you all the tools you need to install to successfully write your first scraper.\nInstall Node.js​\nLet's start with the installation of Node.js. Node.js is an engine for running JavaScript, quite similar to the browser console we used in the previous lessons. You feed it JavaScript code, and it executes it for you. Why not just use the browser console? Because it's limited in its capabilities. Node.js is way more powerful and is much better suited for coding scrapers.\nIf you're on macOS, use this tutorial to install Node.js. If you're using Windows visit the official Node.js website. And if you're on Linux, use your package manager to install nodejs.\nInstall a text editor​\nMany text editors are available for you to choose from when programming. You might already have a preferred one so feel free to use that. Make sure it has syntax highlighting and support for Node.js. If you don't have a text editor, we suggest starting with VSCode. It's free, very popular, and well maintained. Download it here.\nOnce you downloaded and installed it, you can open a folder where we will build your scraper. We recommend starting with a new, empty folder.\nHello world! 👋​\nBefore we start, let's confirm that Node.js was successfully installed on your computer. To do that, run those two commands in your terminal and see if they correctly print your Node.js and npm versions. The next lessons require Node.js version 16 or higher. If you skipped Node.js installation and want to use your existing version of Node.js, make sure that it's 16 or higher.\nIf you installed VSCode in the previous paragraph, you can use the integrated terminal.\nIf you're still wondering what a \"terminal\" is, we suggest googling for a terminal tutorial for your operating system because individual terminals are different. Sometimes a little, sometimes a lot.\nAfter confirming that node is correctly installed on your computer, use your text editor to create a file called hello.js in your folder.\nNow add this piece of code to hello.js and save the file.\nconsole.log('Hello World');\nFinally, run the below command in your terminal:\nYou should see Hello World printed in your terminal. If you do, congratulations, you are now officially a programmer! 🚀\nNext up​\nYou have your computer set up correctly for development, and you've run your first script. Great! In the next lesson we'll set up your project to download a website's HTML using Node.js instead of a browser.",
  "markdown": "# Computer preparation | Academy | Apify Documentation\n\n## Prepare your computer for programming\n\n**Set up your computer to be able to code scrapers with Node.js and JavaScript. Download Node.js and npm and run a Hello World script.**\n\n* * *\n\nBefore you can start writing scraper code, you need to have your computer set up for it. In this lesson, we will show you all the tools you need to install to successfully write your first scraper.\n\n## Install Node.js[​](#install-node \"Direct link to Install Node.js\")\n\nLet's start with the installation of Node.js. Node.js is an engine for running JavaScript, quite similar to the browser console we used in the previous lessons. You feed it JavaScript code, and it executes it for you. Why not just use the browser console? Because it's limited in its capabilities. Node.js is way more powerful and is much better suited for coding scrapers.\n\nIf you're on macOS, use [this tutorial to install Node.js](https://blog.apify.com/how-to-install-nodejs/). If you're using Windows [visit the official Node.js website](https://nodejs.org/en/download/). And if you're on Linux, use your package manager to install `nodejs`.\n\n## Install a text editor[​](#install-an-editor \"Direct link to Install a text editor\")\n\nMany text editors are available for you to choose from when programming. You might already have a preferred one so feel free to use that. Make sure it has syntax highlighting and support for Node.js. If you don't have a text editor, we suggest starting with VSCode. It's free, very popular, and well maintained. [Download it here](https://code.visualstudio.com/download).\n\nOnce you downloaded and installed it, you can open a folder where we will build your scraper. We recommend starting with a new, empty folder.\n\n![How to open a folder in VSCode](https://docs.apify.com/assets/images/vscode-open-folder-4fe8ed6d37a7d37b1c2d8c9356b7a8bb.png)\n\n## Hello world! 👋[​](#hello-world \"Direct link to Hello world! 👋\")\n\nBefore we start, let's confirm that Node.js was successfully installed on your computer. To do that, run those two commands in your terminal and see if they correctly print your Node.js and npm versions. The next lessons **require Node.js version 16 or higher**. If you skipped Node.js installation and want to use your existing version of Node.js, **make sure that it's 16 or higher**.\n\nIf you installed VSCode in the previous paragraph, you can use the integrated terminal.\n\n![How to open a terminal in VSCode](https://docs.apify.com/assets/images/vscode-open-terminal-44dc7539448cf0e3c67f123f664dbfeb.png)\n\n> If you're still wondering what a \"terminal\" is, we suggest googling for a terminal tutorial for your operating system because individual terminals are different. Sometimes a little, sometimes a lot.\n\nAfter confirming that `node` is correctly installed on your computer, use your text editor to create a file called **hello.js** in your folder.\n\n![How to create a file in VSCode](https://docs.apify.com/assets/images/vscode-create-file-85dd6193a61846dcc6bc584b9c83ef6d.png)\n\nNow add this piece of code to **hello.js** and save the file.\n\n```\nconsole.log('Hello World');\n```\n\nFinally, run the below command in your terminal:\n\nYou should see **Hello World** printed in your terminal. If you do, congratulations, you are now officially a programmer! 🚀\n\n![Hello world in VSCode](https://docs.apify.com/assets/images/vscode-hello-world-993a4d46e1828928f34c468db5bf5810.png)\n\n## Next up[​](#next \"Direct link to Next up\")\n\nYou have your computer set up correctly for development, and you've run your first script. Great! In the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup) we'll set up your project to download a website's HTML using Node.js instead of a browser.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-continued",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-continued",
    "loadedTime": "2025-07-21T15:50:24.512Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-continued",
    "title": "Extracting data with Node.js | Academy | Apify Documentation",
    "description": "Continue learning how to create a web scraper with Node.js and Cheerio. Learn how to parse HTML and print the results of the data your scraper has collected.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-continued"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Extracting data with Node.js | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Continue learning how to create a web scraper with Node.js and Cheerio. Learn how to parse HTML and print the results of the data your scraper has collected."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Extracting+data+with+Node.js"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "11781",
      "date": "Mon, 21 Jul 2025 15:50:13 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-13f1a\"",
      "expires": "Mon, 21 Jul 2025 16:00:13 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "6CEB:B33ED:11F68D:1302B4:687E61B5",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000068-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113013.362160, VS0, VE13",
      "x-fastly-request-id": "88e8006380ff019a9d3a9dade0a8cfb278e7e29f",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "_8t55Ue8TYZIbEu-5dIAl_HCL3JJlc_bGGtAaw_DH9O8odl4MmJkUg==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Extracting data with Node.js | Academy\nContinue learning how to create a web scraper with Node.js and Cheerio. Learn how to parse HTML and print the results of the data your scraper has collected.\nIn the first part of the Node.js tutorial we downloaded the HTML of our Warehouse store and parsed it with Cheerio. Now, we will replicate the extraction logic from the Extracting Data with DevTools lessons and finish our scraper.\nQuerying data with Cheerio​\nAs a reminder, the data we need for each product on the page is available in the elements that have the product-item class attribute.\nTo get all the elements with that class using Cheerio, we call the $ function with the appropriate CSS selector. Same as we would with the document.querySelectorAll() function.\n// In browser DevTools Console\nconst products = document.querySelectorAll('.product-item');\n// In Node.js with Cheerio\nconst products = $('.product-item');\nWe will use the same approach as in the previous DevTools lessons. Using a for..of loop we will iterate over the list of products we saved in the products variable. The code is a little different from DevTools, because we're using Node.js and Cheerio instead of a browser's native DOM manipulation functions, but the principle is exactly the same.\nReplace the code in your main.js with the following, and run it with node main.js in your terminal.\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\n\n// Find all products on the page\nconst products = $('.product-item');\n\n// Loop through all the products\n// and print their text to terminal\nfor (const product of products) {\nconst productElement = $(product);\nconst productText = productElement.text();\n\nconsole.log(productText);\n}\nAfter you run this script, you will see data of all the 24 products printed in your terminal. The output will be messy, but that's ok. Next, we will clean it.\nTo clean the output, we need to repeat the process from the DevTools lessons and add individual data point extraction to the loop. From those lessons, we know that each of our product cards includes an <a class=\"product-item__title ...\"> element which holds the product's title, and a <span class=\"price ...\"> element which includes the product's price.\nWe will loop over all the products and extract the data points from each of them using the for..of loop. For reference, this a part of the code from the DevTools lesson, where we collected the data using the browser DevTools Console:\n// This code will only work in the browser, and NOT in Node.js\nconst results = [];\n\nfor (const product of products) {\nconst titleElement = product.querySelector('a.product-item__title');\nconst title = titleElement.textContent.trim();\n\nconst priceElement = subwoofer.querySelector('span.price');\nconst price = priceElement.childNodes[2].nodeValue.trim();\n\nresults.push({ title, price });\n}\nAnd this snippet shows the same piece of code when using Node.js and Cheerio:\nconst results = [];\n\nfor (const product of products) {\nconst titleElement = $(product).find('a.product-item__title');\nconst title = titleElement.text().trim();\n\nconst priceElement = $(product).find('span.price');\nconst price = priceElement.contents()[2].nodeValue.trim();\n\nresults.push({ title, price });\n}\nThe main difference is that we used the .find() function to select the title and price elements and also the .contents() function instead of the childNodes attribute. If you find the differences confusing, don't worry about it. It will begin to feel very natural after a bit of practice.\nThe final scraper code looks like this. Replace the code in your main.js file with this code and run it using node main.js in your terminal.\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\n\n// Find all products on the page\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\nconst titleElement = $(product).find('a.product-item__title');\nconst title = titleElement.text().trim();\n\nconst priceElement = $(product).find('span.price');\nconst price = priceElement.contents()[2].nodeValue.trim();\n\nresults.push({ title, price });\n}\n\nconsole.log(results);\nAfter running the code, you will see this output in your terminal:\n[\n{\ntitle: 'JBL Flip 4 Waterproof Portable Bluetooth Speaker',\nprice: '$74.95',\n},\n{\ntitle: 'Sony XBR-950G BRAVIA 4K HDR Ultra HD TV',\nprice: 'From $1,398.00',\n},\n{\ntitle: 'Sony SACS9 10\" Active Subwoofer',\nprice: '$158.00',\n},\n{\ntitle: 'Sony PS-HX500 Hi-Res USB Turntable',\nprice: '$398.00',\n},\n{\ntitle: 'Klipsch R-120SW Powerful Detailed Home Speaker - Unit',\nprice: '$324.00',\n},\n// ...and more\n];\nCongratulations! You completed the Basics of data extraction section of the Web scraping basics for JavaScript devs course. A quick recap of what you learned:\nThe basic terminology around web scraping, crawling, HTML, CSS and JavaScript.\nHow to use browser DevTools and Console to inspect web pages and manipulate them using CSS and JavaScript.\nHow to install Node.js and set up your computer for building scrapers.\nHow to download a website's HTML using Got Scraping and then parse it using Cheerio to extract valuable data.\nGreat job! 👏🎉\nNext up\nWhat's next? While we were able to extract the data, it's not super useful to have it printed to the terminal. In the next, bonus lesson, we will learn how to convert the data to a CSV and save it to a file.",
  "markdown": "# Extracting data with Node.js | Academy\n\n**Continue learning how to create a web scraper with Node.js and Cheerio. Learn how to parse HTML and print the results of the data your scraper has collected.**\n\n* * *\n\nIn the first part of the Node.js tutorial we downloaded the HTML of our [Warehouse store](https://warehouse-theme-metal.myshopify.com/collections/sales) and parsed it with Cheerio. Now, we will replicate the extraction logic from the [Extracting Data with DevTools](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools) lessons and finish our scraper.\n\n## Querying data with Cheerio[​](#querying-with-cheerio \"Direct link to Querying data with Cheerio\")\n\nAs a reminder, the data we need for each product on the page is available in the elements that have the `product-item` class attribute.\n\n![Selecting an element from the Elements tab](https://docs.apify.com/assets/images/devtools-collection-class-0c9aab05cfd98e0e534e0c9adbac28da.png)\n\nTo get all the elements with that class using Cheerio, we call the `$` function with the appropriate CSS selector. Same as we would with the `document.querySelectorAll()` function.\n\n```\n// In browser DevTools Consoleconst products = document.querySelectorAll('.product-item');\n```\n\n```\n// In Node.js with Cheerioconst products = $('.product-item');\n```\n\nWe will use the same approach as in the previous DevTools lessons. Using a `for..of` loop we will iterate over the list of products we saved in the `products` variable. The code is a little different from DevTools, because we're using Node.js and Cheerio instead of a browser's native DOM manipulation functions, but the principle is exactly the same.\n\nReplace the code in your **main.js** with the following, and run it with `node main.js` in your terminal.\n\n```\n// main.jsimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';// Download HTML with Got Scrapingconst response = await gotScraping(storeUrl);const html = response.body;// Parse HTML with Cheerioconst $ = cheerio.load(html);// Find all products on the pageconst products = $('.product-item');// Loop through all the products// and print their text to terminalfor (const product of products) {    const productElement = $(product);    const productText = productElement.text();    console.log(productText);}\n```\n\nAfter you run this script, you will see data of all the 24 products printed in your terminal. The output will be messy, but that's ok. Next, we will clean it.\n\nTo clean the output, we need to repeat the process from the DevTools lessons and add individual data point extraction to the loop. From those lessons, we know that each of our product cards includes an `<a class=\"product-item__title ...\">` element which holds the product's title, and a `<span class=\"price ...\">` element which includes the product's price.\n\n![Finding child elements in Elements tab](https://docs.apify.com/assets/images/devtools-find-child-elements-c503170e9925d6e564f97317bf7503b6.png)\n\nWe will loop over all the `products` and extract the data points from each of them using the `for..of` loop. For reference, this a part of the code from the DevTools lesson, where we collected the data using the browser **DevTools Console**:\n\n```\n// This code will only work in the browser, and NOT in Node.jsconst results = [];for (const product of products) {    const titleElement = product.querySelector('a.product-item__title');    const title = titleElement.textContent.trim();    const priceElement = subwoofer.querySelector('span.price');    const price = priceElement.childNodes[2].nodeValue.trim();    results.push({ title, price });}\n```\n\nAnd this snippet shows the same piece of code when using **Node.js and Cheerio**:\n\n```\nconst results = [];for (const product of products) {    const titleElement = $(product).find('a.product-item__title');    const title = titleElement.text().trim();    const priceElement = $(product).find('span.price');    const price = priceElement.contents()[2].nodeValue.trim();    results.push({ title, price });}\n```\n\nThe main difference is that we used the [`.find()`](https://cheerio.js.org/classes/Cheerio.html#find) function to select the title and price elements and also the `.contents()` function instead of the `childNodes` attribute. If you find the differences confusing, don't worry about it. It will begin to feel very natural after a bit of practice.\n\nThe final scraper code looks like this. Replace the code in your **main.js** file with this code and run it using `node main.js` in your terminal.\n\n```\n// main.jsimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';// Download HTML with Got Scrapingconst response = await gotScraping(storeUrl);const html = response.body;// Parse HTML with Cheerioconst $ = cheerio.load(html);// Find all products on the pageconst products = $('.product-item');const results = [];for (const product of products) {    const titleElement = $(product).find('a.product-item__title');    const title = titleElement.text().trim();    const priceElement = $(product).find('span.price');    const price = priceElement.contents()[2].nodeValue.trim();    results.push({ title, price });}console.log(results);\n```\n\nAfter running the code, you will see this output in your terminal:\n\n```\n[    {        title: 'JBL Flip 4 Waterproof Portable Bluetooth Speaker',        price: '$74.95',    },    {        title: 'Sony XBR-950G BRAVIA 4K HDR Ultra HD TV',        price: 'From $1,398.00',    },    {        title: 'Sony SACS9 10\" Active Subwoofer',        price: '$158.00',    },    {        title: 'Sony PS-HX500 Hi-Res USB Turntable',        price: '$398.00',    },    {        title: 'Klipsch R-120SW Powerful Detailed Home Speaker - Unit',        price: '$324.00',    },    // ...and more];\n```\n\nCongratulations! You completed the **Basics of data extraction** section of the Web scraping basics for JavaScript devs course. A quick recap of what you learned:\n\n1.  The basic terminology around web scraping, crawling, HTML, CSS and JavaScript.\n2.  How to use browser DevTools and Console to inspect web pages and manipulate them using CSS and JavaScript.\n3.  How to install Node.js and set up your computer for building scrapers.\n4.  How to download a website's HTML using Got Scraping and then parse it using Cheerio to extract valuable data.\n\nGreat job! 👏🎉\n\n## Next up\n\nWhat's next? While we were able to extract the data, it's not super useful to have it printed to the terminal. In the [next, bonus lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/save-to-csv), we will learn how to convert the data to a CSV and save it to a file.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-js-scraper",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-js-scraper",
    "loadedTime": "2025-07-21T15:50:23.025Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-js-scraper",
    "title": "Scraping with Node.js | Academy | Apify Documentation",
    "description": "Learn how to use JavaScript and Node.js to create a web scraper, plus take advantage of the Cheerio and Got-scraping libraries to make your job easier.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-js-scraper"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Scraping with Node.js | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to use JavaScript and Node.js to create a web scraper, plus take advantage of the Cheerio and Got-scraping libraries to make your job easier."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Scraping+with+Node.js"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "9828",
      "date": "Mon, 21 Jul 2025 15:50:09 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-b5dc\"",
      "expires": "Mon, 21 Jul 2025 16:00:09 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "726B:2CA264:111234:121DCE:687E61B1",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100137-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113010.802574, VS0, VE23",
      "x-fastly-request-id": "047b19bdbb9b28da97b4efe9a45f4ed29ca3e4dd",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "1utWxbHao7j8PupSX-IneL1UV6uigDnrTrN-pvomb-usS2Au8A1plg==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Scraping with Node.js | Academy\nLearn how to use JavaScript and Node.js to create a web scraper, plus take advantage of the Cheerio and Got-scraping libraries to make your job easier.\nFinally, we have everything ready to start scraping! Yes, the setup was a bit daunting, but luckily, you only have to do it once. We have our project, we have our main.js file, so let's add some code to it.\nDownloading HTML​\nWe will use the got-scraping library to download the HTML of products that are on sale in the Warehouse store. We already worked with this page earlier in the Extracting Data with DevTools lessons.\nReplace the contents of your main.js file with this code:\n// main.js\nimport { gotScraping } from 'got-scraping';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\nconsole.log(html);\nNow run the script using the node main.js command from the previous lesson. After a brief moment, you should see the page's HTML printed to your terminal.\ngotScraping is an async function and the await keyword is used to pause execution of the script until it returns the response. If you're new to this, go through an introduction to asynchronous JavaScript.\nParsing HTML​\nHaving the HTML printed to the terminal is not very helpful. To extract the data, we first have to parse it. Parsing the HTML allows us to query the individual HTML elements, similarly to the way we did it in the browser in the Extracting Data with DevTools lessons.\nTo parse the HTML with the cheerio library. Replace the code in your main.js with the following code:\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\n// Download HTML with Got Scraping\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// Parse HTML with Cheerio\nconst $ = cheerio.load(html);\nconst headingElement = $('h1');\nconst headingText = headingElement.text();\n\n// Print page title to terminal\nconsole.log(headingText);\nWhen you run the above script, Sales will be printed to the terminal. That's because it's the heading of the Sales page of the Warehouse Store which is located in a h1 element.\nGreat, we successfully parsed the HTML and extracted the text of the <h1> element from it using Node.js and Cheerio. Let's break the code down.\nThe script first downloaded the page's HTML using the Got Scraping library. Then, it parsed the downloaded html with cheerio using the load() function, and allowed us to work with it using the $ variable (the $ name is an old convention). The next $('h1') function call looked inside the parsed HTML and found the <h1> element. Finally, the script extracted the text from the element using the .text() function and printed it to the terminal with console.log().\n$('h1') is very similar to calling document.querySelector('h1') in the browser and element.text() is similar to element.textContent from the earlier DevTools lessons. Visit the cheerio documentation to learn more about its syntax.\nNext up​\nIn the next lesson we will learn more about Cheerio and use it to extract all the products' data from Fakestore.",
  "markdown": "# Scraping with Node.js | Academy\n\n**Learn how to use JavaScript and Node.js to create a web scraper, plus take advantage of the Cheerio and Got-scraping libraries to make your job easier.**\n\n* * *\n\nFinally, we have everything ready to start scraping! Yes, the setup was a bit daunting, but luckily, you only have to do it once. We have our project, we have our **main.js** file, so let's add some code to it.\n\n## Downloading HTML[​](#downloading-html \"Direct link to Downloading HTML\")\n\nWe will use the `got-scraping` library to download the HTML of products that are [on sale in the Warehouse store](https://warehouse-theme-metal.myshopify.com/collections/sales). We already worked with this page earlier in the [Extracting Data with DevTools](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools) lessons.\n\nReplace the contents of your **main.js** file with this code:\n\n```\n// main.jsimport { gotScraping } from 'got-scraping';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';const response = await gotScraping(storeUrl);const html = response.body;console.log(html);\n```\n\nNow run the script using the `node main.js` command from the previous lesson. After a brief moment, you should see the page's HTML printed to your terminal.\n\n> `gotScraping` is an `async` function and the `await` keyword is used to pause execution of the script until it returns the `response`. If you're new to this, go through an [introduction to asynchronous JavaScript](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous).\n\n## Parsing HTML[​](#parsing-html \"Direct link to Parsing HTML\")\n\nHaving the HTML printed to the terminal is not very helpful. To extract the data, we first have to parse it. Parsing the HTML allows us to query the individual HTML elements, similarly to the way we did it in the browser in the [Extracting Data with DevTools](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/using-devtools) lessons.\n\nTo parse the HTML with the `cheerio` library. Replace the code in your **main.js** with the following code:\n\n```\n// main.jsimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';// Download HTML with Got Scrapingconst response = await gotScraping(storeUrl);const html = response.body;// Parse HTML with Cheerioconst $ = cheerio.load(html);const headingElement = $('h1');const headingText = headingElement.text();// Print page title to terminalconsole.log(headingText);\n```\n\nWhen you run the above script, **Sales** will be printed to the terminal. That's because it's the heading of the Sales page of the Warehouse Store which is located in a `h1` element.\n\n![Scraping page heading](https://docs.apify.com/assets/images/node-scraper-title-be9c173ce57d6c4af63aec0ed0c9bcdd.png)\n\nGreat, we successfully parsed the HTML and extracted the text of the `<h1>` element from it using Node.js and Cheerio. Let's break the code down.\n\nThe script first downloaded the page's HTML using the Got Scraping library. Then, it parsed the downloaded `html` with `cheerio` using the `load()` function, and allowed us to work with it using the `$` variable (the `$` name is an old convention). The next `$('h1')` function call looked inside the parsed HTML and found the `<h1>` element. Finally, the script extracted the text from the element using the `.text()` function and printed it to the terminal with `console.log()`.\n\n> `$('h1')` is very similar to calling `document.querySelector('h1')` in the browser and `element.text()` is similar to `element.textContent` from the earlier DevTools lessons. [Visit the cheerio documentation](https://github.com/cheeriojs/cheerio#readme) to learn more about its syntax.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nIn the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-continued) we will learn more about Cheerio and use it to extract all the products' data from Fakestore.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/save-to-csv",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/save-to-csv",
    "loadedTime": "2025-07-21T15:50:38.601Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/save-to-csv",
    "title": "Saving results to CSV | Academy | Apify Documentation",
    "description": "Learn how to save the results of your scraper's collected data to a CSV file that can be opened in Excel, Google Sheets, or any other spreadsheets program.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/save-to-csv"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Saving results to CSV | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to save the results of your scraper's collected data to a CSV file that can be opened in Excel, Google Sheets, or any other spreadsheets program."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Saving+results+to+CSV"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "11220",
      "date": "Mon, 21 Jul 2025 15:50:34 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-110b0\"",
      "expires": "Mon, 21 Jul 2025 16:00:34 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "151E:3D948A:11F73C:1304FA:687E61C9",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100122-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113034.298558, VS0, VE14",
      "x-fastly-request-id": "fb9fd936f289b247cc3a3ec081edb9da86fe4bb2",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "ljArNkF5dqWllJz_knbMv7lt8io_dh894Kb2epa9Tpq120GMyI4VAA==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Saving results to CSV | Academy\nLearn how to save the results of your scraper's collected data to a CSV file that can be opened in Excel, Google Sheets, or any other spreadsheets program.\nIn the last lesson, we were able to extract data about all the on-sale products from Warehouse Store. That's great. But we ended up with results printed to the terminal, which is not very useful for further processing. In this lesson, we'll learn how to save that data into a CSV file that you can then open in Excel or Google Sheets.\nConverting to CSV​\nIt might look like a big programming challenge to transform a JavaScript object into a CSV, but thanks to npm, this is going to be a walk in the park. Google search json to csv npm. You will find that there's a library called json2csv that can convert a JavaScript object to CSV format with a single function call. Perfect!\nTo install json2csv, run this command in your terminal. You need to be in the project's folder - the folder which has the package.json file.\nFirst, we need to import the parse() function from the library.\nimport { parse } from 'json2csv';\nNext, we need to parse the results array from the previous lesson with the imported function.\nconst csv = parse(results);\nThe full code including the earlier scraping part now looks like this. Replace the contents of your main.js file with this code:\n// main.js\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv'; // <---- added a new import\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\nconst titleElement = $(product).find('a.product-item__title');\nconst title = titleElement.text().trim();\n\nconst priceElement = $(product).find('span.price');\nconst price = priceElement.contents()[2].nodeValue.trim();\n\nresults.push({ title, price });\n}\n\nconst csv = parse(results); // <---- added parsing of results to CSV\nconsole.log(csv);\nNow run the script with node main.js. The newly created CSV will be printed to the terminal.\n\"title\",\"price\"\n\"JBL Flip 4 Waterproof Portable Bluetooth Speaker\",\"$74.95\"\n\"Sony XBR-950G BRAVIA 4K HDR Ultra HD TV\",\"From $1,398.00\"\n\"Sony SACS9 10\"\" Active Subwoofer\",\"$158.00\"\n\"Sony PS-HX500 Hi-Res USB Turntable\",\"$398.00\"\n\"Klipsch R-120SW Powerful Detailed Home Speaker - Unit\",\"$324.00\"\n\"Denon AH-C720 In-Ear Headphones\",\"$119.00\"\n\"Sony XBR-85X850F 85-Inch 4K Ultra HD Smart LED TV\",\"$3,498.00\"\n\"Sony XBR-75X850F 75-Inch 4K Ultra HD Smart LED TV\",\"$1,998.00\"\n\"Sony XBR-55A8F 55-Inch 4K Ultra HD Smart BRAVIA OLED TV\",\"$2,298.00\"\n...\nWriting the CSV to a file​\nThe final task that remains is to save our CSV formatted data to a file on our disk, so we can open it or send it to someone. For this, we don't need any extra npm packages because functions for saving files are included in Node.js.\nFirst, we import the writeFileSync function from the fs (file system) package.\nimport { writeFileSync } from 'fs';\nand then call it with a file name and the CSV data.\nwriteFileSync('products.csv', csv);\nWhen we complete the code, it looks like this. Replace the code in your main.js file with this new code.\n// main.js\nimport { writeFileSync } from 'fs'; // <---- added a new import\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst products = $('.product-item');\n\nconst results = [];\nfor (const product of products) {\nconst titleElement = $(product).find('a.product-item__title');\nconst title = titleElement.text().trim();\n\nconst priceElement = $(product).find('span.price');\nconst price = priceElement.contents()[2].nodeValue.trim();\n\nresults.push({ title, price });\n}\n\nconst csv = parse(results);\nwriteFileSync('products.csv', csv); // <---- added writing of CSV to file\nFinally, run it with node main.js in your terminal. After running it, you will find the products.csv file in your project folder. And when you open it with Excel/Google Sheets—voila!\nThis marks the end of the Basics of data extraction section of Web scraping basics for JavaScript devs. If you enjoyed the course, give us a thumbs up down below and if you're eager to learn more...\nNext up​\nNext up are the Basics of crawling. You already know how to build a scraper that finds all the products on sale in the Warehouse Store. In the Basics of crawling section you will learn how to open individual product pages of those products and scrape information that's not available on the listing page, like SKUs, descriptions or reviews.",
  "markdown": "# Saving results to CSV | Academy\n\n**Learn how to save the results of your scraper's collected data to a CSV file that can be opened in Excel, Google Sheets, or any other spreadsheets program.**\n\n* * *\n\nIn the last lesson, we were able to extract data about all the on-sale products from [Warehouse Store](https://warehouse-theme-metal.myshopify.com/collections/sales). That's great. But we ended up with results printed to the terminal, which is not very useful for further processing. In this lesson, we'll learn how to save that data into a CSV file that you can then open in Excel or Google Sheets.\n\n## Converting to CSV[​](#converting-to-csv \"Direct link to Converting to CSV\")\n\nIt might look like a big programming challenge to transform a JavaScript object into a CSV, but thanks to npm, this is going to be a walk in the park. Google search **json to csv npm**. You will find that there's a library called [`json2csv`](https://www.npmjs.com/package/json2csv) that can convert a JavaScript object to CSV format with a single function call. _Perfect!_\n\nTo install `json2csv`, run this command in your terminal. You need to be in the project's folder - the folder which has the `package.json` file.\n\nFirst, we need to import the `parse()` function from the library.\n\n```\nimport { parse } from 'json2csv';\n```\n\nNext, we need to parse the `results` array from the previous lesson with the imported function.\n\n```\nconst csv = parse(results);\n```\n\nThe full code including the earlier scraping part now looks like this. Replace the contents of your **main.js** file with this code:\n\n```\n// main.jsimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';import { parse } from 'json2csv'; // <---- added a new importconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const products = $('.product-item');const results = [];for (const product of products) {    const titleElement = $(product).find('a.product-item__title');    const title = titleElement.text().trim();    const priceElement = $(product).find('span.price');    const price = priceElement.contents()[2].nodeValue.trim();    results.push({ title, price });}const csv = parse(results); // <---- added parsing of results to CSVconsole.log(csv);\n```\n\nNow run the script with `node main.js`. The newly created CSV will be printed to the terminal.\n\n```\n\"title\",\"price\"\"JBL Flip 4 Waterproof Portable Bluetooth Speaker\",\"$74.95\"\"Sony XBR-950G BRAVIA 4K HDR Ultra HD TV\",\"From $1,398.00\"\"Sony SACS9 10\"\" Active Subwoofer\",\"$158.00\"\"Sony PS-HX500 Hi-Res USB Turntable\",\"$398.00\"\"Klipsch R-120SW Powerful Detailed Home Speaker - Unit\",\"$324.00\"\"Denon AH-C720 In-Ear Headphones\",\"$119.00\"\"Sony XBR-85X850F 85-Inch 4K Ultra HD Smart LED TV\",\"$3,498.00\"\"Sony XBR-75X850F 75-Inch 4K Ultra HD Smart LED TV\",\"$1,998.00\"\"Sony XBR-55A8F 55-Inch 4K Ultra HD Smart BRAVIA OLED TV\",\"$2,298.00\"...\n```\n\n## Writing the CSV to a file[​](#writing-to-file \"Direct link to Writing the CSV to a file\")\n\nThe final task that remains is to save our CSV formatted data to a file on our disk, so we can open it or send it to someone. For this, we don't need any extra npm packages because functions for saving files are included in Node.js.\n\nFirst, we import the `writeFileSync` function from the `fs` (file system) package.\n\n```\nimport { writeFileSync } from 'fs';\n```\n\nand then call it with a file name and the CSV data.\n\n```\nwriteFileSync('products.csv', csv);\n```\n\nWhen we complete the code, it looks like this. Replace the code in your **main.js** file with this new code.\n\n```\n// main.jsimport { writeFileSync } from 'fs'; // <---- added a new importimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';import { parse } from 'json2csv';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const products = $('.product-item');const results = [];for (const product of products) {    const titleElement = $(product).find('a.product-item__title');    const title = titleElement.text().trim();    const priceElement = $(product).find('span.price');    const price = priceElement.contents()[2].nodeValue.trim();    results.push({ title, price });}const csv = parse(results);writeFileSync('products.csv', csv); // <---- added writing of CSV to file\n```\n\nFinally, run it with `node main.js` in your terminal. After running it, you will find the **products.csv** file in your project folder. And when you open it with Excel/Google Sheets—voila!\n\n![Displaying CSV data in Google Sheets](https://docs.apify.com/assets/images/csv-data-in-sheets-d1ce507cc6fbdc46c4768d1e08d0076d.png)\n\nThis marks the end of the **Basics of data extraction** section of Web scraping basics for JavaScript devs. If you enjoyed the course, give us a thumbs up down below and if you're eager to learn more...\n\n## Next up[​](#next \"Direct link to Next up\")\n\nNext up are the [**Basics of crawling**](https://docs.apify.com/academy/web-scraping-for-beginners/crawling). You already know how to build a scraper that finds all the products on sale in the [Warehouse Store](https://warehouse-theme-metal.myshopify.com/collections/sales). In the [**Basics of crawling**](https://docs.apify.com/academy/web-scraping-for-beginners/crawling) section you will learn how to open individual product pages of those products and scrape information that's not available on the listing page, like SKUs, descriptions or reviews.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling",
    "loadedTime": "2025-07-21T15:50:38.739Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling",
    "title": "Basics of crawling | Academy | Apify Documentation",
    "description": "Learn how to crawl the web with your scraper. How to extract links and URLs from web pages and how to manage the collected links to visit new pages.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Basics of crawling | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to crawl the web with your scraper. How to extract links and URLs from web pages and how to manage the collected links to visit new pages."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Basics+of+crawling"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "8389",
      "date": "Mon, 21 Jul 2025 15:50:36 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-83d4\"",
      "expires": "Mon, 21 Jul 2025 16:00:36 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "A2C3:142A3B:126A2B:13DC5E:687E61CC",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100080-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113037.579466, VS0, VE14",
      "x-fastly-request-id": "f223efa1fdf31fe282c43ac2cf27ee5db9e69ceb",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "Bqkaz1ay08UutzwmCGfFtrlNGtS4ZHVtzovgavB9equgczdno4AfdA==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Basics of crawling | Academy\nLearn how to crawl the web with your scraper. How to extract links and URLs from web pages and how to manage the collected links to visit new pages.\nWelcome to the second section of our Web scraping basics for JavaScript devs course. In the Basics of data extraction section, we learned how to extract data from a web page. Specifically, a template Shopify site called Warehouse store.\nIn this section, we will take a look at moving between web pages, which we call crawling. We will extract data about all the on-sale products on Warehouse store. To do that, we will need to crawl the individual product pages.\nHow do you crawl?​\nCrawling websites is a fairly straightforward process. We'll start by opening the first web page and extracting all the links (URLs) that lead to the other pages we want to visit. To do that, we'll use the skills learned in the Basics of data extraction course. We'll add some extra filtering to make sure we only get the correct URLs. Then, we'll save those URLs, so in case our scraper crashes with an error, we won't have to extract them again. And, finally, we will visit those URLs one by one.\nAt any point, we can extract URLs, data, or both. Crawling can be separate from data extraction, but it's not a requirement and, in most projects, it's actually easier and faster to do both at the same time. To summarize, it goes like this:\nVisit the start URL.\nExtract new URLs (and data) and save them.\nVisit one of the new-found URLs and save data and/or more URLs from them.\nRepeat 2 and 3 until you have everything you need.\nNext up​\nFirst, let's make sure we all understand the foundations. In the next lesson we will review the scraper code we already have from the Basics of data extraction section of the course.",
  "markdown": "# Basics of crawling | Academy\n\n**Learn how to crawl the web with your scraper. How to extract links and URLs from web pages and how to manage the collected links to visit new pages.**\n\n* * *\n\nWelcome to the second section of our **Web scraping basics for JavaScript devs** course. In the [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) section, we learned how to extract data from a web page. Specifically, a template Shopify site called [Warehouse store](https://warehouse-theme-metal.myshopify.com/).\n\n![on-sale category of Warehouse store](https://docs.apify.com/assets/images/warehouse-store-9982b9901e6a56a6f69035fcd2af4ad5.png)\n\nIn this section, we will take a look at moving between web pages, which we call **crawling**. We will extract data about all the on-sale products on [Warehouse store](https://warehouse-theme-metal.myshopify.com/collections/sales). To do that, we will need to crawl the individual product pages.\n\n## How do you crawl?[​](#how-to-crawl \"Direct link to How do you crawl?\")\n\nCrawling websites is a fairly straightforward process. We'll start by opening the first web page and extracting all the links (URLs) that lead to the other pages we want to visit. To do that, we'll use the skills learned in the [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) course. We'll add some extra filtering to make sure we only get the correct URLs. Then, we'll save those URLs, so in case our scraper crashes with an error, we won't have to extract them again. And, finally, we will visit those URLs one by one.\n\nAt any point, we can extract URLs, data, or both. Crawling can be separate from data extraction, but it's not a requirement and, in most projects, it's actually easier and faster to do both at the same time. To summarize, it goes like this:\n\n1.  Visit the start URL.\n2.  Extract new URLs (and data) and save them.\n3.  Visit one of the new-found URLs and save data and/or more URLs from them.\n4.  Repeat 2 and 3 until you have everything you need.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nFirst, let's make sure we all understand the foundations. In the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/recap-extraction-basics) we will review the scraper code we already have from the [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) section of the course.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/recap-extraction-basics",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/recap-extraction-basics",
    "loadedTime": "2025-07-21T15:50:46.055Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/recap-extraction-basics",
    "title": "Recap - Data extraction | Academy | Apify Documentation",
    "description": "Review our e-commerce website scraper and refresh our memory about its code and the programming techniques we used to extract and save the data.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/recap-extraction-basics"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Recap - Data extraction | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Review our e-commerce website scraper and refresh our memory about its code and the programming techniques we used to extract and save the data."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Recap+-+Data+extraction"
      }
    ],
    "jsonLd": null,
    "headers": {
      "accept-ranges": "bytes",
      "access-control-allow-origin": "*",
      "age": "0",
      "cache-control": "max-age=600",
      "content-type": "text/html; charset=utf-8",
      "date": "Mon, 21 Jul 2025 15:50:46 GMT",
      "etag": "W/\"687e5eba-cfc9\"",
      "expires": "Mon, 21 Jul 2025 16:00:46 GMT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "server": "nginx",
      "strict-transport-security": "max-age=31556952",
      "vary": "Accept-Encoding",
      "via": "1.1 varnish, 1.1 b18bcd54d0f77ca53d7c0ba4b9e54284.cloudfront.net (CloudFront)",
      "x-amz-cf-id": "o-8iufa5H04NoxM2odU6Ottvy10UJ_GPeOPtS-kjL4FxF8OUEiitbg==",
      "x-amz-cf-pop": "IAD89-P2",
      "x-cache": "Miss from cloudfront",
      "x-cache-hits": "0",
      "x-fastly-request-id": "2fdb2c54d663364ae4850c783e27ff67e84040f2",
      "x-frame-options": "SAMEORIGIN",
      "x-github-request-id": "879D:3D3DE9:13D99C:154CF7:687E61D5",
      "x-origin-cache": "HIT",
      "x-proxy-cache": "MISS",
      "x-served-by": "cache-iad-kjyo7100094-IAD",
      "x-timer": "S1753113046.027553,VS0,VE12"
    }
  },
  "screenshotUrl": null,
  "text": "Recap - Data extraction | Academy\nReview our e-commerce website scraper and refresh our memory about its code and the programming techniques we used to extract and save the data.\nWe finished off the first section of the Web scraping basics for JavaScript devs course by creating a web scraper in Node.js. The scraper collected all the on-sale products from Warehouse store. Let's see the code with some comments added.\n// First, we imported all the libraries we needed to\n// download, extract, and convert the data we wanted\nimport { writeFileSync } from 'fs';\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\nimport { parse } from 'json2csv';\n\n// Here, we fetched the website's HTML and saved it to a new variable.\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\n// We used Cheerio, a popular library, to parse (process)\n// the downloaded HTML so that we could manipulate it.\nconst $ = cheerio.load(html);\n\n// Using the .product-item CSS selector, we collected all the HTML\n// elements which contained data about individual products.\nconst products = $('.product-item');\n\n// Then, we prepared a new array to store the results.\nconst results = [];\n\n// And looped over all the elements to extract\n// information about the individual products.\nfor (const product of products) {\n// The product's title was in an <a> element\n// with the CSS class: product-item__title\nconst titleElement = $(product).find('a.product-item__title');\nconst title = titleElement.text().trim();\n// The product's price was in a <span> element\n// with the CSS class: price\nconst priceElement = $(product).find('span.price');\n// Because the <span> also included some useless data,\n// we had to extract the price from a specific HTML node.\nconst price = priceElement.contents()[2].nodeValue.trim();\n\n// We added the data to the results array\n// in the form of an object with keys and values.\nresults.push({ title, price });\n}\n\n// Finally, we formatted the results\n// as a CSV file instead of a JS object\nconst csv = parse(results);\n\n// Then, we saved the CSV to the disk\nwriteFileSync('products.csv', csv);\ncaution\nWe are using JavaScript features like import statements and top-level await. If you see errors like Cannot use import outside of a module, please review the Project setup lesson, where we explain how to enable those features.",
  "markdown": "# Recap - Data extraction | Academy\n\n**Review our e-commerce website scraper and refresh our memory about its code and the programming techniques we used to extract and save the data.**\n\nWe finished off the [first section](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) of the _Web scraping basics for JavaScript devs_ course by creating a web scraper in Node.js. The scraper collected all the on-sale products from [Warehouse store](https://warehouse-theme-metal.myshopify.com/collections/sales). Let's see the code with some comments added.\n\n```\n// First, we imported all the libraries we needed to// download, extract, and convert the data we wantedimport { writeFileSync } from 'fs';import { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';import { parse } from 'json2csv';// Here, we fetched the website's HTML and saved it to a new variable.const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';const response = await gotScraping(storeUrl);const html = response.body;// We used Cheerio, a popular library, to parse (process)// the downloaded HTML so that we could manipulate it.const $ = cheerio.load(html);// Using the .product-item CSS selector, we collected all the HTML// elements which contained data about individual products.const products = $('.product-item');// Then, we prepared a new array to store the results.const results = [];// And looped over all the elements to extract// information about the individual products.for (const product of products) {    // The product's title was in an <a> element    // with the CSS class: product-item__title    const titleElement = $(product).find('a.product-item__title');    const title = titleElement.text().trim();    // The product's price was in a <span> element    // with the CSS class: price    const priceElement = $(product).find('span.price');    // Because the <span> also included some useless data,    // we had to extract the price from a specific HTML node.    const price = priceElement.contents()[2].nodeValue.trim();    // We added the data to the results array    // in the form of an object with keys and values.    results.push({ title, price });}// Finally, we formatted the results// as a CSV file instead of a JS objectconst csv = parse(results);// Then, we saved the CSV to the diskwriteFileSync('products.csv', csv);\n```\n\ncaution\n\nWe are using JavaScript features like `import` statements and top-level `await`. If you see errors like _Cannot use import outside of a module_, please review the [Project setup lesson](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/project-setup#modern-javascript), where we explain how to enable those features.",
  "debug": {
    "requestHandlerMode": "http"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/finding-links",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/finding-links",
    "loadedTime": "2025-07-21T15:50:47.767Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/finding-links",
    "title": "Finding links | Academy | Apify Documentation",
    "description": "Learn what a link looks like in HTML and how to find and extract their URLs when web scraping. Using both DevTools and Node.js.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/finding-links"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Finding links | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn what a link looks like in HTML and how to find and extract their URLs when web scraping. Using both DevTools and Node.js."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Finding+links"
      }
    ],
    "jsonLd": null,
    "headers": {
      "accept-ranges": "bytes",
      "access-control-allow-origin": "*",
      "age": "0",
      "cache-control": "max-age=600",
      "content-type": "text/html; charset=utf-8",
      "date": "Mon, 21 Jul 2025 15:50:47 GMT",
      "etag": "W/\"687e5eba-c3c4\"",
      "expires": "Mon, 21 Jul 2025 16:00:47 GMT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "server": "nginx",
      "strict-transport-security": "max-age=31556952",
      "vary": "Accept-Encoding",
      "via": "1.1 varnish, 1.1 074df32306fddeb7d54ca41312e6888e.cloudfront.net (CloudFront)",
      "x-amz-cf-id": "c_2LTkSjqeVdTlZlv6LSv3dqLiJ34J85juwLoQn2-pbKwYDhVcqjjg==",
      "x-amz-cf-pop": "IAD89-P2",
      "x-cache": "Miss from cloudfront",
      "x-cache-hits": "0",
      "x-fastly-request-id": "d2d884023c0792d2259ae2eb0267caaa0aec1ba5",
      "x-frame-options": "SAMEORIGIN",
      "x-github-request-id": "307A:3CC807:12C219:14354C:687E61D7",
      "x-origin-cache": "HIT",
      "x-proxy-cache": "MISS",
      "x-served-by": "cache-iad-kjyo7100165-IAD",
      "x-timer": "S1753113048.745605,VS0,VE12"
    }
  },
  "screenshotUrl": null,
  "text": "Finding links | Academy | Apify Documentation\nLearn what a link looks like in HTML and how to find and extract their URLs when web scraping using both DevTools and Node.js.\nMany kinds of links exist on the internet, and we'll cover all the types in the advanced Academy courses. For now, let's think of links as HTML anchor elements with <a> tags. A typical link looks like this:\n<a href=\"https://example.com\">This is a link to example.com</a>\nOn a webpage, the link above will look like this: This is a link to example.com When you click it, your browser will navigate to the URL in the <a> tag's href attribute (https://example.com).\nhref means Hypertext REFerence. You don't need to remember this - just know that href typically means some sort of link.\nIf a link is an HTML element, and the URL is an attribute, this means that we can extract links the same way as we extracted data. To test this theory in the browser, we can try running the following code in our DevTools console on any website.\n// Select all the <a> elements.\nconst links = document.querySelectorAll('a');\n// For each of the links...\nfor (const link of links) {\n// get the value of its 'href' attribute...\nconst url = link.href;\n// and print it to console.\nconsole.log(url);\n}\nGo to the Warehouse store Sales category, open the DevTools Console, paste the above code and run it.\nBoom 💥, all the links from the page have now been printed to the console. Most of the links point to other parts of the website, but some links lead to other domains like facebook.com or instagram.com.\nDevTools Console is a fun playground, but Node.js is way more useful. Let's create a new file in our project called crawler.js and add some basic crawling code that prints all the links from the Sales category of Warehouse.\nWe'll start from a boilerplate that's very similar to the scraper we built in Basics of data extraction.\nRun on\nimport * as cheerio from 'cheerio';\nimport { gotScraping } from 'got-scraping';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\n// ------- new code below\n\nconst links = $('a');\n\nfor (const link of links) {\nconst url = $(link).attr('href');\nconsole.log(url);\n}\nAside from importing libraries and downloading HTML, we load the HTML into Cheerio and then use it to retrieve all the <a> elements. After that, we iterate over the collected links and print their href attributes, which we access using the .attr() method.\nWhen you run the above code, you'll see quite a lot of links in the terminal. Some of them may look wrong, because they don't start with the regular https:// protocol. We'll learn what to do with them in the following lessons.\nNext Up​\nThe next lesson will teach you how to select and filter links, so that your crawler will always work only with valid and useful URLs.",
  "markdown": "# Finding links | Academy | Apify Documentation\n\n**Learn what a link looks like in HTML and how to find and extract their URLs when web scraping using both DevTools and Node.js.**\n\n* * *\n\nMany kinds of links exist on the internet, and we'll cover all the types in the advanced Academy courses. For now, let's think of links as [HTML anchor elements](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/a) with `<a>` tags. A typical link looks like this:\n\n```\n<a href=\"https://example.com\">This is a link to example.com</a>\n```\n\nOn a webpage, the link above will look like this: [This is a link to example.com](https://example.com/) When you click it, your browser will navigate to the URL in the `<a>` tag's `href` attribute (`https://example.com`).\n\n> `href` means **H**ypertext **REF**erence. You don't need to remember this - just know that `href` typically means some sort of link.\n\nIf a link is an HTML element, and the URL is an attribute, this means that we can extract links the same way as we extracted data. To test this theory in the browser, we can try running the following code in our DevTools console on any website.\n\n```\n// Select all the <a> elements.const links = document.querySelectorAll('a');// For each of the links...for (const link of links) {    // get the value of its 'href' attribute...    const url = link.href;    // and print it to console.    console.log(url);}\n```\n\nGo to the [Warehouse store Sales category](https://warehouse-theme-metal.myshopify.com/collections/sales), open the DevTools Console, paste the above code and run it.\n\n![links extracted from Warehouse store](https://docs.apify.com/assets/images/warehouse-links-37f7c3164546c93f7b75ca83cf6e0773.png)\n\n**_Boom_** 💥, all the links from the page have now been printed to the console. Most of the links point to other parts of the website, but some links lead to other domains like facebook.com or instagram.com.\n\nDevTools Console is a fun playground, but Node.js is way more useful. Let's create a new file in our project called **crawler.js** and add some basic crawling code that prints all the links from the [Sales category of Warehouse](https://warehouse-theme-metal.myshopify.com/collections/sales).\n\nWe'll start from a boilerplate that's very similar to the scraper we built in [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction/node-js-scraper).\n\n[Run on](https://console.apify.com/actors/kk67IcZkKSSBTslXI?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcImNvZGVcIjpcImltcG9ydCAqIGFzIGNoZWVyaW8gZnJvbSAnY2hlZXJpbyc7XFxuaW1wb3J0IHsgZ290U2NyYXBpbmcgfSBmcm9tICdnb3Qtc2NyYXBpbmcnO1xcblxcbmNvbnN0IHN0b3JlVXJsID0gJ2h0dHBzOi8vd2FyZWhvdXNlLXRoZW1lLW1ldGFsLm15c2hvcGlmeS5jb20vY29sbGVjdGlvbnMvc2FsZXMnO1xcblxcbmNvbnN0IHJlc3BvbnNlID0gYXdhaXQgZ290U2NyYXBpbmcoc3RvcmVVcmwpO1xcbmNvbnN0IGh0bWwgPSByZXNwb25zZS5ib2R5O1xcblxcbmNvbnN0ICQgPSBjaGVlcmlvLmxvYWQoaHRtbCk7XFxuXFxuLy8gLS0tLS0tLSBuZXcgY29kZSBiZWxvd1xcblxcbmNvbnN0IGxpbmtzID0gJCgnYScpO1xcblxcbmZvciAoY29uc3QgbGluayBvZiBsaW5rcykge1xcbiAgICBjb25zdCB1cmwgPSAkKGxpbmspLmF0dHIoJ2hyZWYnKTtcXG4gICAgY29uc29sZS5sb2codXJsKTtcXG59XFxuXCJ9Iiwib3B0aW9ucyI6eyJidWlsZCI6ImxhdGVzdCIsImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6MTAyNCwidGltZW91dCI6MTgwfX0.28PdE3s27h6nCqUFLj6UYLwH9RJRqGQBH5KqnfjfBGw&asrc=run_on_apify)\n\n```\nimport * as cheerio from 'cheerio';import { gotScraping } from 'got-scraping';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);// ------- new code belowconst links = $('a');for (const link of links) {    const url = $(link).attr('href');    console.log(url);}\n```\n\nAside from importing libraries and downloading HTML, we load the HTML into Cheerio and then use it to retrieve all the `<a>` elements. After that, we iterate over the collected links and print their `href` attributes, which we access using the [`.attr()`](https://cheerio.js.org/docs/api/classes/Cheerio#attr) method.\n\nWhen you run the above code, you'll see quite a lot of links in the terminal. Some of them may look wrong, because they don't start with the regular `https://` protocol. We'll learn what to do with them in the following lessons.\n\n## Next Up[​](#next \"Direct link to Next Up\")\n\nThe [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/filtering-links) will teach you how to select and filter links, so that your crawler will always work only with valid and useful URLs.",
  "debug": {
    "requestHandlerMode": "http"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/relative-urls",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/relative-urls",
    "loadedTime": "2025-07-21T15:50:53.419Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/relative-urls",
    "title": "Relative URLs | Academy | Apify Documentation",
    "description": "Learn about absolute and relative URLs used on web pages and how to work with them when parsing HTML with Cheerio in your scraper.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/relative-urls"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Relative URLs | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn about absolute and relative URLs used on web pages and how to work with them when parsing HTML with Cheerio in your scraper."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Relative+URLs"
      }
    ],
    "jsonLd": null,
    "headers": {
      "accept-ranges": "bytes",
      "access-control-allow-origin": "*",
      "age": "0",
      "cache-control": "max-age=600",
      "content-type": "text/html; charset=utf-8",
      "date": "Mon, 21 Jul 2025 15:50:53 GMT",
      "etag": "W/\"687e5eba-dbe1\"",
      "expires": "Mon, 21 Jul 2025 16:00:53 GMT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "server": "nginx",
      "strict-transport-security": "max-age=31556952",
      "vary": "Accept-Encoding",
      "via": "1.1 varnish, 1.1 e880df37740c4e68e519f8478d14cb88.cloudfront.net (CloudFront)",
      "x-amz-cf-id": "HFJAnpC1q6AB3vn3bbPowcPB8egEeveN70otSIuTQ4ECbZJpF4HQdw==",
      "x-amz-cf-pop": "IAD89-P2",
      "x-cache": "Miss from cloudfront",
      "x-cache-hits": "0",
      "x-fastly-request-id": "28347d2e11e470c80b164fc08c68f6dbaf74a773",
      "x-frame-options": "SAMEORIGIN",
      "x-github-request-id": "EAC3:31B5AC:13393A:144886:687E61DD",
      "x-origin-cache": "HIT",
      "x-proxy-cache": "MISS",
      "x-served-by": "cache-iad-kiad7000163-IAD",
      "x-timer": "S1753113053.396837,VS0,VE13"
    }
  },
  "screenshotUrl": null,
  "text": "Relative URLs | Academy | Apify Documentation\nLearn about absolute and relative URLs used on web pages and how to work with them when parsing HTML with Cheerio in your scraper.\nYou might have noticed in the previous lesson that while printing URLs to the DevTools console, they would always show in full length, like this:\nhttps://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones\nBut in the Elements tab, when checking the <a href=\"...\"> attributes, the URLs would look like this:\n/products/denon-ah-c720-in-ear-headphones\nWhat's up with that? This short version of the URL is called a relative URL, and the full length one is called an absolute URL.\nLearn more about absolute and relative URLs.\nWe'll see why the difference between relative URLs and absolute URLs is important a bit later in this lesson.\nBrowser vs Node.js: The Differences​\nLet's update the Node.js code from the Finding links lesson to see why links with relative URLs can be a problem.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\nfor (const link of productLinks) {\nconst url = $(link).attr('href');\nconsole.log(url);\n}\nWhen you run this file in your terminal, you'll immediately see the difference. Unlike in the browser, where looping over elements produced absolute URLs, here in Node.js it only produces the relative ones. This is bad, because we can't use the relative URLs to crawl. They don't include all the necessary information.\nResolving URLs​\nLuckily, there's a process called resolving URLs that creates absolute URLs from relative ones. We need two things. The relative URL, such as /products/denon-ah-c720-in-ear-headphones, and the URL of the website where we found the relative URL (which is https://warehouse-theme-metal.myshopify.com in our case).\nconst websiteUrl = 'https://warehouse-theme-metal.myshopify.com';\nconst relativeUrl = '/products/denon-ah-c720-in-ear-headphones';\n\nconst absoluteUrl = new URL(relativeUrl, websiteUrl);\nconsole.log(absoluteUrl.href);\nIn Node.js, when you create a new URL(), you can optionally pass a second argument, the base URL. When you do, the URL in the first argument will be resolved using the URL in the second argument. Note that the URL created from new URL() is an object, not a string. To get the URL in a string format, we use the url.href property, or alternatively the url.toString() function.\nWhen we plug this into our crawler code, we will get the correct - absolute - URLs.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\n// Split the base URL from the category to use it later.\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\nfor (const link of productLinks) {\nconst relativeUrl = $(link).attr('href');\n// Resolve relative URLs using the website's URL\nconst absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\nconsole.log(absoluteUrl.href);\n}\nCheerio can't resolve the URL itself, because until you provide the necessary information - it doesn't know where you originally downloaded the HTML from. The browser always knows which page you're on, so it will resolve the URLs automatically.\nNext up​\nThe next lesson will teach you how to use the collected URLs to crawl all the individual product pages.",
  "markdown": "# Relative URLs | Academy | Apify Documentation\n\n**Learn about absolute and relative URLs used on web pages and how to work with them when parsing HTML with Cheerio in your scraper.**\n\n* * *\n\nYou might have noticed in the previous lesson that while printing URLs to the DevTools console, they would always show in full length, like this:\n\n```\nhttps://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones\n```\n\nBut in the Elements tab, when checking the `<a href=\"...\">` attributes, the URLs would look like this:\n\n```\n/products/denon-ah-c720-in-ear-headphones\n```\n\nWhat's up with that? This short version of the URL is called a **relative URL**, and the full length one is called an **absolute URL**.\n\n> [Learn more about absolute and relative URLs](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/Web_mechanics/What_is_a_URL#absolute_urls_vs._relative_urls).\n\nWe'll see why the difference between relative URLs and absolute URLs is important a bit later in this lesson.\n\n## Browser vs Node.js: The Differences[​](#browser-vs-node \"Direct link to Browser vs Node.js: The Differences\")\n\nLet's update the Node.js code from the [Finding links lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/finding-links) to see why links with relative URLs can be a problem.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const storeUrl = 'https://warehouse-theme-metal.myshopify.com/collections/sales';const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const productLinks = $('a.product-item__title');for (const link of productLinks) {    const url = $(link).attr('href');    console.log(url);}\n```\n\nWhen you run this file in your terminal, you'll immediately see the difference. Unlike in the browser, where looping over elements produced absolute URLs, here in Node.js it only produces the relative ones. This is bad, because we can't use the relative URLs to crawl. They don't include all the necessary information.\n\n## Resolving URLs[​](#resolving-urls \"Direct link to Resolving URLs\")\n\nLuckily, there's a process called resolving URLs that creates absolute URLs from relative ones. We need two things. The relative URL, such as `/products/denon-ah-c720-in-ear-headphones`, and the URL of the website where we found the relative URL (which is `https://warehouse-theme-metal.myshopify.com` in our case).\n\n```\nconst websiteUrl = 'https://warehouse-theme-metal.myshopify.com';const relativeUrl = '/products/denon-ah-c720-in-ear-headphones';const absoluteUrl = new URL(relativeUrl, websiteUrl);console.log(absoluteUrl.href);\n```\n\nIn Node.js, when you create a `new URL()`, you can optionally pass a second argument, the base URL. When you do, the URL in the first argument will be resolved using the URL in the second argument. Note that the URL created from `new URL()` is an object, not a string. To get the URL in a string format, we use the `url.href` property, or alternatively the `url.toString()` function.\n\nWhen we plug this into our crawler code, we will get the correct - absolute - URLs.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';// Split the base URL from the category to use it later.const WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';const storeUrl = `${WEBSITE_URL}/collections/sales`;const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const productLinks = $('a.product-item__title');for (const link of productLinks) {    const relativeUrl = $(link).attr('href');    // Resolve relative URLs using the website's URL    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);    console.log(absoluteUrl.href);}\n```\n\nCheerio can't resolve the URL itself, because until you provide the necessary information - it doesn't know where you originally downloaded the HTML from. The browser always knows which page you're on, so it will resolve the URLs automatically.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nThe [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/first-crawl) will teach you how to use the collected URLs to crawl all the individual product pages.",
  "debug": {
    "requestHandlerMode": "http"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/filtering-links",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/filtering-links",
    "loadedTime": "2025-07-21T15:50:50.898Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/filtering-links",
    "title": "Filtering links | Academy | Apify Documentation",
    "description": "When you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/filtering-links"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Filtering links | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "When you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Filtering+links"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "11377",
      "date": "Mon, 21 Jul 2025 15:50:49 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-d481\"",
      "expires": "Mon, 21 Jul 2025 15:53:20 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "A52C:197E1F:ED2B2:100963:687E6018",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100022-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113050.769556, VS0, VE15",
      "x-fastly-request-id": "e900591de41f482d98703a0b18696e40c3f868d5",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "qSwO_mU62BlRF5FGV3fH1f_S0sptvRQMA5Qv3NFUMOpBHzKZrLuzIQ==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Filtering links | Academy | Apify Documentation\nWhen you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.\nWeb pages are full of links, but frankly, most of them are useless to us when scraping. Filtering links can be approached in two ways: Targeting the links we're interested in by using unique CSS selectors, or extracting all links and then using pattern matching to find the sought after URLs. In real scraping scenarios, both of these two approaches are often combined for the most effective URL filtering.\nFiltering with unique CSS selectors​\nIn the previous lesson, we grabbed all the links from the HTML document.\nDevTools\nNode.js with Cheerio\ndocument.querySelectorAll('a');\nAttribute selector​\nThat's not the only way to do it, however. Since we're interested in the href attributes, a first very reasonable filter is to exclusively target the <a> tags that have the href attribute (yes, anchor tags without the attribute can and do exist). You can do that by using the CSS attribute selector.\nDevTools\nNode.js\ndocument.querySelectorAll('a[href]');\nAdding the [href] part of the selector will save you from nasty bug hunts on certain pages.\nLink specific selectors​\nLet's go back to the Sales category of Warehouse and see how we could capture only the links to product detail pages. After inspecting the product cards in DevTools, you'll find that the links are available together with the product's title. Getting them will therefore be very similar to getting the product titles in the previous section.\nDevTools\nNode.js\ndocument.querySelectorAll('a.product-item__title');\nWhen we print all the URLs in the DevTools console, we can see that we've correctly filtered only the product detail page URLs.\nfor (const a of document.querySelectorAll('a.product-item__title')) {\nconsole.log(a.href);\n}\ninfo\nIf you try this in Node.js instead of DevTools, you will not get the full URLs, but only so-called relative links. We will explain what those are and how to work with them in the next lesson.\nFiltering with pattern-matching​\nAnother common way to filter links (or any text, really) is by matching patterns with regular expressions.\nLearn more about regular expressions\nWhen we inspect the product URLs, we'll find that they all look like the following:\nhttps://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones\nhttps://warehouse-theme-metal.myshopify.com/products/sony-sacs9-10-inch-active-subwoofer\nhttps://warehouse-theme-metal.myshopify.com/products/sony-ps-hx500-hi-res-usb-turntable\nThat is, they all begin with exactly the same pattern and only differ in the last portion of the path. We could write the pattern like this:\nhttps://warehouse-theme-metal.myshopify.com/products/{PRODUCT_NAME}\nThis means that we can create a regular expression that matches those URLs. You can do it in many ways . For simplicity, let's go with this one:\nhttps?:\\/\\/warehouse-theme-metal\\.myshopify\\.com\\/products\\/[\\w\\-]+\nThis regular expression matches all URLs that use either http or https protocol and point to warehouse-theme-metal.myshopify.com/products/ immediately followed with any number of letters or dashes -.\nA great way to learn more about regular expression syntax and to test your expressions are tools like regex101.com or regexr.com. It's okay if you don't get the hang of it right away!\nTo test our regular expression in the DevTools console, we'll first create a RegExp object and then test the URLs with the regExp.test(string) function.\n// To demonstrate pattern matching, we use only the 'a'\n// selector to select all links on the page.\nfor (const a of document.querySelectorAll('a')) {\nconst regExp = /https?:\\/\\/warehouse-theme-metal\\.myshopify\\.com\\/products\\/[\\w-]+/;\nconst url = a.href;\nif (regExp.test(url)) console.log(url);\n}\nWhen you run this code in DevTools Console on the Sales category of Warehouse, you'll see that it produces a slightly different set of URLs than the CSS filter did.\nThat's because we selected all the links on the page and apparently there are more ways to get to the product detail pages. After careful inspection we can find that we can get there not only by clicking the title, but also by clicking the product's image, which leads to duplicates. Some products also have review links that lead to a specific subsection of the product detail page.\nWith that said, yes, filtering with CSS selectors is often the better and more reliable option. But sometimes, it's not enough, and knowing about pattern matching with regular expressions expands your scraping toolbox and helps you tackle more complex scenarios.\nNext Up​\nIn the next lesson we'll see how rewriting this code to Node.js is not so simple and learn about absolute and relative URLs in the process.",
  "markdown": "# Filtering links | Academy | Apify Documentation\n\n**When you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.**\n\n* * *\n\nWeb pages are full of links, but frankly, most of them are useless to us when scraping. Filtering links can be approached in two ways: Targeting the links we're interested in by using unique CSS selectors, or extracting all links and then using pattern matching to find the sought after URLs. In real scraping scenarios, both of these two approaches are often combined for the most effective URL filtering.\n\n## Filtering with unique CSS selectors[​](#css-filtering \"Direct link to Filtering with unique CSS selectors\")\n\nIn the previous lesson, we grabbed all the links from the HTML document.\n\n*   DevTools\n*   Node.js with Cheerio\n\n```\ndocument.querySelectorAll('a');\n```\n\n### Attribute selector[​](#attribute-selector \"Direct link to Attribute selector\")\n\nThat's not the only way to do it, however. Since we're interested in the `href` attributes, a first very reasonable filter is to exclusively target the `<a>` tags that have the `href` attribute (yes, anchor tags without the attribute can and do exist). You can do that by using the [CSS attribute selector](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors).\n\n*   DevTools\n*   Node.js\n\n```\ndocument.querySelectorAll('a[href]');\n```\n\nAdding the `[href]` part of the selector will save you from nasty bug hunts on certain pages.\n\n### Link specific selectors[​](#specific-selectors \"Direct link to Link specific selectors\")\n\nLet's go back to the [Sales category of Warehouse](https://warehouse-theme-metal.myshopify.com/collections/sales) and see how we could capture only the links to product detail pages. After inspecting the product cards in DevTools, you'll find that the links are available together with the product's title. Getting them will therefore be very similar to getting the product titles in the previous section.\n\n![product detail page link](https://docs.apify.com/assets/images/filtering-product-detail-link-995bf048903e4b4205b52fd364719005.png)\n\n*   DevTools\n*   Node.js\n\n```\ndocument.querySelectorAll('a.product-item__title');\n```\n\nWhen we print all the URLs in the DevTools console, we can see that we've correctly filtered only the product detail page URLs.\n\n```\nfor (const a of document.querySelectorAll('a.product-item__title')) {    console.log(a.href);}\n```\n\ninfo\n\nIf you try this in Node.js instead of DevTools, you will not get the full URLs, but only so-called **relative links**. We will explain what those are and how to work with them in the next lesson.\n\n![Product URLs printed to console](https://docs.apify.com/assets/images/filtering-product-urls-aeaa901d618beff306b45fa48cc138e1.png)\n\n## Filtering with pattern-matching[​](#pattern-matching-filter \"Direct link to Filtering with pattern-matching\")\n\nAnother common way to filter links (or any text, really) is by matching patterns with regular expressions.\n\n> [Learn more about regular expressions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions)\n\nWhen we inspect the product URLs, we'll find that they all look like the following:\n\n```\nhttps://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphoneshttps://warehouse-theme-metal.myshopify.com/products/sony-sacs9-10-inch-active-subwooferhttps://warehouse-theme-metal.myshopify.com/products/sony-ps-hx500-hi-res-usb-turntable\n```\n\nThat is, they all begin with exactly the same pattern and only differ in the last portion of the path. We could write the pattern like this:\n\n```\nhttps://warehouse-theme-metal.myshopify.com/products/{PRODUCT_NAME}\n```\n\nThis means that we can create a regular expression that matches those URLs. You can do it in many ways . For simplicity, let's go with this one:\n\n```\nhttps?:\\/\\/warehouse-theme-metal\\.myshopify\\.com\\/products\\/[\\w\\-]+\n```\n\nThis regular expression matches all URLs that use either `http` or `https` protocol and point to `warehouse-theme-metal.myshopify.com/products/` immediately followed with any number of letters or dashes `-`.\n\n> A great way to learn more about regular expression syntax and to test your expressions are tools like [regex101.com](https://regex101.com/) or [regexr.com](https://regexr.com/). It's okay if you don't get the hang of it right away!\n\nTo test our regular expression in the DevTools console, we'll first create a [`RegExp`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp) object and then test the URLs with the [`regExp.test(string)`](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/test) function.\n\n```\n// To demonstrate pattern matching, we use only the 'a'// selector to select all links on the page.for (const a of document.querySelectorAll('a')) {    const regExp = /https?:\\/\\/warehouse-theme-metal\\.myshopify\\.com\\/products\\/[\\w-]+/;    const url = a.href;    if (regExp.test(url)) console.log(url);}\n```\n\nWhen you run this code in DevTools Console on the [Sales category of Warehouse](https://warehouse-theme-metal.myshopify.com/collections/sales), you'll see that it produces a slightly different set of URLs than the CSS filter did.\n\n![filtering-regex-urls.png](https://docs.apify.com/assets/images/filtering-regex-urls-a4ac344fc73c37d7593ea276bca73872.png)\n\nThat's because we selected all the links on the page and apparently there are more ways to get to the product detail pages. After careful inspection we can find that we can get there not only by clicking the title, but also by clicking the product's image, which leads to duplicates. Some products also have review links that lead to a specific subsection of the product detail page.\n\nWith that said, yes, filtering with CSS selectors is often the better and more reliable option. But sometimes, it's not enough, and knowing about pattern matching with regular expressions expands your scraping toolbox and helps you tackle more complex scenarios.\n\n## Next Up[​](#next \"Direct link to Next Up\")\n\nIn the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/relative-urls) we'll see how rewriting this code to Node.js is not so simple and learn about absolute and relative URLs in the process.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/first-crawl",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/first-crawl",
    "loadedTime": "2025-07-21T15:50:55.216Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/first-crawl",
    "title": "Your first crawl | Academy | Apify Documentation",
    "description": "Learn how to crawl the web using Node.js, Cheerio and an HTTP client. Extract URLs from pages and use them to visit more websites.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/first-crawl"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Your first crawl | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to crawl the web using Node.js, Cheerio and an HTTP client. Extract URLs from pages and use them to visit more websites."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Your+first+crawl"
      }
    ],
    "jsonLd": null,
    "headers": {
      "accept-ranges": "bytes",
      "access-control-allow-origin": "*",
      "age": "0",
      "cache-control": "max-age=600",
      "content-type": "text/html; charset=utf-8",
      "date": "Mon, 21 Jul 2025 15:50:55 GMT",
      "etag": "W/\"687e5eba-11296\"",
      "expires": "Mon, 21 Jul 2025 16:00:55 GMT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "server": "nginx",
      "strict-transport-security": "max-age=31556952",
      "vary": "Accept-Encoding",
      "via": "1.1 varnish, 1.1 dbb909966903df95f63a00d4241f7b7c.cloudfront.net (CloudFront)",
      "x-amz-cf-id": "9RgaalM0BCVnIWVm2Nlcll8eFHLisH_ZEuOv3Daz7eJprRFjHZC5eg==",
      "x-amz-cf-pop": "IAD89-P2",
      "x-cache": "Miss from cloudfront",
      "x-cache-hits": "0",
      "x-fastly-request-id": "605f28505e620a659539f58c29f2d2e9ded55554",
      "x-frame-options": "SAMEORIGIN",
      "x-github-request-id": "1B51:9B227:12D225:13E13F:687E61DE",
      "x-origin-cache": "HIT",
      "x-proxy-cache": "MISS",
      "x-served-by": "cache-iad-kjyo7100128-IAD",
      "x-timer": "S1753113055.180135,VS0,VE22"
    }
  },
  "screenshotUrl": null,
  "text": "Your first crawl | Academy\nLearn how to crawl the web using Node.js, Cheerio and an HTTP client. Extract URLs from pages and use them to visit more websites.\nIn the previous lessons, we learned what crawling is and how to extract URLs from a page's HTML. The only thing that remains is to write the code—let's get right to it!\nIf the code starts to look too complex to you, don't worry. We're showing it for educational purposes, so that you can learn how crawling works. Near the end of this course, we'll show you a much easier and faster way to crawl, using a specialized scraping library. If you want, you can skip the details and go there now.\nProcessing URLs​\nIn the previous lessons, we collected and filtered all the URLs pointing to individual products in the Sales category of Warehouse store. To crawl the URLs, we must take the whole list we collected and download the HTML of each of the pages. See the comments for changes and additions to the code.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst productLinks = $('a.product-item__title');\n\n// Prepare an empty array for our product URLs.\nconst productUrls = [];\n\nfor (const link of productLinks) {\nconst relativeUrl = $(link).attr('href');\nconst absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\n\n// Collect absolute product URLs.\nproductUrls.push(absoluteUrl);\n}\n\n// Loop over the stored URLs to process\n// each product page individually.\nfor (const url of productUrls) {\n// Download HTML.\nconst productResponse = await gotScraping(url);\nconst productHtml = productResponse.body;\n\n// Load into Cheerio to parse the HTML.\nconst $productPage = cheerio.load(productHtml);\n\n// Extract the product's title from the <h1> tag.\nconst productPageTitle = $productPage('h1').text().trim();\n\n// Print the title to the terminal to see\n// confirm we downloaded the correct pages.\nconsole.log(productPageTitle);\n}\nIf you run the crawler from your terminal, it will print the titles of all the products on sale in the Warehouse store.\nHandling errors​\nThe code above is correct, but it's not robust. If something goes wrong, it will crash. That something could be a network error, an internet connection error, or the websites you're trying to reach could be experiencing problems at that moment. Hitting any error like that would cause the current crawler to stop entirely, which means we would lose all the data it had collected so far.\nIn programming, you handle errors by catching and handling them. Typically by printing information that the error occurred and/or retrying.\nThe scraping library we'll show you in the following lessons handles errors and retries automatically for you.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\nconst relativeUrl = $(link).attr('href');\nconst absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\nproductUrls.push(absoluteUrl);\n}\n\nfor (const url of productUrls) {\n// Everything else is exactly the same.\n// We only wrapped the code in try/catch blocks.\n// The try block passes all errors into the catch block.\n// So, instead of crashing the crawler, they can be handled.\ntry {\n// The try block attempts to execute our code\nconst productResponse = await gotScraping(url);\nconst productHtml = productResponse.body;\nconst $productPage = cheerio.load(productHtml);\nconst productPageTitle = $productPage('h1').text().trim();\nconsole.log(productPageTitle);\n} catch (error) {\n// In the catch block, we handle errors.\n// This time, we will print\n// the error message and the url.\nconsole.error(error.message, url);\n}\n}\nAt the time of writing, none of the links have failed; however, as you crawl more pages, you will surely hit a few errors 😉. The important thing is that the crawler will no longer crash if an error does in fact occur, and that it will be able to download the HTML from the working product links.\nIf you thought that the crawl was taking too long to complete, the scraping library we keep referring to will help once again. It automatically parallelizes the downloads and processing of HTML, which leads to significant speed improvements.\nNext up​\nIn the next lesson, we will complete the scraper by extracting data about all the products from their individual pages.",
  "markdown": "# Your first crawl | Academy\n\n**Learn how to crawl the web using Node.js, Cheerio and an HTTP client. Extract URLs from pages and use them to visit more websites.**\n\n* * *\n\nIn the previous lessons, we learned what crawling is and how to extract URLs from a page's HTML. The only thing that remains is to write the code—let's get right to it!\n\n> If the code starts to look too complex to you, don't worry. We're showing it for educational purposes, so that you can learn how crawling works. Near the end of this course, we'll show you a much easier and faster way to crawl, using a specialized scraping library. If you want, you can skip the details and [go there now](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping).\n\n## Processing URLs[​](#processing-urls \"Direct link to Processing URLs\")\n\nIn the previous lessons, we collected and filtered all the URLs pointing to individual products in the [Sales category of Warehouse store](https://warehouse-theme-metal.myshopify.com/collections/sales). To crawl the URLs, we must take the whole list we collected and download the HTML of each of the pages. See the comments for changes and additions to the code.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';const storeUrl = `${WEBSITE_URL}/collections/sales`;const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const productLinks = $('a.product-item__title');// Prepare an empty array for our product URLs.const productUrls = [];for (const link of productLinks) {    const relativeUrl = $(link).attr('href');    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);    // Collect absolute product URLs.    productUrls.push(absoluteUrl);}// Loop over the stored URLs to process// each product page individually.for (const url of productUrls) {    // Download HTML.    const productResponse = await gotScraping(url);    const productHtml = productResponse.body;    // Load into Cheerio to parse the HTML.    const $productPage = cheerio.load(productHtml);    // Extract the product's title from the <h1> tag.    const productPageTitle = $productPage('h1').text().trim();    // Print the title to the terminal to see    // confirm we downloaded the correct pages.    console.log(productPageTitle);}\n```\n\nIf you run the crawler from your terminal, it will print the titles of all the products on sale in the Warehouse store.\n\n## Handling errors[​](#handling-errors \"Direct link to Handling errors\")\n\nThe code above is correct, but it's not robust. If something goes wrong, it will crash. That something could be a network error, an internet connection error, or the websites you're trying to reach could be experiencing problems at that moment. Hitting any error like that would cause the current crawler to stop entirely, which means we would lose all the data it had collected so far.\n\nIn programming, you handle errors by catching and handling them. Typically by printing information that the error occurred and/or retrying.\n\n> The scraping library we'll [show you in the following lessons](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping) handles errors and retries automatically for you.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';const storeUrl = `${WEBSITE_URL}/collections/sales`;const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const productLinks = $('a.product-item__title');const productUrls = [];for (const link of productLinks) {    const relativeUrl = $(link).attr('href');    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);    productUrls.push(absoluteUrl);}for (const url of productUrls) {    // Everything else is exactly the same.    // We only wrapped the code in try/catch blocks.    // The try block passes all errors into the catch block.    // So, instead of crashing the crawler, they can be handled.    try {        // The try block attempts to execute our code        const productResponse = await gotScraping(url);        const productHtml = productResponse.body;        const $productPage = cheerio.load(productHtml);        const productPageTitle = $productPage('h1').text().trim();        console.log(productPageTitle);    } catch (error) {        // In the catch block, we handle errors.        // This time, we will print        // the error message and the url.        console.error(error.message, url);    }}\n```\n\nAt the time of writing, none of the links have failed; however, as you crawl more pages, you will surely hit a few errors 😉. The important thing is that the crawler will no longer crash if an error does in fact occur, and that it will be able to download the HTML from the working product links.\n\n> If you thought that the crawl was taking too long to complete, the [scraping library](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping) we keep referring to will help once again. It automatically parallelizes the downloads and processing of HTML, which leads to significant speed improvements.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nIn the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/scraping-the-data), we will complete the scraper by extracting data about all the products from their individual pages.",
  "debug": {
    "requestHandlerMode": "http"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping",
    "loadedTime": "2025-07-21T15:51:00.507Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping",
    "title": "Professional scraping | Academy | Apify Documentation",
    "description": "Learn how to build scrapers quicker and get better and more robust results by using Crawlee, an open-source library for scraping in Node.js.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Professional scraping | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to build scrapers quicker and get better and more robust results by using Crawlee, an open-source library for scraping in Node.js."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Professional+scraping"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "14495",
      "date": "Mon, 21 Jul 2025 15:50:58 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-172f2\"",
      "expires": "Mon, 21 Jul 2025 16:00:58 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "B7F4:3911FB:150EE2:161F02:687E61E1",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100054-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113059.563995, VS0, VE15",
      "x-fastly-request-id": "995a47a9e1e1571a70bad09e017b863b01abce3a",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "vrxqIvOCdwpS0fkfHRtMR7sxZ5XYbv7OSvq8w6Ioi06SXYpYNiS70g==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Professional scraping | Academy | Apify Documentation\nLearn how to build scrapers quicker and get better and more robust results by using Crawlee, an open-source library for scraping in Node.js.\nWhile it's definitely an interesting exercise to do all the programming manually, and we hope you enjoyed it, it's neither the most effective, nor the most efficient way of scraping websites. Because we scrape for a living at Apify, we've built a library that we use to scrape tens of millions of pages every day.\nIt's called Crawlee, and it is, and always will be, completely open-source and free to use. You don't need to sign up for an Apify account or use the Apify platform. You can use it on your personal computer, on any server, or in any cloud environment you want.\nWe mentioned the benefits of developing using a dedicated scraping library in the previous lessons, but to recap:\nFaster development time because you don't have to write boilerplate code.\nFewer bugs. Crawlee is fully unit-tested and battle-tested on millions of scraper runs.\nFaster and cheaper scrapers because Crawlee automatically scales based on system resources, and we optimize its performance regularly.\nMore robust scrapers. Annoying details like retries, proxy management, error handling, and result storage are all handled out-of-the-box by Crawlee.\nHelpful community. You can join our Discord or talk to us on GitHub. We're almost always there to talk about scraping and programming in general.\ntip\nIf you're still not convinced, read this story about how a data analytics company saved 90% of scraping costs by switching from Scrapy (a scraping library for Python) to Crawlee. We were pretty surprised ourselves, to be honest.\nCrawlee factors away and manages the dull and repetitive parts of web scraper development under the hood, such as:\nAuto-scaling\nRequest concurrency\nQueueing requests\nData storage\nUsing and rotating proxies\nPuppeteer/Playwright setup overhead\nSee all the features\nCrawlee and its resources can be found in various different places:\nOfficial Crawlee documentation\nCrawlee GitHub repository (source code, issues)\nCrawlee on npm\nInstall Crawlee​\nTo use Crawlee, we have to install it from npm. Let's add it to our project from the previous lessons by executing this command in your project's folder.\nAfter the installation completes, create a new file called crawlee.js and add the following code to it:\nimport { CheerioCrawler } from 'crawlee';\n\nconsole.log('Crawlee works!');\nWe are using the new ESM import syntax here (see Node.js docs). To be able to use it, we need to turn our project to module in the package.json file:\n{\n\"name\": \"my-scraping-project\",\n\"type\": \"module\",\n\"dependencies\": {\n\"crawlee\": \"^3.0.0\"\n}\n}\nThen, run the code using node as usual:\nYou'll see \"Crawlee works!\" printed to the console. If it doesn't work, it means Crawlee didn't install correctly. If that's the case, try deleting the node_modules directory and package-lock.json file in your project and install Crawlee again.\nYou don't need to import any other libraries like Cheerio or Got-Scraping. That's because they're both included in Crawlee's CheerioCrawler.\nPrepare the scraper​\nCheerioCrawler automatically visits URLs, downloads HTML using Got-Scraping, and parses it with Cheerio. The benefit of this over writing the code yourself is that it automatically handles the URL queue, errors, retries, proxies, parallelizes the downloads, and much more. Overall, it removes the need to write a lot of boilerplate code.\nTo create a crawler with Crawlee, you only need to provide it with a request handler - a function that gets executed for each page it visits.\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\n// This function will run on every page.\n// Among other things, it gives you access\n// to parsed HTML with the Cheerio $ function.\nrequestHandler: async ({ $, request }) => {\nconsole.log('URL:', request.url);\n// Print the heading of each visited page.\nconsole.log('Title:', $('h1').text().trim());\n},\n});\nBut the above code still doesn't crawl anything. We need to provide it with URLs to crawl. To do that, we call the crawler's addRequests function.\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\nrequestHandler: async ({ $, request }) => {\nconsole.log('URL:', request.url);\nconsole.log('Title:', $('h1').text().trim());\n},\n});\n\n// Add the Sales category of Warehouse store to the queue of URLs.\nawait crawler.addRequests([\n'https://warehouse-theme-metal.myshopify.com/collections/sales',\n]);\n\nawait crawler.run();\nWhen you run the above code, you'll see some internal Crawlee logs and then the two messages your code printed:\nURL: https://warehouse-theme-metal.myshopify.com/collections/sales\nTitle: Sales\n\ncrawler.addRequests uses the RequestQueue under the hood. It's a persistent storage, which means that if your crawler crashes, it doesn't have to start over, but it can continue from where it left off.\nSummary​\nWe added the first URL to the crawler using the addRequests function.\nCheerioCrawler will automatically take the URL from the queue, download its HTML using Got Scraping, and parse it using Cheerio.\nThe crawler executes the requestHandler, where we extract the page's data using the $ variable. You can also access the request itself using the request variable.\nCrawling links​\nThe current scraper only visits the Sales category page, but we want detailed data for all the products. We can use the enqueueLinks() function to add more URLs to the queue. The function automatically extracts URLs from the current page based on a provided CSS selector and adds them to the queue. Once added, the crawler will automatically crawl them.\nimport { CheerioCrawler } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\nrequestHandler: async ({ $, request, enqueueLinks }) => {\nconsole.log('URL:', request.url);\nconsole.log('Title:', $('h1').text().trim());\n\n// We only want to enqueue the URLs from the start URL.\nif (request.label === 'start-url') {\n// enqueueLinks will add all the links\n// that match the provided selector.\nawait enqueueLinks({\n// The selector comes from our earlier code.\nselector: 'a.product-item__title',\n});\n}\n},\n});\n\n// Instead of using a string with URL, we're now\n// using a request object to add more options.\nawait crawler.addRequests([{\nurl: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\n// We label the Request to identify\n// it later in the requestHandler.\nlabel: 'start-url',\n}]);\n\nawait crawler.run();\nWhen you run the code, you'll see the names and URLs of all the products printed to the console. You'll also see that it crawls faster than the manually written code. This is thanks to the parallelization of the requests.\nIf the crawler gets stuck for you at the end, it's ok. It's not actually stuck, but waiting to retry any pages that may have failed.\nWe have the crawler in place, and it's time to extract data. We already have the extraction code from the previous lesson, so we can copy and paste it into the requestHandler with tiny changes. Instead of printing results to the terminal, we will save it to disk.\n// To save data to disk, we need to import Dataset.\nimport { CheerioCrawler, Dataset } from 'crawlee';\n\nconst crawler = new CheerioCrawler({\nrequestHandler: async ({ $, request, enqueueLinks }) => {\nconsole.log(`Fetching URL: ${request.url}`);\n\nif (request.label === 'start-url') {\nawait enqueueLinks({\nselector: 'a.product-item__title',\n});\n// When on the start URL, we don't want to\n// extract any data after we extract the links.\nreturn;\n}\n\n// We copied and pasted the extraction code\n// from the previous lesson with small\n// refactoring: e.g. `$productPage` to `$`.\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n\n// Instead of printing the results to\n// console, we save everything to a file.\nawait Dataset.pushData({\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\n});\n},\n});\n\nawait crawler.addRequests([{\nurl: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\nlabel: 'start-url',\n}]);\n\nawait crawler.run();\nWhen you run the code as usual, you'll see the product URLs printed to the terminal and you'll find the scraped data saved to your disk. Thanks to using the Dataset.pushData() function, Crawlee automatically created a storage directory in your project's location and saved the results there. Each product has its data stored as a separate JSON file.\n./storage/datasets/default/*.json\nThanks to Crawlee, we were able to create a faster and more robust scraper, but with less code than what was needed for the scraper in the earlier lessons.\nNext up​\nIn the next lesson we'll show you how to turn this plain HTTP crawler into a headless browser scraper in only a few lines of code.",
  "markdown": "# Professional scraping | Academy | Apify Documentation\n\n**Learn how to build scrapers quicker and get better and more robust results by using Crawlee, an open-source library for scraping in Node.js.**\n\n* * *\n\nWhile it's definitely an interesting exercise to do all the programming manually, and we hope you enjoyed it, it's neither the most effective, nor the most efficient way of scraping websites. Because we scrape for a living at Apify, we've built a library that we use to scrape tens of millions of pages every day.\n\nIt's called [**Crawlee**](https://crawlee.dev/), and it is, and **always will be**, completely **open-source** and **free** to use. You don't need to sign up for an Apify account or use the Apify platform. You can use it on your personal computer, on any server, or in any cloud environment you want.\n\nWe mentioned the benefits of developing using a dedicated scraping library in the previous lessons, but to recap:\n\n1.  **Faster development time** because you don't have to write boilerplate code.\n2.  **Fewer bugs**. Crawlee is fully unit-tested and battle-tested on millions of scraper runs.\n3.  **Faster and cheaper scrapers** because Crawlee automatically scales based on system resources, and we optimize its performance regularly.\n4.  **More robust scrapers**. Annoying details like retries, proxy management, error handling, and result storage are all handled out-of-the-box by Crawlee.\n5.  **Helpful community**. You can [join our Discord](https://discord.gg/qkMS6pU4cF) or talk to us [on GitHub](https://github.com/apify/crawlee/discussions). We're almost always there to talk about scraping and programming in general.\n\ntip\n\nIf you're still not convinced, [read this story](https://apify.com/success-stories/daltix-analytics-scrapy-python-to-apify) about how a data analytics company saved 90% of scraping costs by switching from Scrapy (a scraping library for Python) to Crawlee. We were pretty surprised ourselves, to be honest.\n\nCrawlee factors away and manages the dull and repetitive parts of web scraper development under the hood, such as:\n\n*   Auto-scaling\n*   Request concurrency\n*   Queueing requests\n*   Data storage\n*   Using and rotating [proxies](https://docs.apify.com/academy/anti-scraping/mitigation/proxies)\n*   Puppeteer/Playwright setup overhead\n*   [See all the features](https://crawlee.dev/docs/introduction)\n\nCrawlee and its resources can be found in various different places:\n\n1.  [Official Crawlee documentation](https://crawlee.dev/)\n2.  [Crawlee GitHub repository (source code, issues)](https://github.com/apify/crawlee)\n3.  [Crawlee on npm](https://www.npmjs.com/package/crawlee)\n\n## Install Crawlee[​](#crawlee-installation \"Direct link to Install Crawlee\")\n\nTo use Crawlee, we have to install it from npm. Let's add it to our project from the previous lessons by executing this command in your project's folder.\n\nAfter the installation completes, create a new file called **crawlee.js** and add the following code to it:\n\n```\nimport { CheerioCrawler } from 'crawlee';console.log('Crawlee works!');\n```\n\nWe are using the new ESM `import` syntax here (see [Node.js docs](https://nodejs.org/dist/latest-v16.x/docs/api/esm.html#enabling)). To be able to use it, we need to turn our project to `module` in the `package.json` file:\n\n```\n{    \"name\": \"my-scraping-project\",    \"type\": \"module\",    \"dependencies\": {        \"crawlee\": \"^3.0.0\"    }}\n```\n\nThen, run the code using `node` as usual:\n\nYou'll see \"**Crawlee works!**\" printed to the console. If it doesn't work, it means Crawlee didn't install correctly. If that's the case, try deleting the `node_modules` directory and `package-lock.json` file in your project and install Crawlee again.\n\n> You don't need to `import` any other libraries like Cheerio or Got-Scraping. That's because they're both included in Crawlee's [`CheerioCrawler`](https://crawlee.dev/docs/guides/cheerio-crawler-guide).\n\n## Prepare the scraper[​](#coding-the-scraper \"Direct link to Prepare the scraper\")\n\n`CheerioCrawler` automatically visits URLs, downloads HTML using **Got-Scraping**, and parses it with **Cheerio**. The benefit of this over writing the code yourself is that it automatically handles the URL queue, errors, retries, proxies, parallelizes the downloads, and much more. Overall, it removes the need to write a lot of boilerplate code.\n\nTo create a crawler with Crawlee, you only need to provide it with a request handler - a function that gets executed for each page it visits.\n\n```\nimport { CheerioCrawler } from 'crawlee';const crawler = new CheerioCrawler({    // This function will run on every page.    // Among other things, it gives you access    // to parsed HTML with the Cheerio $ function.    requestHandler: async ({ $, request }) => {        console.log('URL:', request.url);        // Print the heading of each visited page.        console.log('Title:', $('h1').text().trim());    },});\n```\n\nBut the above code still doesn't crawl anything. We need to provide it with URLs to crawl. To do that, we call the crawler's `addRequests` function.\n\n```\nimport { CheerioCrawler } from 'crawlee';const crawler = new CheerioCrawler({    requestHandler: async ({ $, request }) => {        console.log('URL:', request.url);        console.log('Title:', $('h1').text().trim());    },});// Add the Sales category of Warehouse store to the queue of URLs.await crawler.addRequests([    'https://warehouse-theme-metal.myshopify.com/collections/sales',]);await crawler.run();\n```\n\nWhen you run the above code, you'll see some internal Crawlee logs and then the two messages your code printed:\n\n```\nURL: https://warehouse-theme-metal.myshopify.com/collections/salesTitle: Sales\n```\n\n> `crawler.addRequests` uses the [`RequestQueue`](https://crawlee.dev/docs/guides/request-storage#request-queue) under the hood. It's a persistent storage, which means that if your crawler crashes, it doesn't have to start over, but it can continue from where it left off.\n\n### Summary[​](#summary \"Direct link to Summary\")\n\n1.  We added the first URL to the crawler using the `addRequests` function.\n2.  `CheerioCrawler` will automatically take the URL from the queue, download its HTML using Got Scraping, and parse it using Cheerio.\n3.  The crawler executes the [`requestHandler`](https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlerOptions#requestHandler), where we extract the page's data using the [`$`](https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlingContext) variable. You can also access the request itself using the [`request`](https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlingContext#request) variable.\n\n## Crawling links[​](#crawling-links \"Direct link to Crawling links\")\n\nThe current scraper only visits the Sales category page, but we want detailed data for all the products. We can use the [`enqueueLinks()`](https://crawlee.dev/api/cheerio-crawler/interface/CheerioCrawlingContext#enqueueLinks) function to add more URLs to the queue. The function automatically extracts URLs from the current page based on a provided CSS selector and adds them to the queue. Once added, the crawler will automatically crawl them.\n\n```\nimport { CheerioCrawler } from 'crawlee';const crawler = new CheerioCrawler({    requestHandler: async ({ $, request, enqueueLinks }) => {        console.log('URL:', request.url);        console.log('Title:', $('h1').text().trim());        // We only want to enqueue the URLs from the start URL.        if (request.label === 'start-url') {            // enqueueLinks will add all the links            // that match the provided selector.            await enqueueLinks({                // The selector comes from our earlier code.                selector: 'a.product-item__title',            });        }    },});// Instead of using a string with URL, we're now// using a request object to add more options.await crawler.addRequests([{    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',    // We label the Request to identify    // it later in the requestHandler.    label: 'start-url',}]);await crawler.run();\n```\n\nWhen you run the code, you'll see the names and URLs of all the products printed to the console. You'll also see that it crawls faster than the manually written code. This is thanks to the parallelization of the requests.\n\n> If the crawler gets stuck for you at the end, it's ok. It's not actually stuck, but waiting to retry any pages that may have failed.\n\nWe have the crawler in place, and it's time to extract data. We already have the extraction code from the previous lesson, so we can copy and paste it into the `requestHandler` with tiny changes. Instead of printing results to the terminal, we will save it to disk.\n\n```\n// To save data to disk, we need to import Dataset.import { CheerioCrawler, Dataset } from 'crawlee';const crawler = new CheerioCrawler({    requestHandler: async ({ $, request, enqueueLinks }) => {        console.log(`Fetching URL: ${request.url}`);        if (request.label === 'start-url') {            await enqueueLinks({                selector: 'a.product-item__title',            });            // When on the start URL, we don't want to            // extract any data after we extract the links.            return;        }        // We copied and pasted the extraction code        // from the previous lesson with small        // refactoring: e.g. `$productPage` to `$`.        const title = $('h1').text().trim();        const vendor = $('a.product-meta__vendor').text().trim();        const price = $('span.price').contents()[2].nodeValue;        const reviewCount = parseInt($('span.rating__caption').text(), 10);        const description = $('div[class*=\"description\"] div.rte').text().trim();        // Instead of printing the results to        // console, we save everything to a file.        await Dataset.pushData({            title,            vendor,            price,            reviewCount,            description,        });    },});await crawler.addRequests([{    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',    label: 'start-url',}]);await crawler.run();\n```\n\nWhen you run the code as usual, you'll see the product URLs printed to the terminal and you'll find the scraped data saved to your disk. Thanks to using the [`Dataset.pushData()`](https://crawlee.dev/docs/introduction/saving-data#whats-datasetpushdata) function, Crawlee automatically created a `storage` directory in your project's location and saved the results there. Each product has its data stored as a separate JSON file.\n\n```\n./storage/datasets/default/*.json\n```\n\nThanks to **Crawlee**, we were able to create a **faster and more robust scraper**, but **with less code** than what was needed for the scraper in the earlier lessons.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nIn the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/headless-browser) we'll show you how to turn this plain HTTP crawler into a **headless browser** scraper in only a few lines of code.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/scraping-the-data",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/scraping-the-data",
    "loadedTime": "2025-07-21T15:51:02.099Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/scraping-the-data",
    "title": "Scraping data | Academy | Apify Documentation",
    "description": "Learn how to add data extraction logic to your crawler, which will allow you to extract data from all the websites you crawled.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/scraping-the-data"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Scraping data | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to add data extraction logic to your crawler, which will allow you to extract data from all the websites you crawled."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Scraping+data"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "14959",
      "date": "Mon, 21 Jul 2025 15:50:57 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-23722\"",
      "expires": "Mon, 21 Jul 2025 16:00:57 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "891E:2A73C4:13837D:14F784:687E61E1",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100157-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113057.363736, VS0, VE17",
      "x-fastly-request-id": "f0add3cab51af947cf6f0f07f984a520833134a1",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "i7ZLSe2JFoyX0BpD_vZggIi7yK4hvO4HQL07gb3Z0BF-a-oesKc9fA==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Scraping data | Academy | Apify Documentation\nLearn how to add data extraction logic to your crawler, which will allow you to extract data from all the websites you crawled.\nAt the very beginning of this course, we learned that the term web scraping usually means a combined process of data extraction and crawling. And this is exactly what we'll do in this lesson. We will take the crawling code from the previous lesson, and we will combine it with data extraction code and turn everything into a web scraper.\nThe term product detail page (or PDP) is commonly used on e-commerce websites to describe the page where you can find detailed information about a product. In the Warehouse store, there's, for example, this page describing Denon headphones.\nLet's start writing a script that extracts data from this single PDP. We can use this familiar code as a boilerplate.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';\nconst response = await gotScraping(productUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\n// Attribute extraction code will go here.\nWe will use the techniques learned in the Basics of data extraction section to find and extract the following product attributes:\ntitle\nvendor\nprice\nnumber of reviews\ndescription\nFor brevity, we won't explain how to extract every attribute step-by-step. Review the Basics of data extraction section to learn about DevTools and extracting data.\nTitle​\nGetting the title is quite straightforward. We recommend using h1 for titles where available, because it's the semantically correct way and therefore unlikely to change.\nconst title = $('h1').text().trim();\nVendor​\nVendor name is available as a link with the product-meta__vendor class. We're only interested in the text though.\nconst vendor = $('a.product-meta__vendor').text().trim();\nPrice​\nWe will take a shortcut here and only extract the price as a string that includes currency. In production scrapers, you might want to split it into two fields.\nconst price = $('span.price').contents()[2].nodeValue;\nNumber of reviews​\nFor the review count, we use the parseInt() function to get only the number. Otherwise, we would receive a string like 2 reviews from this element.\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nDescription​\nGetting the description is fairly straightforward as well, but notice the two selectors separated by a space: div[class*=\"description\"] div.rte. This is called a descendant combinator, and it allows you to search for child elements within parent elements. Using any of the selectors separately would lead to unwanted strings in our result.\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\nThis is the final code after putting all the extractors together with the initial boilerplate. It will scrape all the requested attributes from the single URL and print them to the terminal.\nSave it into a new file called product.js and run it with node product.js to see for yourself.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';\nconst response = await gotScraping(productUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\n\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n\nconst product = {\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\n};\n\nconsole.log(product);\nCrawling product detail pages​\nLet's compare the above data extraction example with the crawling code we wrote in the last lesson:\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\nconst relativeUrl = $(link).attr('href');\nconst absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\nproductUrls.push(absoluteUrl);\n}\n\nfor (const url of productUrls) {\ntry {\nconst productResponse = await gotScraping(url);\nconst productHtml = productResponse.body;\nconst $productPage = cheerio.load(productHtml);\nconst productPageTitle = $productPage('h1').text().trim();\nconsole.log(productPageTitle);\n} catch (error) {\nconsole.error(error.message, url);\n}\n}\nWe can see that the code is quite similar. Both scripts download HTML and then process the HTML. To understand how to put them together, we'll go back to the original process of crawling.\nVisit the start URL.\nExtract the next URLs (and data) and save them.\nVisit one of the collected URLs and save data and/or more URLs.\nRepeat step 3 until you have everything you need.\nUsing this flow as guidance, we should be able to connect the pieces of code together to build a scraper which crawls through the products found in the Sales category of Warehouse, and then scrapes the title, vendor, price, review count, and description of each of them.\nBuilding the final scraper​\nLet's create a brand-new file called final.js and write our scraper code there. We'll show the code step by step for easier orientation. At the end, we'll combine the pieces into a runnable example.\nWe'll start by adding our imports and constants at the top of the file, no changes there.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nThen we need to visit the start URL. To scrape all the on-sale product links, we need the Sales page as the start URL.\n// ...\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\nAfter that, we need to extract the next URLs we want to visit (the product detail page URLs). Thus far, the code is exactly the same as the crawler.js code.\n// ...\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\nconst relativeUrl = $(link).attr('href');\nconst absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\nproductUrls.push(absoluteUrl);\n}\nNow the code will start to differ. We will use the crawling logic from earlier to visit all the URLs, but we will replace the placeholder extraction logic we had there. The placeholder logic only extracted the product's title, but we want the vendor, price, number of reviews and description as well.\n// ...\n\n// A new array to save each product in.\nconst results = [];\n\n// An optional array we can save errors to.\nconst errors = [];\n\nfor (const url of productUrls) {\ntry {\n// Download HTML of each product detail.\nconst productResponse = await gotScraping(url);\nconst $productPage = cheerio.load(productResponse.body);\n\n// Use the data extraction logic from above.\n// If copy pasting, be careful about $ -> $productPage.\nconst title = $productPage('h1').text().trim();\nconst vendor = $productPage('a.product-meta__vendor').text().trim();\nconst price = $productPage('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($productPage('span.rating__caption').text(), 10);\nconst description = $productPage('div[class*=\"description\"] div.rte').text().trim();\n\nresults.push({\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\n});\n} catch (error) {\n// Save information about the error to the\n// \"errors\" array to see what's happened.\nerrors.push({ url, msg: error.message });\n}\n}\nFinally, let's combine the above code blocks into a full runnable example. When you run the below code, it will scrape detailed information about all the products on the first page of the Warehouse Sales category. We added a few console logs throughout the code to see what's going on.\nimport { gotScraping } from 'got-scraping';\nimport * as cheerio from 'cheerio';\n\nconst WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\nconst storeUrl = `${WEBSITE_URL}/collections/sales`;\n\nconsole.log('Fetching products on sale.');\nconst response = await gotScraping(storeUrl);\nconst html = response.body;\n\nconst $ = cheerio.load(html);\nconst productLinks = $('a.product-item__title');\n\nconst productUrls = [];\nfor (const link of productLinks) {\nconst relativeUrl = $(link).attr('href');\nconst absoluteUrl = new URL(relativeUrl, WEBSITE_URL);\nproductUrls.push(absoluteUrl);\n}\n\nconsole.log(`Found ${productUrls.length} products.`);\n\nconst results = [];\nconst errors = [];\n\nfor (const url of productUrls) {\ntry {\nconsole.log(`Fetching URL: ${url}`);\nconst productResponse = await gotScraping(url);\nconst $productPage = cheerio.load(productResponse.body);\n\nconst title = $productPage('h1').text().trim();\nconst vendor = $productPage('a.product-meta__vendor').text().trim();\nconst price = $productPage('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($productPage('span.rating__caption').text(), 10);\nconst description = $productPage('div[class*=\"description\"] div.rte').text().trim();\n\nresults.push({\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\n});\n} catch (error) {\nerrors.push({ url, msg: error.message });\n}\n}\n\nconsole.log('RESULTS:', results);\nconsole.log('ERRORS:', errors);\nAnd here's an example of the results you will see after running the above code. We truncated the descriptions for readability. There should be 24 products in your list.\n[\n{\ntitle: 'JBL Flip 4 Waterproof Portable Bluetooth Speaker',\nvendor: 'JBL',\nprice: '$74.95',\nreviewCount: 2,\ndescription: 'JBL Flip 4 is the next generation in the ...',\n},\n{\ntitle: 'Sony XBR-950G BRAVIA 4K HDR Ultra HD TV',\nvendor: 'Sony',\nprice: '$1,398.00',\nreviewCount: 3,\ndescription: 'Unlock the world of ultimate colors and ...',\n},\n{\ntitle: 'Sony SACS9 10\" Active Subwoofer',\nvendor: 'Sony',\nprice: '$158.00',\nreviewCount: 3,\ndescription: 'Put more punch in your movie ...',\n},\n];\nThat's it for the absolute basics of crawling, but we're not done yet. We scraped 24 products from the first page of the Sales category, but the category actually has 50 products on 3 pages. You will learn how to visit all the pages and scrape all the products in the following lessons.\nNext up​\nIn the next lesson we will rewrite the scraper using an open-source web scraping library called Crawlee. It will make the scraper more robust while speeding up development at the same time.",
  "markdown": "# Scraping data | Academy | Apify Documentation\n\n**Learn how to add data extraction logic to your crawler, which will allow you to extract data from all the websites you crawled.**\n\n* * *\n\nAt the [very beginning of this course](https://docs.apify.com/academy/web-scraping-for-beginners), we learned that the term web scraping usually means a combined process of data extraction and crawling. And this is exactly what we'll do in this lesson. We will take the crawling code from the previous lesson, and we will combine it with data extraction code and turn everything into a web scraper.\n\nThe term product detail page (or PDP) is commonly used on e-commerce websites to describe the page where you can find detailed information about a product. In the Warehouse store, there's, for example, [this page describing Denon headphones](https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones).\n\nLet's start writing a script that extracts data from this single PDP. We can use this familiar code as a boilerplate.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';const response = await gotScraping(productUrl);const html = response.body;const $ = cheerio.load(html);// Attribute extraction code will go here.\n```\n\nWe will use the techniques learned in the [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) section to find and extract the following product attributes:\n\n*   title\n*   vendor\n*   price\n*   number of reviews\n*   description\n\n![scraping-title.png](https://docs.apify.com/assets/images/scraping-title-b9819987fa0d66c00b3e394c42ba8f98.png)\n\n> For brevity, we won't explain how to extract every attribute step-by-step. Review the [Basics of data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) section to learn about DevTools and extracting data.\n\n### Title[​](#title \"Direct link to Title\")\n\nGetting the title is quite straightforward. We recommend using `h1` for titles where available, because it's the semantically correct way and therefore unlikely to change.\n\n```\nconst title = $('h1').text().trim();\n```\n\n### Vendor[​](#vendor \"Direct link to Vendor\")\n\nVendor name is available as a link with the `product-meta__vendor` class. We're only interested in the text though.\n\n```\nconst vendor = $('a.product-meta__vendor').text().trim();\n```\n\n### Price[​](#price \"Direct link to Price\")\n\nWe will take a shortcut here and only extract the price as a string that includes currency. In production scrapers, you might want to split it into two fields.\n\n```\nconst price = $('span.price').contents()[2].nodeValue;\n```\n\n### Number of reviews[​](#number-of-reviews \"Direct link to Number of reviews\")\n\nFor the review count, we use the `parseInt()` function to get only the number. Otherwise, we would receive a string like **2 reviews** from this element.\n\n```\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\n```\n\n### Description[​](#description \"Direct link to Description\")\n\nGetting the description is fairly straightforward as well, but notice the two selectors separated by a space: `div[class*=\"description\"] div.rte`. This is called a [descendant combinator](https://developer.mozilla.org/en-US/docs/Web/CSS/Descendant_combinator), and it allows you to search for child elements within parent elements. Using any of the selectors separately would lead to unwanted strings in our result.\n\n```\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n```\n\nThis is the final code after putting all the extractors together with the initial boilerplate. It will scrape all the requested attributes from the single URL and print them to the terminal.\n\nSave it into a new file called `product.js` and run it with `node product.js` to see for yourself.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const productUrl = 'https://warehouse-theme-metal.myshopify.com/products/denon-ah-c720-in-ear-headphones';const response = await gotScraping(productUrl);const html = response.body;const $ = cheerio.load(html);const title = $('h1').text().trim();const vendor = $('a.product-meta__vendor').text().trim();const price = $('span.price').contents()[2].nodeValue;const reviewCount = parseInt($('span.rating__caption').text(), 10);const description = $('div[class*=\"description\"] div.rte').text().trim();const product = {    title,    vendor,    price,    reviewCount,    description,};console.log(product);\n```\n\n## Crawling product detail pages[​](#crawling \"Direct link to Crawling product detail pages\")\n\nLet's compare the above data extraction example with the crawling code we wrote in the last lesson:\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';const storeUrl = `${WEBSITE_URL}/collections/sales`;const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const productLinks = $('a.product-item__title');const productUrls = [];for (const link of productLinks) {    const relativeUrl = $(link).attr('href');    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);    productUrls.push(absoluteUrl);}for (const url of productUrls) {    try {        const productResponse = await gotScraping(url);        const productHtml = productResponse.body;        const $productPage = cheerio.load(productHtml);        const productPageTitle = $productPage('h1').text().trim();        console.log(productPageTitle);    } catch (error) {        console.error(error.message, url);    }}\n```\n\nWe can see that the code is quite similar. Both scripts download HTML and then process the HTML. To understand how to put them together, we'll go back to the [original process of crawling](https://docs.apify.com/academy/web-scraping-for-beginners/crawling).\n\n1.  Visit the start URL.\n2.  Extract the next URLs (and data) and save them.\n3.  Visit one of the collected URLs and save data and/or more URLs.\n4.  Repeat step 3 until you have everything you need.\n\nUsing this flow as guidance, we should be able to connect the pieces of code together to build a scraper which crawls through the products found in the [Sales category of Warehouse](https://warehouse-theme-metal.myshopify.com/collections/sales), and then scrapes the **title**, **vendor**, **price**, **review count**, and **description** of each of them.\n\n## Building the final scraper[​](#building-scraper \"Direct link to Building the final scraper\")\n\nLet's create a brand-new file called **final.js** and write our scraper code there. We'll show the code step by step for easier orientation. At the end, we'll combine the pieces into a runnable example.\n\nWe'll start by adding our imports and constants at the top of the file, no changes there.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';\n```\n\nThen we need to **visit the start URL**. To scrape all the on-sale product links, we need the Sales page as the start URL.\n\n```\n// ...const storeUrl = `${WEBSITE_URL}/collections/sales`;const response = await gotScraping(storeUrl);const html = response.body;\n```\n\nAfter that, we need to **extract the next URLs** we want to visit (the product detail page URLs). Thus far, the code is exactly the same as the **crawler.js** code.\n\n```\n// ...const $ = cheerio.load(html);const productLinks = $('a.product-item__title');const productUrls = [];for (const link of productLinks) {    const relativeUrl = $(link).attr('href');    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);    productUrls.push(absoluteUrl);}\n```\n\nNow the code will start to differ. We will use the crawling logic from earlier to visit all the URLs, but we will replace the placeholder extraction logic we had there. The placeholder logic only extracted the product's title, but we want the vendor, price, number of reviews and description as well.\n\n```\n// ...// A new array to save each product in.const results = [];// An optional array we can save errors to.const errors = [];for (const url of productUrls) {    try {        // Download HTML of each product detail.        const productResponse = await gotScraping(url);        const $productPage = cheerio.load(productResponse.body);        // Use the data extraction logic from above.        // If copy pasting, be careful about $ -> $productPage.        const title = $productPage('h1').text().trim();        const vendor = $productPage('a.product-meta__vendor').text().trim();        const price = $productPage('span.price').contents()[2].nodeValue;        const reviewCount = parseInt($productPage('span.rating__caption').text(), 10);        const description = $productPage('div[class*=\"description\"] div.rte').text().trim();        results.push({            title,            vendor,            price,            reviewCount,            description,        });    } catch (error) {        // Save information about the error to the        // \"errors\" array to see what's happened.        errors.push({ url, msg: error.message });    }}\n```\n\nFinally, let's combine the above code blocks into a full runnable example. When you run the below code, it will scrape detailed information about all the products on the first page of the [Warehouse Sales category](https://warehouse-theme-metal.myshopify.com/collections/sales). We added a few console logs throughout the code to see what's going on.\n\n```\nimport { gotScraping } from 'got-scraping';import * as cheerio from 'cheerio';const WEBSITE_URL = 'https://warehouse-theme-metal.myshopify.com';const storeUrl = `${WEBSITE_URL}/collections/sales`;console.log('Fetching products on sale.');const response = await gotScraping(storeUrl);const html = response.body;const $ = cheerio.load(html);const productLinks = $('a.product-item__title');const productUrls = [];for (const link of productLinks) {    const relativeUrl = $(link).attr('href');    const absoluteUrl = new URL(relativeUrl, WEBSITE_URL);    productUrls.push(absoluteUrl);}console.log(`Found ${productUrls.length} products.`);const results = [];const errors = [];for (const url of productUrls) {    try {        console.log(`Fetching URL: ${url}`);        const productResponse = await gotScraping(url);        const $productPage = cheerio.load(productResponse.body);        const title = $productPage('h1').text().trim();        const vendor = $productPage('a.product-meta__vendor').text().trim();        const price = $productPage('span.price').contents()[2].nodeValue;        const reviewCount = parseInt($productPage('span.rating__caption').text(), 10);        const description = $productPage('div[class*=\"description\"] div.rte').text().trim();        results.push({            title,            vendor,            price,            reviewCount,            description,        });    } catch (error) {        errors.push({ url, msg: error.message });    }}console.log('RESULTS:', results);console.log('ERRORS:', errors);\n```\n\nAnd here's an example of the results you will see after running the above code. We truncated the descriptions for readability. There should be 24 products in your list.\n\n```\n[    {        title: 'JBL Flip 4 Waterproof Portable Bluetooth Speaker',        vendor: 'JBL',        price: '$74.95',        reviewCount: 2,        description: 'JBL Flip 4 is the next generation in the ...',    },    {        title: 'Sony XBR-950G BRAVIA 4K HDR Ultra HD TV',        vendor: 'Sony',        price: '$1,398.00',        reviewCount: 3,        description: 'Unlock the world of ultimate colors and ...',    },    {        title: 'Sony SACS9 10\" Active Subwoofer',        vendor: 'Sony',        price: '$158.00',        reviewCount: 3,        description: 'Put more punch in your movie ...',    },];\n```\n\nThat's it for the absolute basics of crawling, but we're not done yet. We scraped 24 products from the first page of the Sales category, but the category actually has 50 products on 3 pages. You will learn how to visit all the pages and scrape all the products in the following lessons.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nIn the [next lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/pro-scraping) we will rewrite the scraper using an open-source web scraping library called [Crawlee](https://crawlee.dev/). It will make the scraper more robust while speeding up development at the same time.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/exporting-data",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/exporting-data",
    "loadedTime": "2025-07-21T15:51:10.502Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/exporting-data",
    "title": "Exporting data | Academy | Apify Documentation",
    "description": "Learn how to export the data you scraped using Crawlee to CSV or JSON.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/exporting-data"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Exporting data | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to export the data you scraped using Crawlee to CSV or JSON."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Exporting+data"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "10610",
      "date": "Mon, 21 Jul 2025 15:51:07 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-f31b\"",
      "expires": "Mon, 21 Jul 2025 16:01:07 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "D1D0:D3EFD:147D6A:15F367:687E61EB",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000142-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113068.781962, VS0, VE18",
      "x-fastly-request-id": "b4a69f62325da3847f7d8fa4425eaa6f1d30f005",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "IiUNXAveI4TeD9-ad0CuyUUwVCc16bh2MUr8_GDzyRp74VzRsS1A9A==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Exporting data | Academy | Apify Documentation\nLearn how to export the data you scraped using Crawlee to CSV or JSON.\nBut when we look inside the folder, we see that there are a lot of files, and we don't want to work with those manually. We can use the dataset itself to export the data.\nCrawlee's Dataset provides a way to export all your scraped data into one big CSV file. You can then open it in Excel or any other data processor. To do that, you need to call Dataset.exportToCSV() after collecting all the data. That means, after your crawler run finishes.\nAfter you add this one line and run the code, you'll find your CSV with all the scraped products in here:\n./storage/key-value-stores/default/results.csv\nExporting to JSON is very similar to exporting to CSV, but we'll use a different function: Dataset.exportToJSON. Exporting to JSON is useful when you don't want to work with each item separately, but would rather have one big JSON file with all the results.\n./storage/key-value-stores/default/results.json\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n// We removed the headless: false option to hide the browser windows.\nrequestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\nconsole.log(`Fetching URL: ${request.url}`);\n\nif (request.label === 'start-url') {\nawait enqueueLinks({\nselector: 'a.product-item__title',\n});\nreturn;\n}\n\n// Fourth, parse the browser's page with Cheerio.\nconst $ = await parseWithCheerio();\n\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\nconst recommendedProducts = $('.product-recommendations a.product-item__title')\n.map((i, el) => $(el).text().trim())\n.toArray();\n\nawait Dataset.pushData({\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\nrecommendedProducts,\n});\n},\n});\n\nawait crawler.addRequests([{\nurl: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\nlabel: 'start-url',\n}]);\n\nawait crawler.run();\nawait Dataset.exportToCSV('results');",
  "markdown": "# Exporting data | Academy | Apify Documentation\n\n**Learn how to export the data you scraped using Crawlee to CSV or JSON.**\n\nBut when we look inside the folder, we see that there are a lot of files, and we don't want to work with those manually. We can use the dataset itself to export the data.\n\nCrawlee's `Dataset` provides a way to export all your scraped data into one big CSV file. You can then open it in Excel or any other data processor. To do that, you need to call [`Dataset.exportToCSV()`](https://crawlee.dev/api/core/class/Dataset#exportToCSV) after collecting all the data. That means, after your crawler run finishes.\n\nAfter you add this one line and run the code, you'll find your CSV with all the scraped products in here:\n\n```\n./storage/key-value-stores/default/results.csv\n```\n\nExporting to JSON is very similar to exporting to CSV, but we'll use a different function: [`Dataset.exportToJSON`](https://crawlee.dev/api/core/class/Dataset#exportToJSON). Exporting to JSON is useful when you don't want to work with each item separately, but would rather have one big JSON file with all the results.\n\n```\n./storage/key-value-stores/default/results.json\n```\n\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';const crawler = new PlaywrightCrawler({    // We removed the headless: false option to hide the browser windows.    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {        console.log(`Fetching URL: ${request.url}`);        if (request.label === 'start-url') {            await enqueueLinks({                selector: 'a.product-item__title',            });            return;        }        // Fourth, parse the browser's page with Cheerio.        const $ = await parseWithCheerio();        const title = $('h1').text().trim();        const vendor = $('a.product-meta__vendor').text().trim();        const price = $('span.price').contents()[2].nodeValue;        const reviewCount = parseInt($('span.rating__caption').text(), 10);        const description = $('div[class*=\"description\"] div.rte').text().trim();        const recommendedProducts = $('.product-recommendations a.product-item__title')            .map((i, el) => $(el).text().trim())            .toArray();        await Dataset.pushData({            title,            vendor,            price,            reviewCount,            description,            recommendedProducts,        });    },});await crawler.addRequests([{    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',    label: 'start-url',}]);await crawler.run();await Dataset.exportToCSV('results');\n```",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge",
    "loadedTime": "2025-07-21T15:51:18.257Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge",
    "title": "Challenge | Academy | Apify Documentation",
    "description": "Test your knowledge acquired in the previous sections of this course by building an Amazon scraper using Crawlee's CheerioCrawler!",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Challenge | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Test your knowledge acquired in the previous sections of this course by building an Amazon scraper using Crawlee's CheerioCrawler!"
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Challenge"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "9645",
      "date": "Mon, 21 Jul 2025 15:51:17 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-acbc\"",
      "expires": "Mon, 21 Jul 2025 16:01:17 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "25F4:141935:ED723:104BC3:687E61F5",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100142-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113077.211578, VS0, VE15",
      "x-fastly-request-id": "df7d51f77577ad9a618a239d3eff6db1432ef5ad",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "OxW6eL9pnWaMU2f_YC2X2eMN2ylaag_L8bTPS_5ILEtjH07UCZy-tQ==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Challenge | Academy | Apify Documentation\nTest your knowledge acquired in the previous sections of this course by building an Amazon scraper using Crawlee's CheerioCrawler!\nBefore moving onto the other courses in the academy, we recommend following along with this section, as it combines everything you've learned in the previous lessons into one cohesive project that helps you prove to yourself that you've thoroughly understood the material.\nWe recommend that you make sure you've gone through both the data extraction and crawling sections of this course to ensure the smoothest development process.\nLearning 🧠​\nBefore continuing, it is highly recommended to do the following:\nLook over how to build a crawler in Crawlee and ideally code along.\nRead this short article about request labels (this will be extremely useful later on).\nCheck out this tutorial about dynamic pages.\nRead about the RequestQueue.\nOur task​\nOn Amazon, we can use this link to get to the results page of any product we want:\nhttps://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=KEYWORD\nOur crawler's input will look like this:\nThe goal at hand is to scrape all of the products from the first page of results for whatever keyword was provided (for our test case, it will be iPhone), then to scrape all available offers of each product and push the results to the dataset. For context, the offers for a product look like this:\nIn the end, we'd like our final output to look something like this:\n[\n{\n\"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",\n\"asin\": \"B07P6Y7954\",\n\"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",\n\"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",\n\"keyword\": \"iphone\",\n\"seller name\": \"Blutek Intl\",\n\"offer\": \"$162.97\"\n},\n{\n\"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",\n\"asin\": \"B07P6Y7954\",\n\"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",\n\"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",\n\"keyword\": \"iphone\",\n\"sellerName\": \"PLATINUM DEALS\",\n\"offer\": \"$169.98\"\n},\n{\n\"...\": \"...\"\n}\n]\n\nThe asin is the ID of the product, which is data present on the Amazon website.\nEach of the items in the dataset will represent a scraped offer and will have the same title, asin, itemUrl, and description. The offer-specific fields will be sellerName and offer.\nFirst up​\nFrom this course, you should have all the knowledge to build this scraper by yourself. Give it a try, then come back to compare your scraper with our solution.\nThe challenge can be completed using either CheerioCrawler or PlaywrightCrawler. Playwright is significantly slower but doesn't get blocked as much. You will learn the most by implementing both.\nLet's start off this section by initializing and setting up our project with the Crawlee CLI (don't worry, no additional installation is required).",
  "markdown": "# Challenge | Academy | Apify Documentation\n\n**Test your knowledge acquired in the previous sections of this course by building an Amazon scraper using Crawlee's CheerioCrawler!**\n\n* * *\n\nBefore moving onto the other courses in the academy, we recommend following along with this section, as it combines everything you've learned in the previous lessons into one cohesive project that helps you prove to yourself that you've thoroughly understood the material.\n\nWe recommend that you make sure you've gone through both the [data extraction](https://docs.apify.com/academy/web-scraping-for-beginners/data-extraction) and [crawling](https://docs.apify.com/academy/web-scraping-for-beginners/crawling) sections of this course to ensure the smoothest development process.\n\n## Learning 🧠[​](#learning \"Direct link to Learning 🧠\")\n\nBefore continuing, it is highly recommended to do the following:\n\n*   Look over [how to build a crawler in Crawlee](https://crawlee.dev/docs/introduction/first-crawler) and ideally **code along**.\n*   Read [this short article](https://docs.apify.com/academy/node-js/request-labels-in-apify-actors) about [**request labels**](https://crawlee.dev/api/core/class/Request#label) (this will be extremely useful later on).\n*   Check out [this tutorial](https://docs.apify.com/academy/node-js/dealing-with-dynamic-pages) about dynamic pages.\n*   Read about the [RequestQueue](https://crawlee.dev/api/core/class/RequestQueue).\n\n## Our task[​](#our-task \"Direct link to Our task\")\n\nOn Amazon, we can use this link to get to the results page of any product we want:\n\n```\nhttps://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=KEYWORD\n```\n\nOur crawler's input will look like this:\n\nThe goal at hand is to scrape all of the products from the first page of results for whatever keyword was provided (for our test case, it will be **iPhone**), then to scrape all available offers of each product and push the results to the dataset. For context, the offers for a product look like this:\n\n![Amazon product offers](https://docs.apify.com/assets/images/product-offers-20910dbac0f5cc3df6089143b924ac5b.jpg)\n\nIn the end, we'd like our final output to look something like this:\n\n```\n[    {        \"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",        \"asin\": \"B07P6Y7954\",        \"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",        \"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",        \"keyword\": \"iphone\",        \"seller name\": \"Blutek Intl\",        \"offer\": \"$162.97\"    },    {        \"title\": \"Apple iPhone 6 a1549 16GB Space Gray Unlocked (Certified Refurbished)\",        \"asin\": \"B07P6Y7954\",        \"itemUrl\": \"https://www.amazon.com/Apple-iPhone-Unlocked-Certified-Refurbished/dp/B00YD547Q6/ref=sr_1_2?s=wireless&ie=UTF8&qid=1539772626&sr=1-2&keywords=iphone\",        \"description\": \"What's in the box: Certified Refurbished iPhone 6 Space Gray 16GB Unlocked , USB Cable/Adapter. Comes in a Generic Box with a 1 Year Limited Warranty.\",        \"keyword\": \"iphone\",        \"sellerName\": \"PLATINUM DEALS\",        \"offer\": \"$169.98\"    },    {        \"...\": \"...\"    }]\n```\n\n> The `asin` is the ID of the product, which is data present on the Amazon website.\n\nEach of the items in the dataset will represent a scraped offer and will have the same `title`, `asin`, `itemUrl`, and `description`. The offer-specific fields will be `sellerName` and `offer`.\n\n## First up[​](#next \"Direct link to First up\")\n\nFrom this course, you should have all the knowledge to build this scraper by yourself. Give it a try, then come back to compare your scraper with our solution.\n\nThe challenge can be completed using either [CheerioCrawler](https://crawlee.dev/api/cheerio-crawler/class/CheerioCrawler) or [PlaywrightCrawler](https://crawlee.dev/api/playwright-crawler/class/PlaywrightCrawler). Playwright is significantly slower but doesn't get blocked as much. You will learn the most by implementing both.\n\nLet's start off this section by [initializing and setting up](https://docs.apify.com/academy/web-scraping-for-beginners/challenge/initializing-and-setting-up) our project with the Crawlee CLI (don't worry, no additional installation is required).",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/headless-browser",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/headless-browser",
    "loadedTime": "2025-07-21T15:51:12.626Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/headless-browser",
    "title": "Headless browsers | Academy | Apify Documentation",
    "description": "Learn how to scrape the web with a headless browser using only a few lines of code. Chrome, Firefox, Safari, Edge - all are supported.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/crawling/headless-browser"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Headless browsers | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Learn how to scrape the web with a headless browser using only a few lines of code. Chrome, Firefox, Safari, Edge - all are supported."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Headless+browsers"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "13405",
      "date": "Mon, 21 Jul 2025 15:51:06 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-160dc\"",
      "expires": "Mon, 21 Jul 2025 16:01:06 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "EA83:2E8207:132383:14994F:687E61EA",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000038-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113067.592620, VS0, VE11",
      "x-fastly-request-id": "7a27d744a1959e3a54120db4c1efc3c052e084d6",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "0oYo2Uam1Uve3fgpz1k4Pl1udWN7afv1jepKVhSHdNGNb3va_4T1ZQ==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Headless browsers | Academy | Apify Documentation\nLearn how to scrape the web with a headless browser using only a few lines of code. Chrome, Firefox, Safari, Edge - all are supported.\nA headless browser is a browser that runs without a user interface (UI). This means that it's normally controlled by automated scripts. Headless browsers are very popular in scraping because they can help you render JavaScript or programmatically behave like a human user to prevent blocking. The two most popular libraries for controlling headless browsers are Puppeteer and Playwright. Crawlee supports both.\nBuilding a Playwright scraper​\nOur focus will be on Playwright, which boasts additional features and better documentation. Notably, it originates from the same team responsible for Puppeteer.\nCrawlee has a built-in support for building Playwright scrapers. Let's reuse code of the Cheerio scraper from the previous lesson. It'll take us just a few changes to turn it into a full headless scraper.\nFirst, we must install Playwright into our project. It's not included in Crawlee, because it's quite large as it bundles all the browsers.\nAfter Playwright installs, we can proceed with updating the scraper code. Let's create a new file called browser.js and put the new code there. As always, the comments in the example describe changes in the code. Everything else is the same as before.\n// First, import PlaywrightCrawler instead of CheerioCrawler\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n// Second, tell the browser to run with visible UI,\n// so that we can see what's going on.\nheadless: false,\n// Third, replace $ with parseWithCheerio function.\nrequestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\nconsole.log(`Fetching URL: ${request.url}`);\n\nif (request.label === 'start-url') {\nawait enqueueLinks({\nselector: 'a.product-item__title',\n});\nreturn;\n}\n\n// Fourth, parse the browser's page with Cheerio.\nconst $ = await parseWithCheerio();\n\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n\nawait Dataset.pushData({\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\n});\n},\n});\n\nawait crawler.addRequests([{\nurl: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\nlabel: 'start-url',\n}]);\n\nawait crawler.run();\ntip\nThe parseWithCheerio function is available even in CheerioCrawler and all the other Crawlee crawlers. If you think you'll often switch up the crawlers, you can use it to further reduce the number of needed line changes.\nWhen you run the code with node browser.js, you'll see a browser window open and then the individual pages getting scraped, each in a new browser tab.\nThat's it. In 4 lines of code, we transformed our crawler from a static HTTP crawler to a headless browser crawler. The crawler now runs the same as before, but uses a Chromium browser instead of plain HTTP requests. This isn't possible without Crawlee.\nUsing Playwright in combination with Cheerio like this is only one of many ways how you can utilize Playwright (and Puppeteer) with Crawlee. In the advanced courses of the Academy, we will go deeper into using headless browsers for scraping and web automation (RPA) use cases.\nRunning in headless mode​\nWe said that headless browsers didn't have a UI, but while scraping with the above scraper code, you could definitely see the browser. That's because we added the headless: false option. This is useful for debugging and seeing what's going on in the browser. Once your scraper is complete, you can remove the line and the crawler will run without a UI.\nYou can also switch between headless and headful (with UI) using the CRAWLEE_HEADLESS environment variable. This allows you to change the mode without touching your code.\nMacOS/Linux\nWindows CMD\nWindows Powershell\nCRAWLEE_HEADLESS=1 node browser.js\nDynamically loaded data​\nOne of the important benefits of using a browser is that it allows you to extract data that's dynamically loaded, such as data that's only fetched after a user scrolls or interacts with the page. In our case, it's the \"You may also like\" section of the product detail pages. Those products aren't available in the initial HTML, but the browser loads them later using an API.\nIf we added an appropriate selector to our original CheerioCrawler code, it would not extract the information, but a browser automatically fetches and renders this extra data.\nLet's add this new extractor to our code. It collects the names of the recommended products.\n// ...\nconst recommendedProducts = $('.product-recommendations a.product-item__title')\n.map((i, el) => $(el).text().trim())\n.toArray();\n// ...\nawait Dataset.pushData({\n// ...\nrecommendedProducts,\n});\nAnd here's the complete, runnable code:\nimport { PlaywrightCrawler, Dataset } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n// We removed the headless: false option to hide the browser windows.\nrequestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {\nconsole.log(`Fetching URL: ${request.url}`);\n\nif (request.label === 'start-url') {\nawait enqueueLinks({\nselector: 'a.product-item__title',\n});\nreturn;\n}\n\n// Fourth, parse the browser's page with Cheerio.\nconst $ = await parseWithCheerio();\n\nconst title = $('h1').text().trim();\nconst vendor = $('a.product-meta__vendor').text().trim();\nconst price = $('span.price').contents()[2].nodeValue;\nconst reviewCount = parseInt($('span.rating__caption').text(), 10);\nconst description = $('div[class*=\"description\"] div.rte').text().trim();\n// We added one more extractor to get all the recommended products.\nconst recommendedProducts = $('.product-recommendations a.product-item__title')\n.map((i, el) => $(el).text().trim())\n.toArray();\n\nawait Dataset.pushData({\ntitle,\nvendor,\nprice,\nreviewCount,\ndescription,\n// And we saved the extracted product names.\nrecommendedProducts,\n});\n},\n});\n\nawait crawler.addRequests([{\nurl: 'https://warehouse-theme-metal.myshopify.com/collections/sales',\nlabel: 'start-url',\n}]);\n\nawait crawler.run();\nWhen you run the code, you'll find the recommended product names correctly extracted in the dataset files. If you tried the same with our earlier CheerioCrawler code, you would find the recommendedProducts array empty in your results. That's because Cheerio can't make the API call to retrieve the additional data, like a browser can.\nNext up​\nWe learned how to scrape with Cheerio and Playwright, but how do we export the data for further processing? Let's learn that in the next and final lesson of the Basics of crawling section.",
  "markdown": "# Headless browsers | Academy | Apify Documentation\n\n**Learn how to scrape the web with a headless browser using only a few lines of code. Chrome, Firefox, Safari, Edge - all are supported.**\n\n* * *\n\nA headless browser is a browser that runs without a user interface (UI). This means that it's normally controlled by automated scripts. Headless browsers are very popular in scraping because they can help you render JavaScript or programmatically behave like a human user to prevent blocking. The two most popular libraries for controlling headless browsers are [Puppeteer](https://pptr.dev/) and [Playwright](https://playwright.dev/). **Crawlee** supports both.\n\n## Building a Playwright scraper[​](#playwright-scraper \"Direct link to Building a Playwright scraper\")\n\n> Our focus will be on Playwright, which boasts additional features and better documentation. Notably, it originates from the same team responsible for Puppeteer.\n\nCrawlee has a built-in support for building Playwright scrapers. Let's reuse code of the Cheerio scraper from the previous lesson. It'll take us just a few changes to turn it into a full headless scraper.\n\nFirst, we must install Playwright into our project. It's not included in Crawlee, because it's quite large as it bundles all the browsers.\n\nAfter Playwright installs, we can proceed with updating the scraper code. Let's create a new file called `browser.js` and put the new code there. As always, the comments in the example describe changes in the code. Everything else is the same as before.\n\n```\n// First, import PlaywrightCrawler instead of CheerioCrawlerimport { PlaywrightCrawler, Dataset } from 'crawlee';const crawler = new PlaywrightCrawler({    // Second, tell the browser to run with visible UI,    // so that we can see what's going on.    headless: false,    // Third, replace $ with parseWithCheerio function.    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {        console.log(`Fetching URL: ${request.url}`);        if (request.label === 'start-url') {            await enqueueLinks({                selector: 'a.product-item__title',            });            return;        }        // Fourth, parse the browser's page with Cheerio.        const $ = await parseWithCheerio();        const title = $('h1').text().trim();        const vendor = $('a.product-meta__vendor').text().trim();        const price = $('span.price').contents()[2].nodeValue;        const reviewCount = parseInt($('span.rating__caption').text(), 10);        const description = $('div[class*=\"description\"] div.rte').text().trim();        await Dataset.pushData({            title,            vendor,            price,            reviewCount,            description,        });    },});await crawler.addRequests([{    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',    label: 'start-url',}]);await crawler.run();\n```\n\ntip\n\nThe `parseWithCheerio` function is available even in `CheerioCrawler` and all the other Crawlee crawlers. If you think you'll often switch up the crawlers, you can use it to further reduce the number of needed line changes.\n\nWhen you run the code with `node browser.js`, you'll see a browser window open and then the individual pages getting scraped, each in a new browser tab.\n\nThat's it. In 4 lines of code, we transformed our crawler from a static HTTP crawler to a headless browser crawler. The crawler now runs the same as before, but uses a Chromium browser instead of plain HTTP requests. This isn't possible without Crawlee.\n\nUsing Playwright in combination with Cheerio like this is only one of many ways how you can utilize Playwright (and Puppeteer) with Crawlee. In the advanced courses of the Academy, we will go deeper into using headless browsers for scraping and web automation (RPA) use cases.\n\n## Running in headless mode[​](#running-headless \"Direct link to Running in headless mode\")\n\nWe said that headless browsers didn't have a UI, but while scraping with the above scraper code, you could definitely see the browser. That's because we added the `headless: false` option. This is useful for debugging and seeing what's going on in the browser. Once your scraper is complete, you can remove the line and the crawler will run without a UI.\n\nYou can also switch between headless and headful (with UI) using the [`CRAWLEE_HEADLESS`](https://crawlee.dev/docs/guides/configuration#crawlee_headless) environment variable. This allows you to change the mode without touching your code.\n\n*   MacOS/Linux\n*   Windows CMD\n*   Windows Powershell\n\n```\nCRAWLEE_HEADLESS=1 node browser.js\n```\n\n## Dynamically loaded data[​](#dynamic-data \"Direct link to Dynamically loaded data\")\n\nOne of the important benefits of using a browser is that it allows you to extract data that's dynamically loaded, such as data that's only fetched after a user scrolls or interacts with the page. In our case, it's the \"**You may also like**\" section of the product detail pages. Those products aren't available in the initial HTML, but the browser loads them later using an API.\n\n![headless-dynamic-data.png](https://docs.apify.com/assets/images/headless-dynamic-data-556e6fe0874146dbff6ccef48365ed66.png)\n\nIf we added an appropriate selector to our original `CheerioCrawler` code, it would not extract the information, but a browser automatically fetches and renders this extra data.\n\nLet's add this new extractor to our code. It collects the names of the recommended products.\n\n```\n// ...const recommendedProducts = $('.product-recommendations a.product-item__title')    .map((i, el) => $(el).text().trim())    .toArray();// ...await Dataset.pushData({    // ...    recommendedProducts,});\n```\n\nAnd here's the complete, runnable code:\n\n```\nimport { PlaywrightCrawler, Dataset } from 'crawlee';const crawler = new PlaywrightCrawler({    // We removed the headless: false option to hide the browser windows.    requestHandler: async ({ parseWithCheerio, request, enqueueLinks }) => {        console.log(`Fetching URL: ${request.url}`);        if (request.label === 'start-url') {            await enqueueLinks({                selector: 'a.product-item__title',            });            return;        }        // Fourth, parse the browser's page with Cheerio.        const $ = await parseWithCheerio();        const title = $('h1').text().trim();        const vendor = $('a.product-meta__vendor').text().trim();        const price = $('span.price').contents()[2].nodeValue;        const reviewCount = parseInt($('span.rating__caption').text(), 10);        const description = $('div[class*=\"description\"] div.rte').text().trim();        // We added one more extractor to get all the recommended products.        const recommendedProducts = $('.product-recommendations a.product-item__title')            .map((i, el) => $(el).text().trim())            .toArray();        await Dataset.pushData({            title,            vendor,            price,            reviewCount,            description,            // And we saved the extracted product names.            recommendedProducts,        });    },});await crawler.addRequests([{    url: 'https://warehouse-theme-metal.myshopify.com/collections/sales',    label: 'start-url',}]);await crawler.run();\n```\n\nWhen you run the code, you'll find the recommended product names correctly extracted in the dataset files. If you tried the same with our earlier `CheerioCrawler` code, you would find the `recommendedProducts` array empty in your results. That's because Cheerio can't make the API call to retrieve the additional data, like a browser can.\n\n## Next up[​](#next \"Direct link to Next up\")\n\nWe learned how to scrape with Cheerio and Playwright, but how do we export the data for further processing? Let's learn that in the [next and final lesson](https://docs.apify.com/academy/web-scraping-for-beginners/crawling/exporting-data) of the Basics of crawling section.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/initializing-and-setting-up",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/initializing-and-setting-up",
    "loadedTime": "2025-07-21T15:51:26.532Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/initializing-and-setting-up",
    "title": "Initializing & setting up | Academy | Apify Documentation",
    "description": "When you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/initializing-and-setting-up"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Initializing & setting up | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "When you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Initializing+%26+setting+up"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "9718",
      "date": "Mon, 21 Jul 2025 15:51:25 GMT",
      "server": "nginx",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-ba6d\"",
      "expires": "Mon, 21 Jul 2025 16:01:25 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "D484:27A1E:DCDD8:F432A:687E61FD",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100089-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113085.365243, VS0, VE14",
      "x-fastly-request-id": "ed84726ee3325c9813f98bc77789fea426fce324",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "2mRmHEawgoyubBzjXeTL_V-SaQKOpfQsaLMtfy4WFT4IL6ny90SzFg==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Initializing & setting up | Academy\nWhen you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.\nThe Crawlee CLI speeds up the process of setting up a Crawlee project. Navigate to the directory you'd like your project's folder to live, then open up a terminal instance and run the following command:\nOnce you run this command, you'll get prompted into a menu which you can navigate using your arrow keys. Each of these options will generate a different boilerplate code when selected. We're going to work with CheerioCrawler today, so we'll select the CheerioCrawler template project template, and then press Enter.\nOnce it's completed, open up the amazon-crawler folder that was generated by the npx crawlee create command. We're going to modify the main.js boilerplate to fit our needs:\n// main.js\nimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';\nimport { router } from './routes.js';\n\n// Grab our keyword from the input\nconst { keyword } = await KeyValueStore.getInput();\n\nconst crawler = new CheerioCrawler({\nrequestHandler: router,\n\n// If you have access to Apify Proxy, you can use residential proxies and\n// high retry count which helps with blocking\n// If you don't, your local IP address will likely be fine for a few requests if you scrape slowly.\n// proxyConfiguration: await Actor.createProxyConfiguration({ groups: ['RESIDENTIAL'] }),\n// maxRequestRetries: 10,\n});\n\nlog.info('Starting the crawl.');\nawait crawler.run([{\n// Turn the keyword into a link we can make a request with\nurl: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\nlabel: 'START',\nuserData: {\nkeyword,\n},\n}]);\nlog.info('Crawl finished.');\nFinally, we'll add the following input file to INPUT.json in the project's root directory (next to package.json, node_modules and others)",
  "markdown": "# Initializing & setting up | Academy\n\n**When you extract links from a web page, you often end up with a lot of irrelevant URLs. Learn how to filter the links to only keep the ones you need.**\n\nThe Crawlee CLI speeds up the process of setting up a Crawlee project. Navigate to the directory you'd like your project's folder to live, then open up a terminal instance and run the following command:\n\nOnce you run this command, you'll get prompted into a menu which you can navigate using your arrow keys. Each of these options will generate a different boilerplate code when selected. We're going to work with CheerioCrawler today, so we'll select the **CheerioCrawler template project** template, and then press **Enter**.\n\nOnce it's completed, open up the **amazon-crawler** folder that was generated by the `npx crawlee create` command. We're going to modify the **main.js** boilerplate to fit our needs:\n\n```\n// main.jsimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';import { router } from './routes.js';// Grab our keyword from the inputconst { keyword } = await KeyValueStore.getInput();const crawler = new CheerioCrawler({    requestHandler: router,    // If you have access to Apify Proxy, you can use residential proxies and    // high retry count which helps with blocking    // If you don't, your local IP address will likely be fine for a few requests if you scrape slowly.    // proxyConfiguration: await Actor.createProxyConfiguration({ groups: ['RESIDENTIAL'] }),    // maxRequestRetries: 10,});log.info('Starting the crawl.');await crawler.run([{    // Turn the keyword into a link we can make a request with    url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,    label: 'START',    userData: {        keyword,    },}]);log.info('Crawl finished.');\n```\n\nFinally, we'll add the following input file to **INPUT.json** in the project's root directory (next to `package.json`, `node_modules` and others)",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/modularity",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/modularity",
    "loadedTime": "2025-07-21T15:51:33.043Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/modularity",
    "title": "Modularity | Academy | Apify Documentation",
    "description": "Before you build your first web scraper with Crawlee, it is important to understand the concept of modularity in programming.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/modularity"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Modularity | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Before you build your first web scraper with Crawlee, it is important to understand the concept of modularity in programming."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Modularity"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "11289",
      "date": "Mon, 21 Jul 2025 15:51:31 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-10ff8\"",
      "expires": "Mon, 21 Jul 2025 16:01:31 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "C7D5:27339:B8AAA:D0079:687E6203",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000122-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113092.988681, VS0, VE8",
      "x-fastly-request-id": "b345e2a6e64a0fa1e72212bd32a754a1f3f3056b",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "bELqEAg04-jcB1E03qlrrVGGxaHLOrolJGxEyNd9JfWl10bljX654A==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Modularity | Academy | Apify Documentation\nBefore you build your first web scraper with Crawlee, it is important to understand the concept of modularity in programming.\nNow that we've gotten our first request going, the first challenge is going to be selecting all of the resulting products on the page. Back in the browser, we'll use the DevTools hover tool to inspect a product.\nBingo! Each product seems to have a data-asin attribute, which includes the ASIN (product ID) data we want. Now, we can select each of these elements with this selector: div > div[data-asin]:not([data-asin=\"\"]). Then, we'll scrape some data about each product, and push a request to the main product page so we can grab hold of the description.\nBut, before we start scraping, let's pause to talk a bit about the important concept of modularity. You may have noticed the src folder inside of your project, which by default has a routes.js file in it. We're going to use this to create modularized functions which can then be conditionally executed by our crawler.\n// routes.js\nimport { createCheerioRouter } from 'crawlee';\nimport { BASE_URL } from './constants.js';\n\nexport const router = createCheerioRouter();\n\nrouter.addDefaultHandler(({ log }) => {\nlog.info('Route reached.');\n});\n\n// Add a handler to our router to handle requests with the 'START' label\nrouter.addHandler('START', async ({ $, crawler, request }) => {\nconst { keyword } = request.userData;\n\nconst products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\n// loop through the resulting products\nfor (const product of products) {\nconst element = $(product);\nconst titleElement = $(element.find('.a-text-normal[href]'));\n\nconst url = `${BASE_URL}${titleElement.attr('href')}`;\n\n// scrape some data from each and to a request\n// to the crawler for its page\nawait crawler.addRequests([{\nurl,\nlabel: 'PRODUCT',\nuserData: {\n// Pass the scraped data about the product to the next\n// request so that it can be used there\ndata: {\ntitle: titleElement.first().text().trim(),\nasin: element.attr('data-asin'),\nitemUrl: url,\nkeyword,\n},\n},\n}]);\n}\n});\n\nrouter.addHandler('PRODUCT', ({ log }) => log.info('on a product page!'));\nAlso notice that we are importing BASE_URL from constants.js. Here is what that file looks like:\n// constants.js\nexport const BASE_URL = 'https://www.amazon.com';\nAnd here is what our main.js file currently looks like:\n// main.js\nimport { CheerioCrawler, log, KeyValueStore } from 'crawlee';\nimport { router } from './routes.js';\nimport { BASE_URL } from './constants.js';\n\nconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};\n\nconst crawler = new CheerioCrawler({\nrequestHandler: router,\n});\n\nawait crawler.addRequests([\n{\n// Use BASE_URL here instead\nurl: `${BASE_URL}/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\nlabel: 'START',\nuserData: {\nkeyword,\n},\n},\n]);\n\nlog.info('Starting the crawl.');\nawait crawler.run();\nlog.info('Crawl finished.');\nOne of the main reasons we modularize our code is to prevent massive and difficult to read files by separating concerns into separate files. In our main.js file, we're handling the initialization, configuration, and running of our crawler. In routes.js, we determine how the crawler should handle different routes, and in constants.js we define non-changing values that will be used throughout the project.\nOrganized code makes everyone happy, including you - the one developing the scraper! Spaghetti is super awesome, but not when it comes to programming 🍝\nThis can even be optimized further by putting our label items into constants.js, like so:\n// constants.js\nexport const BASE_URL = 'https://www.amazon.com';\n\nexport const labels = {\nSTART: 'START',\nPRODUCT: 'PRODUCT',\nOFFERS: 'OFFERS',\n};\nThen, the labels can be used by importing labels and accessing labels.START, labels.PRODUCT, or labels.OFFERS.\nThis is not necessary, but it is best practice, as it can prevent dumb typos that can cause nasty bugs 🐞 For the rest of this lesson, all of the examples using labels will be using the imported versions.\nIf you haven't already read the Best practices lesson in the Web scraping basics for JavaScript devs course, please give it a read.\nNext up​\nNow that we've gotten that out of the way, we can finally continue with our Amazon scraper. Let's do it!",
  "markdown": "# Modularity | Academy | Apify Documentation\n\n**Before you build your first web scraper with Crawlee, it is important to understand the concept of modularity in programming.**\n\n* * *\n\nNow that we've gotten our first request going, the first challenge is going to be selecting all of the resulting products on the page. Back in the browser, we'll use the DevTools hover tool to inspect a product.\n\n![Result products](https://docs.apify.com/assets/images/result-items-b1e131e323428eb8baaaf4e991bb79b5.jpg)\n\n**Bingo!** Each product seems to have a `data-asin` attribute, which includes the ASIN (product ID) data we want. Now, we can select each of these elements with this selector: `div > div[data-asin]:not([data-asin=\"\"])`. Then, we'll scrape some data about each product, and push a request to the main product page so we can grab hold of the description.\n\nBut, before we start scraping, let's pause to talk a bit about the important concept of **modularity**. You may have noticed the **src** folder inside of your project, which by default has a **routes.js** file in it. We're going to use this to create modularized functions which can then be conditionally executed by our crawler.\n\n```\n// routes.jsimport { createCheerioRouter } from 'crawlee';import { BASE_URL } from './constants.js';export const router = createCheerioRouter();router.addDefaultHandler(({ log }) => {    log.info('Route reached.');});// Add a handler to our router to handle requests with the 'START' labelrouter.addHandler('START', async ({ $, crawler, request }) => {    const { keyword } = request.userData;    const products = $('div > div[data-asin]:not([data-asin=\"\"])');    // loop through the resulting products    for (const product of products) {        const element = $(product);        const titleElement = $(element.find('.a-text-normal[href]'));        const url = `${BASE_URL}${titleElement.attr('href')}`;        // scrape some data from each and to a request        // to the crawler for its page        await crawler.addRequests([{            url,            label: 'PRODUCT',            userData: {                // Pass the scraped data about the product to the next                // request so that it can be used there                data: {                    title: titleElement.first().text().trim(),                    asin: element.attr('data-asin'),                    itemUrl: url,                    keyword,                },            },        }]);    }});router.addHandler('PRODUCT', ({ log }) => log.info('on a product page!'));\n```\n\nAlso notice that we are importing `BASE_URL` from **constants.js**. Here is what that file looks like:\n\n```\n// constants.jsexport const BASE_URL = 'https://www.amazon.com';\n```\n\nAnd here is what our **main.js** file currently looks like:\n\n```\n// main.jsimport { CheerioCrawler, log, KeyValueStore } from 'crawlee';import { router } from './routes.js';import { BASE_URL } from './constants.js';const { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};const crawler = new CheerioCrawler({    requestHandler: router,});await crawler.addRequests([    {        // Use BASE_URL here instead        url: `${BASE_URL}/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,        label: 'START',        userData: {            keyword,        },    },]);log.info('Starting the crawl.');await crawler.run();log.info('Crawl finished.');\n```\n\nOne of the main reasons we **modularize** our code is to prevent massive and difficult to read files by separating concerns into separate files. In our **main.js** file, we're handling the initialization, configuration, and running of our crawler. In **routes.js**, we determine how the crawler should handle different routes, and in **constants.js** we define non-changing values that will be used throughout the project.\n\nOrganized code makes everyone happy, including you - the one developing the scraper! Spaghetti is super awesome, [but not when it comes to programming](https://www.urbandictionary.com/define.php?term=spaghetti+code) 🍝\n\nThis can even be optimized further by putting our `label` items into **constants.js**, like so:\n\n```\n// constants.jsexport const BASE_URL = 'https://www.amazon.com';export const labels = {    START: 'START',    PRODUCT: 'PRODUCT',    OFFERS: 'OFFERS',};\n```\n\nThen, the labels can be used by importing `labels` and accessing `labels.START`, `labels.PRODUCT`, or `labels.OFFERS`.\n\nThis is not necessary, but it is best practice, as it can prevent dumb typos that can cause nasty bugs 🐞 For the rest of this lesson, all of the examples using labels will be using the imported versions.\n\n> If you haven't already read the **Best practices** lesson in the **Web scraping basics for JavaScript devs** course, please [give it a read](https://docs.apify.com/academy/web-scraping-for-beginners/best-practices).\n\n## Next up[​](#next \"Direct link to Next up\")\n\nNow that we've gotten that out of the way, we can finally continue with our Amazon scraper. [Let's do it](https://docs.apify.com/academy/web-scraping-for-beginners/challenge/scraping-amazon)!",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/scraping-amazon",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/scraping-amazon",
    "loadedTime": "2025-07-21T15:51:36.506Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/scraping-amazon",
    "title": "Scraping Amazon | Academy | Apify Documentation",
    "description": "Before you build your first web scraper with Crawlee, it is important to understand the concept of modularity in programming.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/challenge/scraping-amazon"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Scraping Amazon | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Before you build your first web scraper with Crawlee, it is important to understand the concept of modularity in programming."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Scraping+Amazon"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "12979",
      "date": "Mon, 21 Jul 2025 15:51:34 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-19ae0\"",
      "expires": "Mon, 21 Jul 2025 16:01:34 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "FFE1:2FA06C:128B41:1402E9:687E6206",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kiad7000115-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113095.680912, VS0, VE18",
      "x-fastly-request-id": "9dc7b16a158d378624a58c8386eb1a937d7638ad",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "tn7FsSm4XcSOSObLLGZuMSEYrSdRpOsbXfVaAgUkTLXzcgAL_auEdw==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Scraping Amazon | Academy | Apify Documentation\nBuild your first web scraper with Crawlee. Let's extract product information from Amazon to give you an idea of what real-world scraping looks like.\nIn our quick chat about modularity, we finished the code for the results page and added a request for each product to the crawler's RequestQueue. Here, we need to scrape the description, so it shouldn't be too hard:\n// routes.js\n\n// ...\n\nrouter.addHandler(labels.PRODUCT, async ({ $ }) => {\nconst element = $('div#productDescription');\n\nconst description = element.text().trim();\n\nconsole.log(description); // works!\n});\nGreat! But wait, where do we go from here? We need to go to the offers page next and scrape each offer, but how can we do that? Let's take a small break from writing the scraper and open up Proxyman to analyze requests which we might be difficult to find in the network tab, then we'll click the button on the product page that loads up all of the product offers:\nAfter clicking this button and checking back in Proxyman, we discovered this link:\nYou can find the request below in the network tab just fine, but with Proxyman, it is much easier and faster due to the extended filtering options.\nhttps://www.amazon.com/gp/aod/ajax/ref=auto_load_aod?asin=B07ZPKBL9V&pc=dp\nThe asin query parameter matches up with our product's ASIN, which means we can use this for any product of which we have the ASIN.\nHere's what this page looks like:\nWow, that's ugly. But for our scenario, this is really great. When we click the View offers button, we usually have to wait for the offers to load and render, which would mean we could have to switch our entire crawler to a PuppeteerCrawler or PlaywrightCrawler. The data on this page we've just found appears to be loaded statically, which means we can still use CheerioCrawler and keep the scraper as efficient as possible 😎\nIt's totally possible to scrape the same data as this crawler using Puppeteer or Playwright; however, with this offers link found in Postman, we can follow the same workflow much more quickly with static HTTP requests using CheerioCrawler.\nFirst, we'll create a request for each product's offers page:\n// routes.js\n\n// ...\n\nrouter.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\nconst { data } = request.userData;\n\nconst element = $('div#productDescription');\n\n// Add to the request queue\nawait crawler.addRequests([{\nurl: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,\nlabel: labels.OFFERS,\nuserData: {\ndata: {\n...data,\ndescription: element.text().trim(),\n},\n},\n}]);\n});\nFinally, we can handle the offers in a separate handler:\n// routes.js\n\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\nconst { data } = request.userData;\n\nfor (const offer of $('#aod-offer')) {\nconst element = $(offer);\n\nawait Dataset.pushData({\n...data,\nsellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\noffer: element.find('.a-price .a-offscreen').text().trim(),\n});\n\n}\n});\nFinal code​\nThat should be it! Let's make sure we've all got the same code:\n// constants.js\nexport const BASE_URL = 'https://www.amazon.com';\n\nexport const labels = {\nSTART: 'START',\nPRODUCT: 'PRODUCT',\nOFFERS: 'OFFERS',\n};\n// routes.js\nimport { createCheerioRouter, Dataset } from 'crawlee';\nimport { BASE_URL, labels } from './constants';\n\nexport const router = createCheerioRouter();\n\nrouter.addHandler(labels.START, async ({ $, crawler, request }) => {\nconst { keyword } = request.userData;\n\nconst products = $('div > div[data-asin]:not([data-asin=\"\"])');\n\nfor (const product of products) {\nconst element = $(product);\nconst titleElement = $(element.find('.a-text-normal[href]'));\n\nconst url = `${BASE_URL}${titleElement.attr('href')}`;\n\nawait crawler.addRequests([\n{\nurl,\nlabel: labels.PRODUCT,\nuserData: {\ndata: {\ntitle: titleElement.first().text().trim(),\nasin: element.attr('data-asin'),\nitemUrl: url,\nkeyword,\n},\n},\n},\n]);\n}\n});\n\nrouter.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {\nconst { data } = request.userData;\n\nconst element = $('div#productDescription');\n\nawait crawler.addRequests([\n{\nurl: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,\nlabel: labels.OFFERS,\nuserData: {\ndata: {\n...data,\ndescription: element.text().trim(),\n},\n},\n},\n]);\n});\n\nrouter.addHandler(labels.OFFERS, async ({ $, request }) => {\nconst { data } = request.userData;\n\nfor (const offer of $('#aod-offer')) {\nconst element = $(offer);\n\nawait Dataset.pushData({\n...data,\nsellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),\noffer: element.find('.a-price .a-offscreen').text().trim(),\n});\n}\n});\n// main.js\nimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';\nimport { router } from './routes.js';\n\n// Grab our keyword from the input\nconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};\n\nconst crawler = new CheerioCrawler({\nrequestHandler: router,\n});\n\n// Add our initial requests\nawait crawler.addRequests([\n{\n// Turn the inputted keyword into a link we can make a request with\nurl: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,\nlabel: 'START',\nuserData: {\nkeyword,\n},\n},\n]);\n\nlog.info('Starting the crawl.');\nawait crawler.run();\nlog.info('Crawl finished.');\nWrap up 💥​\nNice work! You've officially built your first scraper with Crawlee! You're now ready to take on the rest of the Apify Academy with confidence.\nFor now, this is the last section of the Web scraping basics for JavaScript devs course. If you want to learn more about web scraping, we recommend checking venturing out and following the other lessons in the Academy. We will keep updating the Academy with more content regularly until we cover all the advanced and expert topics we promised at the beginning.",
  "markdown": "# Scraping Amazon | Academy | Apify Documentation\n\n**Build your first web scraper with Crawlee. Let's extract product information from Amazon to give you an idea of what real-world scraping looks like.**\n\n* * *\n\nIn our quick chat about modularity, we finished the code for the results page and added a request for each product to the crawler's **RequestQueue**. Here, we need to scrape the description, so it shouldn't be too hard:\n\n```\n// routes.js// ...router.addHandler(labels.PRODUCT, async ({ $ }) => {    const element = $('div#productDescription');    const description = element.text().trim();    console.log(description); // works!});\n```\n\nGreat! But wait, where do we go from here? We need to go to the offers page next and scrape each offer, but how can we do that? Let's take a small break from writing the scraper and open up [Proxyman](https://docs.apify.com/academy/tools/proxyman) to analyze requests which we might be difficult to find in the network tab, then we'll click the button on the product page that loads up all of the product offers:\n\n![View offers button](https://docs.apify.com/assets/images/view-offers-button-11e242f1d72d36745d86c7a0f114e637.jpg)\n\nAfter clicking this button and checking back in Proxyman, we discovered this link:\n\n> You can find the request below in the network tab just fine, but with Proxyman, it is much easier and faster due to the extended filtering options.\n\n```\nhttps://www.amazon.com/gp/aod/ajax/ref=auto_load_aod?asin=B07ZPKBL9V&pc=dp\n```\n\nThe `asin` [query parameter](https://www.branch.io/glossary/query-parameters/) matches up with our product's ASIN, which means we can use this for any product of which we have the ASIN.\n\nHere's what this page looks like:\n\n![View offers page](https://docs.apify.com/assets/images/offers-page-f9880ec59ed265ab8bf0acce5e2a212b.jpg)\n\nWow, that's ugly. But for our scenario, this is really great. When we click the **View offers** button, we usually have to wait for the offers to load and render, which would mean we could have to switch our entire crawler to a **PuppeteerCrawler** or **PlaywrightCrawler**. The data on this page we've just found appears to be loaded statically, which means we can still use CheerioCrawler and keep the scraper as efficient as possible 😎\n\n> It's totally possible to scrape the same data as this crawler using [Puppeteer or Playwright](https://docs.apify.com/academy/puppeteer-playwright); however, with this offers link found in Postman, we can follow the same workflow much more quickly with static HTTP requests using CheerioCrawler.\n\nFirst, we'll create a request for each product's offers page:\n\n```\n// routes.js// ...router.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {    const { data } = request.userData;    const element = $('div#productDescription');    // Add to the request queue    await crawler.addRequests([{        url: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,        label: labels.OFFERS,        userData: {            data: {                ...data,                description: element.text().trim(),            },        },    }]);});\n```\n\nFinally, we can handle the offers in a separate handler:\n\n```\n// routes.jsrouter.addHandler(labels.OFFERS, async ({ $, request }) => {    const { data } = request.userData;    for (const offer of $('#aod-offer')) {        const element = $(offer);        await Dataset.pushData({            ...data,            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),            offer: element.find('.a-price .a-offscreen').text().trim(),        });    }});\n```\n\n## Final code[​](#final-code \"Direct link to Final code\")\n\nThat should be it! Let's make sure we've all got the same code:\n\n```\n// constants.jsexport const BASE_URL = 'https://www.amazon.com';export const labels = {    START: 'START',    PRODUCT: 'PRODUCT',    OFFERS: 'OFFERS',};\n```\n\n```\n// routes.jsimport { createCheerioRouter, Dataset } from 'crawlee';import { BASE_URL, labels } from './constants';export const router = createCheerioRouter();router.addHandler(labels.START, async ({ $, crawler, request }) => {    const { keyword } = request.userData;    const products = $('div > div[data-asin]:not([data-asin=\"\"])');    for (const product of products) {        const element = $(product);        const titleElement = $(element.find('.a-text-normal[href]'));        const url = `${BASE_URL}${titleElement.attr('href')}`;        await crawler.addRequests([            {                url,                label: labels.PRODUCT,                userData: {                    data: {                        title: titleElement.first().text().trim(),                        asin: element.attr('data-asin'),                        itemUrl: url,                        keyword,                    },                },            },        ]);    }});router.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {    const { data } = request.userData;    const element = $('div#productDescription');    await crawler.addRequests([        {            url: `${BASE_URL}/gp/aod/ajax/ref=auto_load_aod?asin=${data.asin}&pc=dp`,            label: labels.OFFERS,            userData: {                data: {                    ...data,                    description: element.text().trim(),                },            },        },    ]);});router.addHandler(labels.OFFERS, async ({ $, request }) => {    const { data } = request.userData;    for (const offer of $('#aod-offer')) {        const element = $(offer);        await Dataset.pushData({            ...data,            sellerName: element.find('div[id*=\"soldBy\"] a[aria-label]').text().trim(),            offer: element.find('.a-price .a-offscreen').text().trim(),        });    }});\n```\n\n```\n// main.jsimport { CheerioCrawler, KeyValueStore, log } from 'crawlee';import { router } from './routes.js';// Grab our keyword from the inputconst { keyword = 'iphone' } = (await KeyValueStore.getInput()) ?? {};const crawler = new CheerioCrawler({    requestHandler: router,});// Add our initial requestsawait crawler.addRequests([    {        // Turn the inputted keyword into a link we can make a request with        url: `https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=${keyword}`,        label: 'START',        userData: {            keyword,        },    },]);log.info('Starting the crawl.');await crawler.run();log.info('Crawl finished.');\n```\n\n## Wrap up 💥[​](#wrap-up \"Direct link to Wrap up 💥\")\n\nNice work! You've officially built your first scraper with Crawlee! You're now ready to take on the rest of the Apify Academy with confidence.\n\nFor now, this is the last section of the **Web scraping basics for JavaScript devs** course. If you want to learn more about web scraping, we recommend checking venturing out and following the other lessons in the Academy. We will keep updating the Academy with more content regularly until we cover all the advanced and expert topics we promised at the beginning.",
  "debug": {
    "requestHandlerMode": "browser"
  }
},
{
  "url": "https://docs.apify.com/academy/web-scraping-for-beginners/best-practices",
  "crawl": {
    "loadedUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/best-practices",
    "loadedTime": "2025-07-21T15:51:43.717Z",
    "referrerUrl": "https://docs.apify.com/academy/web-scraping-for-beginners",
    "depth": 1,
    "httpStatusCode": 200
  },
  "metadata": {
    "canonicalUrl": "https://docs.apify.com/academy/web-scraping-for-beginners/best-practices",
    "title": "Best practices | Academy | Apify Documentation",
    "description": "Understand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.",
    "author": null,
    "keywords": null,
    "languageCode": "en",
    "openGraph": [
      {
        "property": "og:url",
        "content": "https://docs.apify.com/academy/web-scraping-for-beginners/best-practices"
      },
      {
        "property": "og:locale",
        "content": "en"
      },
      {
        "property": "og:title",
        "content": "Best practices | Academy | Apify Documentation"
      },
      {
        "property": "og:description",
        "content": "Understand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code."
      },
      {
        "property": "og:image",
        "content": "https://apify.com/og-image/docs-article?title=Best+practices"
      }
    ],
    "jsonLd": null,
    "headers": {
      "content-type": "text/html; charset=utf-8",
      "content-length": "11393",
      "date": "Mon, 21 Jul 2025 15:51:42 GMT",
      "server": "nginx",
      "x-origin-cache": "HIT",
      "last-modified": "Mon, 21 Jul 2025 15:37:30 GMT",
      "access-control-allow-origin": "*",
      "strict-transport-security": "max-age=31556952",
      "etag": "W/\"687e5eba-b4a4\"",
      "expires": "Mon, 21 Jul 2025 16:01:42 GMT",
      "cache-control": "max-age=600",
      "content-encoding": "gzip",
      "x-proxy-cache": "MISS",
      "x-github-request-id": "99D6:3A019C:11EB36:136352:687E620E",
      "accept-ranges": "bytes",
      "via": "1.1 varnish, 1.1 c18437b8fe98e7c0f27f6f8fee39d67a.cloudfront.net (CloudFront)",
      "x-served-by": "cache-iad-kjyo7100150-IAD",
      "x-cache-hits": "0",
      "x-timer": "S1753113103.546372, VS0, VE11",
      "x-fastly-request-id": "c1ee42d14e771bc7bcffd834b9582cf34e50e2ff",
      "x-frame-options": "SAMEORIGIN",
      "vary": "Accept-Encoding",
      "x-cache": "Miss from cloudfront",
      "x-amz-cf-pop": "DFW57-P7",
      "x-amz-cf-id": "wQAh72tsGUnm20EWYCEskGdBqdeboSXiVVKIi1bMiw89lusbquhkQA==",
      "age": "0",
      "x-firefox-spdy": "h2"
    }
  },
  "screenshotUrl": null,
  "text": "Best practices | Academy | Apify Documentation\nBest practices when writing scrapers\nUnderstand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.\nEvery developer has their own style, which evolves as they grow and learn. While one dev might prefer a more functional style, another might find an imperative approach to be more intuitive. We at Apify understand this, and have written this best practices lesson with that in mind.\nThe goal of this lesson is not to force you into a specific paradigm or to make you think that you're doing things wrong, but instead to provide you some insight into the standards and best practices that we at Apify follow to ensure readable, maintainable, scalable code.\nCode style​\nWhen it comes to your code style when writing scrapers, there are some general things we recommend.\nClean code​\nPraise clean code! Use proper variable and function names that are descriptive of what they are, and split your code into smaller pure functions.\nConstant variables​\nDefine any constant variables that globally apply to the scraper in a single file named constants.js, from where they will all be imported. Constant variable names should be in UPPERCASE_WITH_UNDERSCORES style.\nIf you have a whole lot of constant variables, they can be in a folder named constants organized into different files.\nUse ES6 JavaScript​\nIf you're writing your scraper in JavaScript, use ES6 features and ditch the old ones which they replace. This means using const and let instead of var, includes instead of indexOf, etc.\nTo learn more about some of the most popular (and awesome) ES6+ features, check out this article.\nNo magic numbers​\nAvoid using magic numbers as much as possible. Either declare them as a constant variable in your constants.js file, or if they are only used once, add a comment explaining what the number is.\nDon't write code like this:\nconst x = (y) => (y - 32) * (5 / 9);\nThat is quite confusing due to the nondescriptive naming and the magic numbers. Do this instead:\n// Converts a fahrenheit value to celsius\nconst fahrenheitToCelsius = (celsius) => (celsius - 32) * (5 / 9);\nDon't be shy to add comments to your code! Even when using descriptive function and variable naming, it might still be a good idea to add a comment in places where you had to make a tough decision or chose an unusual choice.\nIf you're a true pro, use JSDoc to comment and document your code.\nLogging​\nLogging helps you understand exactly what your scraper is doing. Generally, having more logs is better than having fewer. Especially make sure to log your catch blocks - no error should pass unseen unless there is a good reason.\nFor scrapers that will run longer than usual, keep track of some useful stats (such as itemsScraped or errorsHit) and log them to the console on an interval.\nThe meaning of your log messages should make sense to an outsider who is not familiar with the inner workings of your scraper. Avoid log lines with just numbers or just URLs - always identify what the number/string means.\nHere is an example of an \"incorrect\" log message:\n300 https://example.com/1234 1234\nAnd here is that log message translated into something that makes much more sense to the end user:\nIndex 1234 --- https://example.com/1234 --- took 300 ms\nInput​\nWhen it comes to accepting input into a scraper, two main best practices should be followed.\nSet limits​\nWhen allowing your users to pass input properties which could break the scraper (such as timeout set to 0), be sure to disallow ridiculous values. Set a maximum/minimum number allowed, maximum array input length, etc.\nValidate​\nValidate the input provided by the user! This should be the very first thing your scraper does. If the fields in the input are missing or in an incorrect type/format, either parse the value and correct it programmatically or throw an informative error telling the user how to fix the error.\nOn the Apify platform, you can use the input schema to both validate inputs and generate a clean UI for those using your scraper.\nError handling​\nErrors are bound to occur in scrapers. Perhaps it got blocked, or perhaps the data scraped was corrupted in some way.\nWhatever the reason, a scraper shouldn't completely crash when an error occurs. Use try...catch blocks to catch errors and log useful messages. The log messages should indicate where the error happened, and what type of error happened.\nBad error log message:\nCannot read property “0” from undefined\nGood error log message:\nCould not parse an address, skipping the page. Url: https://www.example-website.com/people/1234\nThis doesn't mean that you should absolutely litter your code with try...catch blocks, but it does mean that they should be placed in error-prone areas (such as API calls or testing a string with a specific regular expression).\nIf the error that has occurred renders that run of the scraper completely useless, exit the process immediately.\nLogging is the minimum you should be doing though. For example, if you have an entire object of scraped data and just the price field fails to be parsed, you might not want to throw away the rest of that data. Rather, it could still be pushed to the output and a log message like this could appear:\nWe could not parse the price of product: Men's Trainers Orange, pushing anyways.\nThis really depends on your use case though. If you want 100% clean data, you might not want to push incomplete objects and just retry (ideally) or log an error message instead.\nRecap​\nWow, that's a whole lot of things to abide by! How will you remember all of them? Try to follow these three points:\nDescribe your code as you write it with good naming, constants, and comments. It should read like a book.\nAdd log messages at points throughout your code so that when it's running, you (and everyone else) know what's going on.\nHandle errors appropriately. Log the error and either retry, or continue on. Only throw if the error will be caught or if the error is absolutely detrimental to the scraper's run.",
  "markdown": "# Best practices | Academy | Apify Documentation\n\n## Best practices when writing scrapers\n\n**Understand the standards and best practices that we here at Apify abide by to write readable, scalable, and maintainable code.**\n\n* * *\n\nEvery developer has their own style, which evolves as they grow and learn. While one dev might prefer a more [functional](https://en.wikipedia.org/wiki/Functional_programming) style, another might find an [imperative](https://en.wikipedia.org/wiki/Imperative_programming) approach to be more intuitive. We at Apify understand this, and have written this best practices lesson with that in mind.\n\nThe goal of this lesson is not to force you into a specific paradigm or to make you think that you're doing things wrong, but instead to provide you some insight into the standards and best practices that we at Apify follow to ensure readable, maintainable, scalable code.\n\n## Code style[​](#code-style \"Direct link to Code style\")\n\nWhen it comes to your code style when writing scrapers, there are some general things we recommend.\n\n### Clean code[​](#clean-code \"Direct link to Clean code\")\n\nPraise [clean code](https://blog.risingstack.com/javascript-clean-coding-best-practices-node-js-at-scale/)! Use proper variable and function names that are descriptive of what they are, and split your code into smaller [pure](https://en.wikipedia.org/wiki/Pure_function) functions.\n\n### Constant variables[​](#constants \"Direct link to Constant variables\")\n\nDefine any [constant variables](https://softwareengineering.stackexchange.com/questions/250619/best-practices-reasons-for-string-constants-in-javascript) that globally apply to the scraper in a single file named **constants.js**, from where they will all be imported. Constant variable names should be in `UPPERCASE_WITH_UNDERSCORES` style.\n\n> If you have a whole lot of constant variables, they can be in a folder named **constants** organized into different files.\n\n### Use ES6 JavaScript[​](#use-es6 \"Direct link to Use ES6 JavaScript\")\n\nIf you're writing your scraper in JavaScript, use [ES6](https://www.w3schools.com/js/js_es6.asp) features and ditch the old ones which they replace. This means using `const` and `let` instead of `var`, `includes` instead of `indexOf`, etc.\n\n> To learn more about some of the most popular (and awesome) ES6+ features, check out [this](https://medium.com/@matthiasvstephens/why-is-es6-so-awesome-88bff6857849) article.\n\n### No magic numbers[​](#no-magic-numbers \"Direct link to No magic numbers\")\n\nAvoid using [magic numbers](https://en.wikipedia.org/wiki/Magic_number_\\(programming\\)) as much as possible. Either declare them as a **constant** variable in your **constants.js** file, or if they are only used once, add a comment explaining what the number is.\n\nDon't write code like this:\n\n```\nconst x = (y) => (y - 32) * (5 / 9);\n```\n\nThat is quite confusing due to the nondescriptive naming and the magic numbers. Do this instead:\n\n```\n// Converts a fahrenheit value to celsiusconst fahrenheitToCelsius = (celsius) => (celsius - 32) * (5 / 9);\n```\n\nDon't be shy to add comments to your code! Even when using descriptive function and variable naming, it might still be a good idea to add a comment in places where you had to make a tough decision or chose an unusual choice.\n\n> If you're a true pro, use [JSDoc](https://jsdoc.app/) to comment and document your code.\n\n## Logging[​](#logging \"Direct link to Logging\")\n\nLogging helps you understand exactly what your scraper is doing. Generally, having more logs is better than having fewer. Especially make sure to log your `catch` blocks - no error should pass unseen unless there is a good reason.\n\nFor scrapers that will run longer than usual, keep track of some useful stats (such as **itemsScraped** or **errorsHit**) and log them to the console on an interval.\n\nThe meaning of your log messages should make sense to an outsider who is not familiar with the inner workings of your scraper. Avoid log lines with just numbers or just URLs - always identify what the number/string means.\n\nHere is an example of an \"incorrect\" log message:\n\n```\n300  https://example.com/1234  1234\n```\n\nAnd here is that log message translated into something that makes much more sense to the end user:\n\n```\nIndex 1234 --- https://example.com/1234 --- took 300 ms\n```\n\n## Input[​](#input \"Direct link to Input\")\n\nWhen it comes to accepting input into a scraper, two main best practices should be followed.\n\n### Set limits[​](#set-limits \"Direct link to Set limits\")\n\nWhen allowing your users to pass input properties which could break the scraper (such as **timeout** set to **0**), be sure to disallow ridiculous values. Set a maximum/minimum number allowed, maximum array input length, etc.\n\n### Validate[​](#validate \"Direct link to Validate\")\n\nValidate the input provided by the user! This should be the very first thing your scraper does. If the fields in the input are missing or in an incorrect type/format, either parse the value and correct it programmatically or throw an informative error telling the user how to fix the error.\n\n> On the Apify platform, you can use the [input schema](https://docs.apify.com/academy/deploying-your-code/input-schema) to both validate inputs and generate a clean UI for those using your scraper.\n\n## Error handling[​](#error-handling \"Direct link to Error handling\")\n\nErrors are bound to occur in scrapers. Perhaps it got blocked, or perhaps the data scraped was corrupted in some way.\n\nWhatever the reason, a scraper shouldn't completely crash when an error occurs. Use `try...catch` blocks to catch errors and log useful messages. The log messages should indicate where the error happened, and what type of error happened.\n\nBad error log message:\n\n```\nCannot read property “0” from undefined\n```\n\nGood error log message:\n\n```\nCould not parse an address, skipping the page. Url: https://www.example-website.com/people/1234\n```\n\nThis doesn't mean that you should absolutely litter your code with `try...catch` blocks, but it does mean that they should be placed in error-prone areas (such as API calls or testing a string with a specific regular expression).\n\n> If the error that has occurred renders that run of the scraper completely useless, exit the process immediately.\n\nLogging is the minimum you should be doing though. For example, if you have an entire object of scraped data and just the **price** field fails to be parsed, you might not want to throw away the rest of that data. Rather, it could still be pushed to the output and a log message like this could appear:\n\n```\nWe could not parse the price of product: Men's Trainers Orange, pushing anyways.\n```\n\nThis really depends on your use case though. If you want 100% clean data, you might not want to push incomplete objects and just retry (ideally) or log an error message instead.\n\n## Recap[​](#recap \"Direct link to Recap\")\n\nWow, that's a whole lot of things to abide by! How will you remember all of them? Try to follow these three points:\n\n1.  Describe your code as you write it with good naming, constants, and comments. It **should read like a book**.\n2.  Add log messages at points throughout your code so that when it's running, you (and everyone else) know what's going on.\n3.  Handle errors appropriately. Log the error and either retry, or continue on. Only throw if the error will be caught or if the error is absolutely detrimental to the scraper's run.",
  "debug": {
    "requestHandlerMode": "browser"
  }
}]